{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for analyzing layer data \n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import yaspy\n",
    "import scipy\n",
    "import pyvista as pv\n",
    "from brainspace.gradient import GradientMaps\n",
    "from brainspace.utils.parcellation import map_to_labels\n",
    "import brainspace.gradient.alignment as ga\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pv.start_xvfb()\n",
    "\n",
    "#schaefer\n",
    "atlas_schaefer100 = nib.load('/Users/dennis.jungchildmind.org/Downloads/Schaefer2018_100Parcels_17Networks_order.dlabel.nii').get_fdata()[0].astype(int)\n",
    "atlas_schaefer400 = nib.load('/Users/dennis.jungchildmind.org/Downloads/Schaefer2018_400Parcels_17Networks_order.dlabel.nii').get_fdata()[0].astype(int)\n",
    "atlas_schaefer1000 = nib.load('/Users/dennis.jungchildmind.org/Downloads/Schaefer2018_1000Parcels_17Networks_order.dlabel.nii').get_fdata()[0].astype(int)\n",
    "\n",
    "\n",
    "#glasser\n",
    "'''\n",
    "atlas_path_R = '/Users/dennis.jungchildmind.org/Downloads/Glasser_2016.32k.R.label.gii'\n",
    "atlas_R = nib.load(atlas_path_R).darrays[0].data\n",
    "atlas_path_L = '/Users/dennis.jungchildmind.org/Downloads/Glasser_2016.32k.L.label.gii'\n",
    "atlas_L = nib.load(atlas_path_L).darrays[0].data\n",
    "atlas = np.concatenate([atlas_L, atlas_R])\n",
    "'''\n",
    "#Desikan\n",
    "'''\n",
    "atlas_path_R = '/Users/dennis.jungchildmind.org/Downloads/Desikan.32k.R.label.gii'\n",
    "atlas_R = nib.load(atlas_path_R).darrays[0].data\n",
    "atlas_path_L = '/Users/dennis.jungchildmind.org/Downloads/Desikan.32k.L.label.gii'\n",
    "atlas_L = nib.load(atlas_path_L).darrays[0].data\n",
    "atlas = np.concatenate([atlas_L, atlas_R])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "baseDir = '/Users/dennis.jungchildmind.org/Downloads/BigBrain/thickness/resample/'\n",
    "#layer 0 is the pial surface\n",
    "#layer 6 is the white matter surface\n",
    "#supragranular layer 1-3, from surface 0-3\n",
    "#granular layer 4, from surface 3-4\n",
    "#infragranular layer 5-6, from surface 4-6\n",
    "#find files from the baseDir with the following pattern: '32k.shape.gii' separately for lh and rh\n",
    "# Dictionary to store layer group data\n",
    "layer_group_data = {\n",
    "    'supra': {\n",
    "        'lh': nib.load(os.path.join(baseDir, 'lh.0-3.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, 'rh.0-3.32k.shape.gii')).darrays[0].data\n",
    "    },\n",
    "    'gra': {\n",
    "        'lh': nib.load(os.path.join(baseDir, 'lh.3-4.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, 'rh.3-4.32k.shape.gii')).darrays[0].data\n",
    "    },\n",
    "    'infra': {\n",
    "        'lh': nib.load(os.path.join(baseDir, 'lh.4-6.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, 'rh.4-6.32k.shape.gii')).darrays[0].data\n",
    "    },\n",
    "    'infra_exvivo': {\n",
    "        'lh': nib.load(os.path.join(baseDir, 'lh.3-6.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, 'rh.3-6.32k.shape.gii')).darrays[0].data\n",
    "    },\n",
    "    'total': {\n",
    "        'lh': nib.load(os.path.join(baseDir, 'lh.0-6.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, 'rh.0-6.32k.shape.gii')).darrays[0].data\n",
    "    }\n",
    "}\n",
    "\n",
    "#for the ex vivo data, supragranular is from 1-3 and infragranular is from 4-6\n",
    "layer_group_data_2 = {\n",
    "    'supra': {\n",
    "        'lh': nib.load(os.path.join(baseDir, 'lh.0-3.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, 'rh.0-3.32k.shape.gii')).darrays[0].data\n",
    "    },\n",
    "\n",
    "    'infra': {\n",
    "        'lh': nib.load(os.path.join(baseDir, 'lh.3-6.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, 'rh.3-6.32k.shape.gii')).darrays[0].data\n",
    "    },\n",
    "    'total': {\n",
    "        'lh': nib.load(os.path.join(baseDir, 'lh.0-6.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, 'rh.0-6.32k.shape.gii')).darrays[0].data\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#individual layers\n",
    "# Dictionary to store layer data\n",
    "layer_data = {}\n",
    "\n",
    "# Layer ranges\n",
    "layer_ranges = range(1,7)\n",
    "\n",
    "# Load data for each layer\n",
    "for layer in layer_ranges:\n",
    "    start = layer-1\n",
    "    layer_data[f'L{layer}'] = {\n",
    "        'lh': nib.load(os.path.join(baseDir, f'lh.{start}-{layer}.32k.shape.gii')).darrays[0].data,\n",
    "        'rh': nib.load(os.path.join(baseDir, f'rh.{start}-{layer}.32k.shape.gii')).darrays[0].data\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Surface files and settings\n",
    "lh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.L.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "rh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.R.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "\n",
    "data2plot = layer_data#either layer_data or layer_group_data\n",
    "cmap = 'jet'#'autumn'\n",
    "# Plot each hemisphere\n",
    "for k in ['lh','rh']:\n",
    "    fig, axes = plt.subplots(2, len(data2plot), figsize=(len(data2plot)*3, 4))\n",
    "    surf = lh_surf if k=='lh' else rh_surf\n",
    "    \n",
    "    # Plot each layer\n",
    "    for i, layer_name in enumerate(data2plot.keys()):\n",
    "        data = data2plot[layer_name][k].copy()  # Make a copy to avoid modifying original\n",
    "        data[np.where(data == 0)] = np.nan  # Use np.where to find zeros\n",
    "        data_min = np.nanmin(data)  # Use nanmin to ignore NaN values\n",
    "        data_max = np.nanmax(data)*0.8  # Use nanmax to ignore NaN values\n",
    "        print('min:', f'{data_min:.3f}', 'max:', f'{data_max:.3f}')\n",
    "        norm = plt.Normalize(vmin=data_min, vmax=data_max)\n",
    "        #calculate mean and std of data\n",
    "        mean_data = np.nanmean(data)  # Use nanmean to ignore NaN values\n",
    "        std_data = np.nanstd(data)  # Use nanstd to ignore NaN values\n",
    "        print('mean:', f'{mean_data:.3f}', '±', f'{std_data:.3f}')\n",
    "        # Plot lateral and medial views\n",
    "        for view_idx, view in enumerate(['lateral', 'medial']):\n",
    "            plotter = yaspy.Plotter(surf, hemi=k)\n",
    "            plotter.overlay(data, cmap=cmap, alpha=1, vmin=data_min, vmax=data_max)\n",
    "            plotter.border(data, alpha=0)\n",
    "            img = plotter.screenshot(view)\n",
    "            axes[view_idx,i].imshow(img)\n",
    "            axes[view_idx,i].axis('off')\n",
    "            \n",
    "            if view_idx == 0:\n",
    "                axes[0,i].set_title(layer_name, fontweight='bold')\n",
    "            else:\n",
    "                divider = make_axes_locatable(axes[1,i])\n",
    "                cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.05)\n",
    "                cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                           cax=cax, orientation='horizontal')\n",
    "                cbar.set_label('Thickness (mm)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf, OAS, GraphicalLasso\n",
    "\n",
    " \n",
    "def improved_partial_correlation_with_shrinkage(x):\n",
    "    \"\"\"\n",
    "    Partial correlation with Ledoit-Wolf shrinkage applied consistently\n",
    "    x: (n_features, n_samples) \n",
    "    Returns: (n_features, n_features) partial correlation matrix\n",
    "    \"\"\"\n",
    "  \n",
    "    n_features, n_samples = x.shape\n",
    "    \n",
    "    # Step 1: Handle NaN values\n",
    "    x_clean = x.copy()\n",
    "    for i in range(n_features):\n",
    "        mask = np.isnan(x_clean[i, :])\n",
    "        if mask.any():\n",
    "            x_clean[i, mask] = np.nanmean(x_clean[i, :])\n",
    "    \n",
    "    # Step 2: Calculate mean across features for each sample\n",
    "    x_mean = np.mean(x_clean, axis=0)  # Shape: (n_samples,)\n",
    "    \n",
    "    # Step 3: Create augmented data matrix\n",
    "    # Transpose for sklearn: samples x features\n",
    "    augmented_data = np.column_stack([x_clean.T, x_mean])  # Shape: (n_samples, n_features+1)\n",
    "    \n",
    "    # Step 4: Apply Ledoit-Wolf shrinkage\n",
    "    lw = LedoitWolf()\n",
    "    shrunk_cov = lw.fit(augmented_data).covariance_  # Shape: (n_features+1, n_features+1)\n",
    "    #oa = OAS()\n",
    "    #shrunk_cov = oa.fit(augmented_data).covariance_  # Shape: (n_features+1, n_features+1)\n",
    "    # Convert to correlation matrix\n",
    "    def cov_to_corr(cov):\n",
    "        std = np.sqrt(np.diag(cov))\n",
    "        return cov / np.outer(std, std)\n",
    "    \n",
    "    shrunk_corr = cov_to_corr(shrunk_cov)  # Shape: (n_features+1, n_features+1)\n",
    "    \n",
    "    # Step 5: Extract components for partial correlation\n",
    "    r_ij = shrunk_corr[:n_features, :n_features]  # Shape: (n_features, n_features)\n",
    "    r_ic = shrunk_corr[:n_features, -1]           # Shape: (n_features,)\n",
    "    \n",
    "    # Step 6: Calculate partial correlations\n",
    "    r_icjc = np.outer(r_ic, r_ic)  # Shape: (n_features, n_features)\n",
    "    denominator = np.sqrt(np.outer((1 - r_ic**2), (1 - r_ic**2)))  # Shape: (n_features, n_features)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    denominator[denominator < np.finfo(float).eps] = np.finfo(float).eps\n",
    "    \n",
    "    partial_corr = (r_ij - r_icjc) / denominator  # Shape: (n_features, n_features)\n",
    "    \n",
    "    # Step 7: Clean up the matrix\n",
    "    np.fill_diagonal(partial_corr, 0)\n",
    "    partial_corr[np.isnan(partial_corr) | np.isinf(partial_corr)] = 0\n",
    "    partial_corr = np.clip(partial_corr, -1, 1)\n",
    "    \n",
    "    return partial_corr  # Shape: (n_features, n_features) \n",
    "\n",
    "def calculate_partial_correlations(x):\n",
    "    \"\"\"\n",
    "    Calculate partial correlations between regions while controlling for mean thickness.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : numpy.ndarray\n",
    "        Input data matrix with shape (n_regions, n_samples)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    partial_corr : numpy.ndarray\n",
    "        Matrix of partial correlations between regions\n",
    "    \"\"\"\n",
    "    # First get pairwise correlations between regions\n",
    "    \n",
    "    r_ij = np.corrcoef(x)\n",
    "    # Calculate mean thickness across regions\n",
    "    x_mean = np.nanmean(x, axis=0)\n",
    "    # Get correlations between each region and the mean\n",
    "    r_ic = np.corrcoef(x, x_mean[np.newaxis,:])[-1,:-1]\n",
    "    \n",
    "    # Calculate partial correlations controlling for mean\n",
    "    # Formula: (r_ij - r_i,m * r_j,m) / sqrt((1-r_i,m^2)(1-r_j,m^2))\n",
    "    # where r_i,m is correlation of region i with mean\n",
    "    r_icjc = np.outer(r_ic, r_ic)\n",
    "    partial_corr = (r_ij - r_icjc) / np.sqrt(np.outer((1-r_ic**2), (1-r_ic**2)))\n",
    "\n",
    "\n",
    "    # zero out correlations of 1 (to avoid division by 0)\n",
    "    partial_corr[np.isclose(partial_corr, 1)] = 0\n",
    "    # Fisher z-transform the correlations\n",
    "    #partial_corr = 0.5 * np.log((1 + partial_corr) / (1 - partial_corr))\n",
    "\n",
    "    # Clean up any invalid values\n",
    "    partial_corr[np.isnan(partial_corr) | np.isinf(partial_corr)] = 0\n",
    "    \n",
    "    return partial_corr\n",
    "\n",
    "\n",
    "def calculate_correlation_matrix(data, transpose, partial_corr, shrink_it, first_index):\n",
    "    \"\"\"Calculate correlation matrix based on given parameters.\"\"\"\n",
    "    if transpose:\n",
    "        data = data[first_index:].T\n",
    "    else:\n",
    "        data = data[first_index:]\n",
    "        \n",
    "    if partial_corr:\n",
    "        corr = calculate_partial_correlations(data)\n",
    "        if shrink_it:\n",
    "            print('shrinkage')\n",
    "            corr = improved_partial_correlation_with_shrinkage(data)\n",
    "    else:\n",
    "        corr = np.corrcoef(data)\n",
    "        \n",
    "    return corr\n",
    "\n",
    "def plot_correlation_matrix(corr, transpose, title, radius):\n",
    "    \"\"\"Plot correlation matrix with consistent formatting.\"\"\"\n",
    "    # Remove diagonal values\n",
    "    corr_plot = corr.copy()\n",
    "    corr_plot[np.eye(corr_plot.shape[0], dtype=bool)] = np.nan\n",
    "    \n",
    "    # Plot matrix\n",
    "    if transpose:\n",
    "        im = plt.imshow(corr_plot, cmap='jet', vmin=0, vmax=1)\n",
    "    else:\n",
    "        im = plt.imshow(corr_plot, cmap='bwr')\n",
    "        \n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.title(f'{title}\\nRadius {radius}mm')\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('Subject')\n",
    "    \n",
    "    return corr\n",
    "\n",
    "def plot_subject_similarity(hemisphere_data, radii, transpose=False, partial_corr=False, shrink_it=False):\n",
    "    \"\"\"\n",
    "    Plot similarity matrices between subjects for different thickness measures and radii.\n",
    "    \n",
    "    Args:\n",
    "        hemisphere_data (dict): Dictionary containing thickness data for one hemisphere\n",
    "        radii (list): List of radius values used for smoothing\n",
    "        transpose (bool): Whether to transpose the data matrices\n",
    "        partial_corr (bool): Whether to calculate partial correlations\n",
    "        shrink_it (bool): Whether to apply shrinkage to correlation calculation\n",
    "    \"\"\"\n",
    "    # Initialize correlation dictionaries\n",
    "    correlation_dicts = {\n",
    "        'total': {},\n",
    "        'infra': {},\n",
    "        'supra': {},\n",
    "        'relative': {},\n",
    "        'ratio': {},\n",
    "        'ratio_exvivo': {}\n",
    "    }\n",
    "\n",
    "    first_index = 1\n",
    "    plt.figure(figsize=(len(correlation_dicts)*4,len(radii)*4))\n",
    "\n",
    "    # Plot matrices for each radius\n",
    "    for i, radius in enumerate(radii):\n",
    "        # Define measures and their plot positions \n",
    "        measures = [\n",
    "            ('total', 'Total Thickness', i+1),\n",
    "            ('infra', 'WM to Inf Thickness', i+1+1),\n",
    "            ('supra', 'Inf to Pial Thickness', i+1+2),\n",
    "            ('relative', 'Relative Thickness', i+1+3),\n",
    "            ('ratio', 'Ratio Thickness', i+1+4),\n",
    "            ('ratio_exvivo', 'Ratio Ex-vivo Thickness', i+1+5)\n",
    "        ]\n",
    "        \n",
    "        # Plot each measure\n",
    "        for measure, title, plot_pos in measures:\n",
    "            print(len(radii), len(measures), plot_pos)\n",
    "            plt.subplot(len(radii), len(measures), plot_pos)\n",
    "            \n",
    "            # Calculate correlation matrix\n",
    "            corr = calculate_correlation_matrix(\n",
    "                hemisphere_data[measure][i],\n",
    "                transpose,\n",
    "                partial_corr,\n",
    "                shrink_it,\n",
    "                first_index\n",
    "            )\n",
    "            \n",
    "            # Store and plot correlation matrix\n",
    "            correlation_dicts[measure][i] = plot_correlation_matrix(\n",
    "                corr,\n",
    "                transpose,\n",
    "                title,\n",
    "                radius\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return tuple(correlation_dicts[k] for k in ['total', 'infra', 'supra', 'relative', 'ratio','ratio_exvivo'])\n",
    "\n",
    "\n",
    "\n",
    "def parcellate_data(matrix, atlas, hemisphere):\n",
    "    \"\"\"Parcellate cortical layer data using a given atlas.\"\"\"\n",
    "    # Load atlas\n",
    "    parcelIndices = atlas\n",
    "    max_parcel_ind = int(np.max(parcelIndices))\n",
    "    \n",
    "    # Split indices by hemisphere\n",
    "    if hemisphere == 'lh':\n",
    "        parcelIndices = parcelIndices[0:32492]\n",
    "    elif hemisphere == 'rh':\n",
    "        parcelIndices = parcelIndices[32492:]\n",
    "    \n",
    "    # Initialize output array\n",
    "    parcellated = []\n",
    "    \n",
    "    # Parcellate data for each subject\n",
    "    for subject_idx in range(matrix.shape[0]):\n",
    "        m = int(max_parcel_ind/2)+1\n",
    "        #m = 180+1 #for glasser or deikan (35)\n",
    "       # m = 35+1;#desikan\n",
    "        # Initialize array for this hemisphere's parcels\n",
    "        subject_data = np.zeros(m)\n",
    "        \n",
    "        # Get subject's data\n",
    "        data = matrix[subject_idx]\n",
    "        \n",
    "        # Set parcel range based on hemisphere\n",
    "        if hemisphere == 'lh':\n",
    "            parcel_range = range(1, m)\n",
    "        elif hemisphere == 'rh':\n",
    "            parcel_range = range(m, max_parcel_ind+1)\n",
    "            #parcel_range = range(1,m) #for glasser or deikan\n",
    "            \n",
    "        # Calculate mean for each parcel\n",
    "        for k in parcel_range:\n",
    "            mask = parcelIndices == k\n",
    "            if np.sum(mask) > 0:\n",
    "                # Use trimmed mean with 10% trim on both ends\n",
    "                # Science advances paper, \"structural covariance, Parcellation Approach in Methods\"\n",
    "                parcel_idx = k if hemisphere == 'lh' else k-m+1\n",
    "                #parcel_idx = k  #for glasser or deikan\n",
    "                subject_data[parcel_idx] = scipy.stats.trim_mean(data[mask][~np.isnan(data[mask])], proportiontocut=0.1)\n",
    "                #subject_data[parcel_idx] = np.nanmedian(data[mask])\n",
    "                \n",
    "        # Append subject's parcellated data\n",
    "        parcellated.append(subject_data)\n",
    "            \n",
    "    # Convert list to numpy array\n",
    "    return np.vstack(parcellated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['infra','infra_exvivo', 'supra', 'gra', 'L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'relative', 'total','ratio','ratio_exvivo']\n",
    "hemispheres = {'lh': 'left', 'rh': 'right'}\n",
    "\n",
    "# Initialize dictionaries\n",
    "data = {hemi: {dtype: [] for dtype in data_types} for hemi in hemispheres}\n",
    "\n",
    "\n",
    "\n",
    "for hemi in hemispheres:\n",
    "    if os.path.exists(f'{baseDir}/{hemi}.0-6.32k.shape.gii'):\n",
    "        # Load thickness data\n",
    "        print(hemi)\n",
    "        thickness_data = {\n",
    "            #nib.load(os.path.join(baseDir, 'lh.0-3.32k.shape.gii')).darrays[0].data,\n",
    "            \n",
    "            'supra': nib.load(f'{baseDir}/{hemi}.0-3.32k.shape.gii').darrays[0].data,\n",
    "            'gra': nib.load(f'{baseDir}/{hemi}.3-4.32k.shape.gii').darrays[0].data,\n",
    "            'infra': nib.load(f'{baseDir}/{hemi}.4-6.32k.shape.gii').darrays[0].data,\n",
    "            'infra_exvivo': nib.load(f'{baseDir}/{hemi}.3-6.32k.shape.gii').darrays[0].data,\n",
    "            'total': nib.load(f'{baseDir}/{hemi}.0-6.32k.shape.gii').darrays[0].data,\n",
    "            'L1': nib.load(f'{baseDir}/{hemi}.0-1.32k.shape.gii').darrays[0].data,\n",
    "            'L2': nib.load(f'{baseDir}/{hemi}.1-2.32k.shape.gii').darrays[0].data,\n",
    "            'L3': nib.load(f'{baseDir}/{hemi}.2-3.32k.shape.gii').darrays[0].data,\n",
    "            'L4': nib.load(f'{baseDir}/{hemi}.3-4.32k.shape.gii').darrays[0].data,\n",
    "            'L5': nib.load(f'{baseDir}/{hemi}.4-5.32k.shape.gii').darrays[0].data,\n",
    "            'L6': nib.load(f'{baseDir}/{hemi}.5-6.32k.shape.gii').darrays[0].data,\n",
    "        }\n",
    "\n",
    "        thickness_data['relative'] = thickness_data['supra'] / thickness_data['infra']\n",
    "        #set thickness_data['relative'] to 0 if it is nan\n",
    "        thickness_data['relative'][np.isnan(thickness_data['relative'])] = 0\n",
    "        thickness_data['relative'][np.isinf(thickness_data['relative'])] = 0\n",
    "\n",
    "        #for ratio (which is total infragranular layer over total thickness)\n",
    "        thickness_data['ratio_supra'] = thickness_data['supra'] / thickness_data['total']\n",
    "        thickness_data['ratio_supra'][np.isnan(thickness_data['ratio_supra'])] = 0\n",
    "        thickness_data['ratio_supra'][np.isinf(thickness_data['ratio_supra'])] = 0\n",
    "\n",
    "        thickness_data['ratio_infra'] = thickness_data['infra'] / thickness_data['total']\n",
    "        thickness_data['ratio_infra'][np.isnan(thickness_data['ratio_infra'])] = 0\n",
    "        thickness_data['ratio_infra'][np.isinf(thickness_data['ratio_infra'])] = 0\n",
    "\n",
    "        #Ex-vivo style proportion for (infra over total)\n",
    "        thickness_data['ratio_exvivo'] = thickness_data['infra_exvivo'] / thickness_data['total']\n",
    "        thickness_data['ratio_exvivo'][np.isnan(thickness_data['ratio_exvivo'])] = 0\n",
    "        thickness_data['ratio_exvivo'][np.isinf(thickness_data['ratio_exvivo'])] = 0\n",
    "    \n",
    "        #should i normalize here?\n",
    "        # Process each data type\n",
    "        for dtype in data_types:\n",
    "            reshaped_data = thickness_data[dtype].reshape(-1, 1)\n",
    "            if len(data[hemi][dtype]) == 0:\n",
    "                data[hemi][dtype] = reshaped_data\n",
    "            else:\n",
    "                data[hemi][dtype] = np.concatenate((data[hemi][dtype], reshaped_data), axis=1)\n",
    "        \n",
    "        # Clean up relative thickness data\n",
    "        data[hemi]['relative'][np.isnan(data[hemi]['relative'])] = 0\n",
    "        data[hemi]['relative'][np.isinf(data[hemi]['relative'])] = 0\n",
    "\n",
    "# Rename variables to match rest of code\n",
    "lh_data = data['lh']\n",
    "rh_data = data['rh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for smoothed data from both hemispheres for each subject\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,10))\n",
    "\n",
    "layer_type = 'ratio'\n",
    "\n",
    "if layer_type == 'total':\n",
    "    xlimit = (0,1) if normalize_data else (0,4)\n",
    "    xlabel = 'Cortical thickness (mm)'\n",
    "    title = 'Distribution of cortical thickness by subject'\n",
    "elif layer_type == 'relative':\n",
    "    xlimit = (0,1) if normalize_data else (0,3)\n",
    "    xlabel = 'Relative cortical thickness'\n",
    "    title = 'Distribution of relative cortical thickness by subject'\n",
    "elif layer_type == 'infra' or layer_type == 'supra':\n",
    "    xlimit = (0,2) if normalize_data else (0,2)\n",
    "    xlabel = 'Cortical thickness (mm)'\n",
    "    title = 'Distribution of cortical thickness by subject'\n",
    "elif 'ratio' in layer_type:\n",
    "    xlimit = (0,1)\n",
    "    xlabel = 'ratio cortical thickness'\n",
    "    title = 'Distribution of ratio cortical thickness by subject'\n",
    "\n",
    "# Transpose data to get shape (11, 32491) so each row is a subject\n",
    "lh_data_by_subject = lh_data[layer_type].T\n",
    "rh_data_by_subject = rh_data[layer_type].T\n",
    "\n",
    "# Plot histogram for each subject for left hemisphere\n",
    "for i in range(lh_data_by_subject.shape[0]):\n",
    "    # Remove zeros from the data\n",
    "    non_zero_data = lh_data_by_subject[i][lh_data_by_subject[i] != 0]\n",
    "    hist = ax1.hist(non_zero_data, bins=100, range=xlimit, alpha=0.4, label=f'Subject {i+1}')\n",
    "    # Add vertical line for mean using the color of the last histogram\n",
    "    mean = np.mean(non_zero_data)\n",
    "    ax1.axvline(x=mean, color=hist[2][0].get_facecolor(), linestyle='--', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Cortical thickness (mm)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('LH: Distribution of cortical thickness by subject')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(xlimit)\n",
    "\n",
    "# Plot histogram for each subject for right hemisphere\n",
    "for i in range(rh_data_by_subject.shape[0]):\n",
    "    # Remove zeros from the data\n",
    "    non_zero_data = rh_data_by_subject[i][rh_data_by_subject[i] != 0]\n",
    "    hist = ax2.hist(non_zero_data, bins=100, range=xlimit, alpha=0.4, label=f'Subject {i+1}')\n",
    "    # Add vertical line for mean using the color of the last histogram\n",
    "    mean = np.mean(non_zero_data)\n",
    "    ax2.axvline(x=mean, color=hist[2][0].get_facecolor(), linestyle='--', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Cortical thickness (mm)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('RH: Distribution of cortical thickness by subject')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(xlimit)\n",
    "\n",
    "\n",
    "# Make plots square by setting aspect ratio\n",
    "ax1.set_aspect(1.0/ax1.get_data_ratio())\n",
    "ax2.set_aspect(1.0/ax2.get_data_ratio())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hemisphere_data(data, atlas, hemi):\n",
    "    \"\"\"\n",
    "    Process hemisphere data by parcellating metrics and calculating derived measures\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        Dictionary containing hemisphere data with metrics like 'total', 'infra', etc\n",
    "    atlas : array-like\n",
    "        Atlas data for parcellation\n",
    "    hemi: str, 'lh' or 'rh' \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Processed and parcellated data\n",
    "    \"\"\"\n",
    "    # Initialize output dictionary\n",
    "    data_parc = {\n",
    "        'total': [], 'infra': [], 'infra_exvivo': [], 'supra': [], 'gra': [], 'relative': [], 'ratio': [], 'ratio_exvivo': [],\n",
    "        'L1': [], 'L2': [], 'L3': [], 'L4': [], 'L5': [], 'L6': []\n",
    "    }\n",
    "\n",
    "    # Parcellate basic metrics\n",
    "    for metric in ['total', 'infra', 'infra_exvivo', 'supra', 'gra']:\n",
    "        data_parc[metric] = parcellate_data(data[metric].T, atlas, hemi).T\n",
    "\n",
    "    # Parcellate individual layers\n",
    "    for layer in ['L1', 'L2', 'L3', 'L4', 'L5', 'L6']:\n",
    "        data_parc[layer] = parcellate_data(data[layer].T, atlas, hemi).T\n",
    "\n",
    "    # Calculate relative and ratio metrics\n",
    "    data_parc['relative'] = data_parc['supra'] / data_parc['infra_exvivo']# match style of ex vivo data\n",
    "    data_parc['ratio_supra'] = data_parc['supra'] / data_parc['total']  \n",
    "    data_parc['ratio_infra'] = data_parc['infra'] / data_parc['total']\n",
    "    data_parc['ratio_exvivo'] = data_parc['infra_exvivo'] / data_parc['total']\n",
    "\n",
    "    # Clean up invalid values\n",
    "    for metric in ['relative', 'ratio_supra', 'ratio_infra', 'ratio_exvivo']:\n",
    "        data_parc[metric][data_parc[metric] == np.inf] = 0\n",
    "        data_parc[metric][data_parc[metric] == np.nan] = 0\n",
    "\n",
    "    return data_parc\n",
    "\n",
    "\n",
    "\n",
    "#plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hemisphere_parcellation(lh_data_parc, rh_data_parc, atlas):\n",
    "    \"\"\"\n",
    "    Process left and right hemisphere parcellation data using an atlas\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lh_data_parc : dict\n",
    "        Left hemisphere parcellation data\n",
    "    rh_data_parc : dict \n",
    "        Right hemisphere parcellation data\n",
    "    atlas : array-like\n",
    "        Atlas data for mapping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : Processed left and right hemisphere data\n",
    "    \"\"\"\n",
    "    # Process left hemisphere data\n",
    "    for key in lh_data_parc.keys():\n",
    "        print(key)\n",
    "        mask_lh = atlas[:32492] != 0\n",
    "        min_atlas = np.min(atlas[:32492][atlas[:32492] != 0])\n",
    "        max_atlas = np.max(atlas[:32492][atlas[:32492] != 0])\n",
    "        lh_data_parc[key] = map_to_labels(lh_data_parc[key].ravel()[1:], atlas[:32492], \n",
    "                                         mask=mask_lh, fill=np.nan,\n",
    "                                         source_lab=np.arange(min_atlas, max_atlas+1))\n",
    "        #lh_data_parc[key] = map_to_labels(lh_data_parc[key].ravel(), atlas[:32492], \n",
    "        #                                mask=None, fill=np.nan,\n",
    "        #                                 source_lab=np.concatenate(([0], np.arange(min_atlas, max_atlas+1))))\n",
    "\n",
    "    # Process right hemisphere data \n",
    "    for key in rh_data_parc.keys():\n",
    "        print(key)\n",
    "        mask_rh = atlas[32492:] != 0\n",
    "        min_atlas = np.min(atlas[32492:][atlas[32492:] != 0])\n",
    "        max_atlas = np.max(atlas[32492:][atlas[32492:] != 0])\n",
    "        rh_data_parc[key] = map_to_labels(rh_data_parc[key].ravel()[1:], atlas[32492:],\n",
    "                                         mask=mask_rh, fill=np.nan,\n",
    "                                         source_lab=np.arange(min_atlas, max_atlas+1))\n",
    "        #rh_data_parc[key] = map_to_labels(rh_data_parc[key].ravel(), atlas[32492:],\n",
    "        #                                 mask=None, fill=np.nan,\n",
    "        #                                 source_lab=np.concatenate(([0], np.arange(min_atlas, max_atlas+1))))\n",
    "        \n",
    "    return lh_data_parc, rh_data_parc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_hemispheres_data(lh_data_parc, rh_data_parc):\n",
    "    \"\"\"\n",
    "    Combine left and right hemisphere data into grouped dictionaries and fill NaN values with means\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lh_data_parc : dict\n",
    "        Left hemisphere parcellation data\n",
    "    rh_data_parc : dict\n",
    "        Right hemisphere parcellation data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : Three dictionaries containing grouped layer data:\n",
    "            - layer_group_parc: Combined supra/gra/infra/total thickness data\n",
    "            - layer_ind_parc: Individual layer (L1-L6) thickness data  \n",
    "            - layer_extra_parc: Relative and ratio thickness metrics\n",
    "    \"\"\"\n",
    "    # Combine hemisphere data into groups\n",
    "    layer_group_parc = {\n",
    "        'supra': {'lh': lh_data_parc['supra'], 'rh': rh_data_parc['supra']},\n",
    "        'gra': {'lh': lh_data_parc['gra'], 'rh': rh_data_parc['gra']},\n",
    "        'infra': {'lh': lh_data_parc['infra'], 'rh': rh_data_parc['infra']},\n",
    "       #'infra_exvivo': {'lh': lh_data_parc['infra_exvivo'], 'rh': rh_data_parc['infra_exvivo']},\n",
    "        'total': {'lh': lh_data_parc['total'], 'rh': rh_data_parc['total']},\n",
    "    }\n",
    "\n",
    "    layer_ind_parc = {\n",
    "        'L1': {'lh': lh_data_parc['L1'], 'rh': rh_data_parc['L1']},\n",
    "        'L2': {'lh': lh_data_parc['L2'], 'rh': rh_data_parc['L2']},\n",
    "        'L3': {'lh': lh_data_parc['L3'], 'rh': rh_data_parc['L3']},\n",
    "        'L4': {'lh': lh_data_parc['L4'], 'rh': rh_data_parc['L4']},\n",
    "        'L5': {'lh': lh_data_parc['L5'], 'rh': rh_data_parc['L5']},\n",
    "        'L6': {'lh': lh_data_parc['L6'], 'rh': rh_data_parc['L6']},\n",
    "    }\n",
    "\n",
    "    layer_extra_parc = {\n",
    "        'relative': {'lh': lh_data_parc['relative'], 'rh': rh_data_parc['relative']},\n",
    "        'ratio': {'lh': lh_data_parc['ratio'], 'rh': rh_data_parc['ratio']}\n",
    "        #'ratio_exvivo': {'lh': lh_data_parc['ratio_exvivo'], 'rh': rh_data_parc['ratio_exvivo']},\n",
    "    }\n",
    "\n",
    "    # Fill NaN values with means\n",
    "    for key in layer_group_parc.keys():\n",
    "        for hemi in ['lh', 'rh']:\n",
    "            layer_group_parc[key][hemi][np.isnan(layer_group_parc[key][hemi])] = np.mean(layer_group_parc[key][hemi][~np.isnan(layer_group_parc[key][hemi])])\n",
    "\n",
    "    for key in layer_ind_parc.keys():\n",
    "        for hemi in ['lh', 'rh']:\n",
    "            layer_ind_parc[key][hemi][np.isnan(layer_ind_parc[key][hemi])] = np.mean(layer_ind_parc[key][hemi][~np.isnan(layer_ind_parc[key][hemi])])\n",
    "\n",
    "    for key in layer_extra_parc.keys():\n",
    "        for hemi in ['lh', 'rh']:\n",
    "            layer_extra_parc[key][hemi][np.isnan(layer_extra_parc[key][hemi])] = np.mean(layer_extra_parc[key][hemi][~np.isnan(layer_extra_parc[key][hemi])])\n",
    "\n",
    "    return layer_group_parc, layer_ind_parc, layer_extra_parc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process left and right hemisphere data\n",
    "np.save('/Users/dennis.jungchildmind.org/Library/CloudStorage/OneDrive-ChildMindInstitute/layer_project/cortical_layer/jupyter-notebook/lh_raw_data_bigbrain.npy', lh_data)\n",
    "np.save('/Users/dennis.jungchildmind.org/Library/CloudStorage/OneDrive-ChildMindInstitute/layer_project/cortical_layer/jupyter-notebook/rh_raw_data_bigbrain.npy', rh_data)\n",
    "\n",
    "lh_parc100, rh_parc100 = process_hemisphere_parcellation(process_hemisphere_data(lh_data,atlas_schaefer1000,'lh'),process_hemisphere_data(rh_data,atlas_schaefer1000,'rh'), atlas_schaefer1000)\n",
    "group_parc100, ind_parc100, extra_parc100 = combine_hemispheres_data(lh_parc100, rh_parc100)\n",
    "\n",
    "np.save('/Users/dennis.jungchildmind.org/Library/CloudStorage/OneDrive-ChildMindInstitute/layer_project/cortical_layer/jupyter-notebook/group_parc100_bigbrain.npy', group_parc100)\n",
    "np.save('/Users/dennis.jungchildmind.org/Library/CloudStorage/OneDrive-ChildMindInstitute/layer_project/cortical_layer/jupyter-notebook/extra_parc100_bigbrain.npy', extra_parc100)\n",
    "\n",
    "\n",
    "lh_parc400, rh_parc400 = process_hemisphere_parcellation(process_hemisphere_data(lh_data,atlas_schaefer400,'lh'),process_hemisphere_data(rh_data,atlas_schaefer400,'rh'), atlas_schaefer400)\n",
    "group_parc400, ind_parc400, extra_parc400 = combine_hemispheres_data(lh_parc400, rh_parc400)\n",
    "\n",
    "lh_parc1000, rh_parc1000 = process_hemisphere_parcellation(process_hemisphere_data(lh_data,atlas_schaefer1000,'lh'),process_hemisphere_data(rh_data,atlas_schaefer1000,'rh'), atlas_schaefer1000)\n",
    "group_parc1000, ind_parc1000, extra_parc1000 = combine_hemispheres_data(lh_parc1000, rh_parc1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to calculate correlation coefficient for group_parc400\n",
    "#combine data across all group_parcs by change no np.array to np.stack \n",
    "group_parc_all = np.stack([\n",
    "                           #left hemi supra\n",
    "                           group_parc400['supra']['lh'],\n",
    "                           group_parc400['infra']['lh'],\n",
    "                           group_parc400['total']['lh'],\n",
    "                           layer_group_data['supra']['lh'],\n",
    "                           layer_group_data['infra']['lh'],\n",
    "                           layer_group_data['total']['lh'],\n",
    "                           #right hemi supra\n",
    "                           group_parc400['supra']['rh'],\n",
    "                           group_parc400['infra']['rh'],\n",
    "                           group_parc400['total']['rh'],\n",
    "                           layer_group_data['supra']['rh'],\n",
    "                           layer_group_data['infra']['rh'],\n",
    "                           layer_group_data['total']['rh']])\n",
    "\n",
    "#perform correlation across the first dimension of group_parc_all\n",
    "# np.corrcoef will return a 12x12 correlation matrix since group_parc_all has shape (12, 32492)\n",
    "# Each row will be correlated with every other row\n",
    "tmp_corr = np.corrcoef(group_parc_all)\n",
    "#set diagonal to 0\n",
    "\n",
    "plt.imshow(tmp_corr, vmin=0, vmax=1, cmap='RdBu_r')\n",
    "#draw line at the middle xine and ylien\n",
    "plt.axvline(x=len(group_parc_all)/2-0.5, color='black', linestyle='-',linewidth=2)\n",
    "plt.axhline(y=len(group_parc_all)/2-0.5, color='black', linestyle='-',linewidth=2)\n",
    "#for x tick put s,i,g and repeat for y tick\n",
    "plt.xticks(np.arange(len(group_parc_all)), ['Supra', 'Infra', 'Total', 'Supra', 'Infra', 'Total']*2, fontsize=10, rotation=45)\n",
    "plt.yticks(np.arange(len(group_parc_all)), ['Supra', 'Infra', 'Total', 'Supra', 'Infra', 'Total']*2, fontsize=10, rotation=45)\n",
    "# Add labels for groups of 3 ticks\n",
    "\n",
    "\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Surface files and settings\n",
    "lh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.L.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "rh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.R.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "\n",
    "#data2plot = group_parc400#either layer_data or layer_group_data\n",
    "data2plot = extra_parc100\n",
    "# Plot each hemisphere\n",
    "for k in ['lh','rh']:\n",
    "    fig, axes = plt.subplots(2, len(data2plot), figsize=(len(data2plot)*3, 4))\n",
    "    surf = lh_surf if k=='lh' else rh_surf\n",
    "    \n",
    "    # Plot each layer\n",
    "    for i, layer_name in enumerate(data2plot.keys()):\n",
    "        data = data2plot[layer_name][k]\n",
    "        data_min = data.min()\n",
    "        data_max = data.max()\n",
    "        cmap = 'jet'\n",
    "        cbar_label = 'Thickness (mm)'\n",
    "        \n",
    "        if layer_name   ==  'relative':\n",
    "            data_min = 0.8\n",
    "            data_max = 1.2\n",
    "            cmap = 'RdBu_r'\n",
    "            cbar_label = 'Relative (supra/infra_exvivo)'\n",
    "        elif layer_name == 'ratio' or layer_name == 'ratio_exvivo':\n",
    "            data_min = 0.4\n",
    "            data_max = 0.6\n",
    "            cmap = 'RdBu_r'\n",
    "            cbar_label = 'Ratio (supra/total)'\n",
    "\n",
    "        data_masked = data.copy()\n",
    "        if k == 'lh':\n",
    "            data_masked[atlas_schaefer400[0:32492] == 0] = np.nan\n",
    "        else:\n",
    "            data_masked[atlas_schaefer400[32492:] == 0] = np.nan\n",
    "        data = data_masked\n",
    "        print('min:', f'{data_min:.3f}', 'max:', f'{data_max:.3f}')\n",
    "        norm = plt.Normalize(vmin=data_min, vmax=data_max)\n",
    "        #calculate mean and std of data\n",
    "        mean_data = data.mean()\n",
    "        std_data = data.std()\n",
    "        print('mean:', f'{mean_data:.3f}', '±', f'{std_data:.3f}')\n",
    "        # Plot lateral and medial views\n",
    "        for view_idx, view in enumerate(['lateral', 'medial']):\n",
    "            plotter = yaspy.Plotter(surf, hemi=k)\n",
    "            plotter.overlay(data, cmap=cmap, alpha=1, vmin=data_min, vmax=data_max)\n",
    "            plotter.border(data, alpha=0)\n",
    "            img = plotter.screenshot(view)\n",
    "            if len(data2plot) == 1:\n",
    "                axes[view_idx].imshow(img)\n",
    "                axes[view_idx].axis('off')\n",
    "            else:\n",
    "                axes[view_idx,i].imshow(img)\n",
    "                axes[view_idx,i].axis('off')\n",
    "            \n",
    "            if view_idx == 0:\n",
    "                if len(data2plot) == 1:\n",
    "                    axes[0].set_title(layer_name, fontweight='bold')\n",
    "                else:   \n",
    "                    axes[0,i].set_title(layer_name, fontweight='bold')\n",
    "            else:\n",
    "                if len(data2plot) == 1:\n",
    "                    divider = make_axes_locatable(axes[1])\n",
    "                else:\n",
    "                    divider = make_axes_locatable(axes[1,i])\n",
    "                cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.05)\n",
    "                cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                           cax=cax, orientation='horizontal')\n",
    "                cbar.set_label(cbar_label)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niwrap3912",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

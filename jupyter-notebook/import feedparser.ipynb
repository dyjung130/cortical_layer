import feedparser
import requests
from datetime import datetime, timedelta
import json
import os
from typing import List, Dict
import time

class NatureNeuroscienceRSSParser:
    def __init__(self, rss_url: str = "http://feeds.nature.com/neuro/rss/current"):
        self.rss_url = rss_url
        self.articles_file = "nature_neuroscience_articles.json"
        
    def fetch_and_parse_rss(self) -> List[Dict]:
        """
        Fetch and parse the RSS feed from Nature Neuroscience
        Returns a list of dictionaries containing article information
        """
        try:
            # Fetch the RSS feed
            feed = feedparser.parse(self.rss_url)
            
            articles = []
            
            # Parse each entry in the feed
            for entry in feed.entries:
                article = {
                    'title': self.clean_title(entry.title),
                    'link': entry.link,
                    'date': self.parse_date(entry.get('published', '')),
                    'summary': self.clean_summary(entry.get('summary', '')),
                    'authors': self.extract_authors(entry),
                    'doi': self.extract_doi(entry),
                    'fetched_on': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                articles.append(article)
            
            return articles
            
        except Exception as e:
            print(f"Error fetching RSS feed: {e}")
            return []
    
    def clean_title(self, title: str) -> str:
        """Remove CDATA tags and clean up the title"""
        if title:
            # Remove CDATA wrapper if present
            title = title.replace('<![CDATA[', '').replace(']]>', '')
            # Clean up any HTML tags
            import re
            title = re.sub(r'<[^>]+>', '', title)
        return title.strip()
    
    def clean_summary(self, summary: str) -> str:
        """Clean up the summary text"""
        if summary:
            # Remove CDATA wrapper if present
            summary = summary.replace('<![CDATA[', '').replace(']]>', '')
            # Remove HTML tags but keep the text
            import re
            summary = re.sub(r'<[^>]+>', ' ', summary)
            summary = re.sub(r'\s+', ' ', summary)  # Replace multiple spaces with single space
        return summary.strip()
    
    def parse_date(self, date_str: str) -> str:
        """Parse and format the publication date"""
        if not date_str:
            return ""
        
        # Try to parse different date formats
        try:
            # feedparser usually handles this well
            parsed_date = datetime.strptime(date_str[:10], '%Y-%m-%d')
            return parsed_date.strftime('%Y-%m-%d')
        except:
            return date_str
    
    def extract_authors(self, entry) -> List[str]:
        """Extract authors from the entry"""
        authors = []
        if hasattr(entry, 'authors'):
            for author in entry.authors:
                if hasattr(author, 'name'):
                    authors.append(author.name)
        return authors
    
    def extract_doi(self, entry) -> str:
        """Extract DOI from the entry"""
        # Try to find DOI in various fields
        doi = ""
        if hasattr(entry, 'prism_doi'):
            doi = entry.prism_doi
        elif hasattr(entry, 'id'):
            if 'doi:' in entry.id:
                doi = entry.id.replace('doi:', '')
        return doi
    
    def load_existing_articles(self) -> List[Dict]:
        """Load previously saved articles from JSON file"""
        if os.path.exists(self.articles_file):
            try:
                with open(self.articles_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return []
        return []
    
    def save_articles(self, articles: List[Dict]):
        """Save articles to JSON file"""
        try:
            with open(self.articles_file, 'w', encoding='utf-8') as f:
                json.dump(articles, f, indent=2, ensure_ascii=False)
            print(f"Articles saved to {self.articles_file}")
        except Exception as e:
            print(f"Error saving articles: {e}")
    
    def get_new_articles(self, current_articles: List[Dict], existing_articles: List[Dict]) -> List[Dict]:
        """Filter out articles that already exist"""
        existing_links = {article['link'] for article in existing_articles}
        new_articles = []
        
        for article in current_articles:
            if article['link'] not in existing_links:
                new_articles.append(article)
        
        return new_articles
    
    def display_articles(self, articles: List[Dict], title: str = "Articles"):
        """Display articles in a formatted way"""
        print(f"\n{'='*60}")
        print(f"{title} ({len(articles)} articles)")
        print(f"{'='*60}")
        
        for i, article in enumerate(articles, 1):
            print(f"\n{i}. {article['title']}")
            print(f"   Link: {article['link']}")
            print(f"   Date: {article['date']}")
            if article['authors']:
                authors_str = ', '.join(article['authors'][:3])  # Show first 3 authors
                if len(article['authors']) > 3:
                    authors_str += f" et al. ({len(article['authors'])} total authors)"
                print(f"   Authors: {authors_str}")
            if article['doi']:
                print(f"   DOI: {article['doi']}")
            if article['summary']:
                # Show first 200 characters of summary
                summary = article['summary'][:200] + "..." if len(article['summary']) > 200 else article['summary']
                print(f"   Summary: {summary}")
            print("-" * 60)
    
    def check_daily_updates(self):
        """Main method to check for daily updates"""
        print(f"Checking Nature Neuroscience RSS feed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Fetch current articles
        current_articles = self.fetch_and_parse_rss()
        
        if not current_articles:
            print("No articles found or error occurred while fetching.")
            return
        
        # Load existing articles
        existing_articles = self.load_existing_articles()
        
        # Find new articles
        new_articles = self.get_new_articles(current_articles, existing_articles)
        
        if new_articles:
            print(f"\nFound {len(new_articles)} new articles!")
            self.display_articles(new_articles, "NEW ARTICLES")
            
            # Combine and save all articles
            all_articles = new_articles + existing_articles
            self.save_articles(all_articles)
        else:
            print("No new articles found.")
        
        # Display all current articles
        self.display_articles(current_articles, "ALL CURRENT ARTICLES")
        
        return new_articles

def main():
    """Main function to run the RSS parser"""
    parser = NatureNeuroscienceRSSParser()
    
    # Check for updates
    new_articles = parser.check_daily_updates()
    
    # You can also save just the new articles to a separate file if needed
    if new_articles:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        new_articles_file = f"new_articles_{timestamp}.json"
        with open(new_articles_file, 'w', encoding='utf-8') as f:
            json.dump(new_articles, f, indent=2, ensure_ascii=False)
        print(f"\nNew articles also saved to {new_articles_file}")

def daily_check_with_schedule():
    """Function to run daily checks (you can use this with a scheduler)"""
    import schedule
    
    def job():
        parser = NatureNeuroscienceRSSParser()
        parser.check_daily_updates()
    
    # Schedule the job to run daily at 9 AM
    schedule.every().day.at("09:00").do(job)
    
    print("Scheduler started. Will check for new articles daily at 9:00 AM")
    print("Press Ctrl+C to stop...")
    
    try:
        while True:
            schedule.run_pending()
            time.sleep(3600)  # Check every hour
    except KeyboardInterrupt:
        print("\nScheduler stopped.")

if __name__ == "__main__":
    # Run once
    main()
    
    # Uncomment the line below if you want to run this as a daily scheduled task
    # daily_check_with_schedule()
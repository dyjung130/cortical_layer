{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for analyzing layer data \n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import pyvista as pv\n",
    "import yaspy\n",
    "from brainspace.gradient import GradientMaps\n",
    "from brainspace.gradient.kernels import compute_affinity\n",
    "import brainspace.gradient.alignment as ga\n",
    "from brainspace.utils.parcellation import map_to_labels\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pingouin as pg\n",
    "import pandas as pd\n",
    "\n",
    "do_glasser = False\n",
    "add_bigbrain_data = False\n",
    "do_parcellation = True#should be TRUE most of the time\n",
    "combine_hemispheres = False\n",
    "hcp_save_subpath = 'HCP'\n",
    "exvivo_save_subpath = 'EXVIVO'\n",
    "\n",
    "\n",
    "\n",
    "#for bigbrain data if it is included ...? in ex vivo data.\n",
    "baseDir_bigbrain = '/Users/dennis.jungchildmind.org/Downloads/BigBrain/thickness/resample/'\n",
    "\n",
    "\n",
    "if do_glasser:\n",
    "    atlas_data_lh = nib.load('/Users/dennis.jungchildmind.org/Library/CloudStorage/OneDrive-ChildMindInstitute/parcellation/glasser/Glasser_2016.32k.L.label.gii').darrays[0].data\n",
    "    atlas_data_rh = nib.load('/Users/dennis.jungchildmind.org/Library/CloudStorage/OneDrive-ChildMindInstitute/parcellation/glasser/Glasser_2016.32k.R.label.gii').darrays[0].data  \n",
    "    num_parcels = 180\n",
    "else:\n",
    "    #https://github.com/ThomasYeoLab/CBIG/blob/master/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal/Parcellations/HCP/fslr32k/cifti/Schaefer2018_200Parcels_7Networks_order.dscalar.nii\n",
    "    atlas_path = '/Users/dennis.jungchildmind.org/OneDrive - Child Mind Institute/parcellation/schaefer2018/Schaefer2018_400Parcels_7Networks_order.dlabel.nii'\n",
    "    atlas = nib.load(atlas_path).get_fdata()[0].astype(int)\n",
    "    #split atlas into left and right hemispheres for later use       \n",
    "    atlas_data_lh = atlas[:len(atlas)//2]\n",
    "    atlas_data_rh = atlas[len(atlas)//2:]\n",
    "    atlas_data_rh = atlas_data_rh - np.min(atlas_data_rh[atlas_data_rh != 0])+1\n",
    "    atlas_data_rh[atlas_data_rh <= 0] = 0\n",
    "    num_parcels = int(atlas_path.split('_')[1].split('Parcels')[0])  # Extract number of parcels from atlas path\n",
    "\n",
    "    yeo_networks = nib.load(atlas_path).header.get_axis(0)\n",
    "    yeo_network_data = yeo_networks.get_element(0)[1]\n",
    "\n",
    "    #each yeo_network_data[key] has ('17Networks_LH_VisCent_ExStr_1',...), save the third string after string split by '_'\n",
    "    yeo_network_names = []\n",
    "    yeo_network_colors = []\n",
    "    for i,key in enumerate(yeo_network_data):\n",
    "        if i == 0:\n",
    "            yeo_network_names.append(np.nan)\n",
    "            yeo_network_colors.append((0,0,0,0))\n",
    "            continue#skip the first key\n",
    "    # print(yeo_network_data[key][0])#network name\n",
    "        network_parts = yeo_network_data[key][0].split('_')\n",
    "        #print(network_parts)\n",
    "        yeo_network_names.append(network_parts[2])  # Append the network name (VisCent)\n",
    "        yeo_network_colors.append(yeo_network_data[key][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decompose_to_permutation_and_signs(R):\n",
    "    \"\"\"\n",
    "    Decompose orthogonal matrix R into permutation + sign flips\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    R : array (10, 10)\n",
    "        Orthogonal matrix from procrustes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    perm_idx : array (10,)\n",
    "        Permutation indices\n",
    "    signs : array (10,)\n",
    "        Sign flips (+1 or -1)\n",
    "    \"\"\"\n",
    "    n = R.shape[0]\n",
    "    \n",
    "    # Find the permutation by matching largest absolute values\n",
    "    # This finds which target component each source component maps to\n",
    "    abs_R = np.abs(R)\n",
    "    \n",
    "    # Use Hungarian algorithm to find optimal assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(-abs_R)\n",
    "    \n",
    "    # Extract signs from the matched elements\n",
    "    signs = np.sign(R[row_ind, col_ind])\n",
    "    \n",
    "    # col_ind gives us the permutation\n",
    "    perm_idx = col_ind\n",
    "    \n",
    "    return perm_idx, signs\n",
    "\n",
    "\n",
    "\n",
    "# Function to create eigenvalue plot\n",
    "def plot_eigenvalues(gm_aligned, color='#2E86C1', filename=None, save=False, SAVEFOLDER='./figures/eigenvalues'):\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))   \n",
    "    \n",
    "    # Calculate and plot normalized eigenvalues \n",
    "    eigenvalues = gm_aligned.lambdas_ / np.sum(gm_aligned.lambdas_)\n",
    "    ax.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'o-', \n",
    "            color=color, linewidth=2, markersize=6)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Component', fontsize=18)\n",
    "    ax.set_ylabel('Variance explained', fontsize=18)\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_xlim(0, len(eigenvalues))\n",
    "    ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        # Make folder if it doesn't exist\n",
    "        if not os.path.exists(SAVEFOLDER):\n",
    "            os.makedirs(SAVEFOLDER)\n",
    "        plt.savefig(os.path.join(SAVEFOLDER, f'{filename}_eigenvalues.png'), dpi=300)\n",
    "        plt.close()\n",
    "        return\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "\n",
    "#parcellation of the data based on the given atlas (atlas_data).\n",
    "def parcellate_data(data,atlas_data):\n",
    "    \"\"\"\n",
    "    data: numpy array\n",
    "    atlas_data: numpy array\n",
    "    hemisphere: string\n",
    "    \"\"\"\n",
    "\n",
    "    #parcellate data\n",
    "    len_unique = len(np.unique(atlas_data[atlas_data != 0]))\n",
    "    data_parc = np.zeros((len_unique))\n",
    "  \n",
    "    for i in range(len_unique):\n",
    "        parcel_data = data[atlas_data == i+1]\n",
    "        # First remove NaN values\n",
    "        parcel_data = parcel_data[~np.isnan(parcel_data)]\n",
    "        if len(parcel_data) == 0:\n",
    "            data_parc[i] = 0\n",
    "            continue\n",
    "        # Calculate mean and std of non-NaN values\n",
    "        parcel_data_mean = np.nanmean(parcel_data)\n",
    "        parcel_data_std = np.nanstd(parcel_data)\n",
    "        # Keep only values within 2 std of mean\n",
    "        # This keeps values that are strictly less than 2*std from the mean.\n",
    "        # If the intent is to include values *within* 2 std (inclusive), then <= should be used instead of <.\n",
    "        # Additionally, if the standard deviation is zero, this will remove all but the exact mean.\n",
    "        # This is a correct method to identify and filter outliers by the classic definition, but may not work as expected if parcel_data_std == 0 or parcel_data has few elements.\n",
    "\n",
    "        mask = np.abs(parcel_data - parcel_data_mean) < (2 * parcel_data_std)\n",
    "        filtered_data = parcel_data[mask]\n",
    "        # Calculate final mean of filtered data\n",
    "        if len(filtered_data) > 0:\n",
    "            data_parc[i] = np.nanmean(filtered_data)\n",
    "        else:\n",
    "            print(data[atlas_data == i+1])\n",
    "            print('no data',i)\n",
    "            data_parc[i] = 0\n",
    "    return data_parc\n",
    "\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(corr, transpose, title, vmin,vmax,cmap):\n",
    "    \"\"\"Plot correlation matrix with consistent formatting.\"\"\"\n",
    "    # Remove diagonal values\n",
    "    corr_plot = corr.copy()\n",
    "   # corr_plot[np.eye(corr_plot.shape[0], dtype=bool)] = np.nan\n",
    "    \n",
    "    # Plot matrix\n",
    "    im = plt.imshow(corr_plot, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        \n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.title(f'{title}',fontsize=16,fontweight='bold')\n",
    "    if transpose:\n",
    "        plt.xlabel('Subject',fontsize=16)\n",
    "        plt.ylabel('Subject',fontsize=16)\n",
    "    else:\n",
    "        plt.xlabel('Parcel',fontsize=16)\n",
    "        plt.ylabel('Parcel',fontsize=16)\n",
    "    \n",
    "    return corr\n",
    "\n",
    "def plot_subject_similarity(hemisphere_data, transpose=False, vmin = None, vmax = None, cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Plot similarity matrices between subjects for different thickness measures and radii.\n",
    "    \n",
    "    Args:\n",
    "        hemisphere_data (dict): Dictionary containing thickness data for one hemisphere\n",
    "        radii (list): List of radius values used for smoothing\n",
    "        transpose (bool): Whether to transpose the data matrices\n",
    "        partial_corr (bool): Whether to calculate partial correlations\n",
    "        shrink_it (bool): Whether to apply shrinkage to correlation calculation\n",
    "    \"\"\"\n",
    "    # Initialize correlation dictionaries\n",
    "    correlation_dicts = {\n",
    "        'total': {},\n",
    "        'infra': {},\n",
    "        'supra': {},\n",
    "        'relative': {},\n",
    "        'ratio_supra': {},\n",
    "        'ratio_infra': {}\n",
    "    }\n",
    "\n",
    "    first_index = 0\n",
    "    plt.figure(figsize=(len(correlation_dicts)*4,4))\n",
    "\n",
    "    # Define measures and their plot positions\n",
    "    measures = [\n",
    "        ('total', 'Total Thickness', 1),\n",
    "        ('infra', 'Infra Thickness', 2), \n",
    "        ('supra', 'Supra Thickness', 3),\n",
    "        ('relative', 'Relative (Supra/Infra)', 4),\n",
    "        ('ratio_supra', 'Ratio (Supra/Total)', 5),\n",
    "        ('ratio_infra', 'Ratio (Infra/Total)', 6)\n",
    "    ]\n",
    "    \n",
    "    # Plot each measure\n",
    "    for measure, title, plot_pos in measures:\n",
    "        plt.subplot(1, len(measures), plot_pos)\n",
    "        data = hemisphere_data[measure]\n",
    "\n",
    "        if transpose:\n",
    "            data = data[first_index:].T\n",
    "        else:\n",
    "            data = data[first_index:]\n",
    "\n",
    "\n",
    "        #do zscore\n",
    "       #data = zscore(data)\n",
    "\n",
    "        # Yes, np.corrcoef calculates Pearson correlation coefficients\n",
    "        \n",
    "        corr = np.corrcoef(data)\n",
    "\n",
    "        #do cosine similarity\n",
    "        #from sklearn.metrics.pairwise import cosine_similarity\n",
    "        #corr = cosine_similarity(data)\n",
    "\n",
    "        '''\n",
    "        from scipy.stats import kendalltau\n",
    "\n",
    "        # Calculate Kendall's tau for all pairs, building the matrix\n",
    "        n = data.shape[0]\n",
    "        corr = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                # kendalltau returns (correlation, p-value)\n",
    "                tau, _ = kendalltau(data[i], data[j])\n",
    "                corr[i, j] = tau\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        import dcor\n",
    "        # Compute a distance correlation matrix (n_samples x n_samples)\n",
    "        # Assume data.shape = (regions, subjects) if not transposed, else (subjects, regions)\n",
    "        if transpose:\n",
    "            n = data.shape[0]\n",
    "            corr = np.zeros((n, n))\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    corr[i, j] = dcor.distance_correlation(data[i, :], data[j, :])\n",
    "        else:\n",
    "            n = data.shape[0]\n",
    "            corr = np.zeros((n, n))\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    corr[i, j] = dcor.distance_correlation(data[i, :], data[j, :])\n",
    "        \n",
    "        '''\n",
    "        '''\n",
    "        from scipy.stats import spearmanr\n",
    "\n",
    "        # Calculate Spearman's rank correlation matrix\n",
    "        # spearmanr returns both correlation matrix and p-values matrix\n",
    "        corr, _ = spearmanr(data, axis=0 if transpose else 1)\n",
    "        # If corr is a scalar, expand to 2D for consistency\n",
    "        if np.isscalar(corr):\n",
    "            corr = np.array([[corr]])\n",
    "        '''\n",
    "        # Store and plot correlation matrix\n",
    "        correlation_dicts[measure][0] = plot_correlation_matrix(\n",
    "            corr,\n",
    "            transpose,\n",
    "            title,\n",
    "            vmin,\n",
    "            vmax,\n",
    "            cmap\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return tuple(correlation_dicts[k] for k in ['total', 'infra', 'supra', 'relative', 'ratio_supra', 'ratio_infra'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_subject_similarity_hcp(hemisphere_data, transpose=False, vmin = None, vmax = None, cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Plot similarity matrices between subjects for different thickness measures and radii.\n",
    "\n",
    "    Args:\n",
    "        hemisphere_data (dict): Dictionary containing thickness data for one hemisphere\n",
    "        radii (list): List of radius values used for smoothing\n",
    "        transpose (bool): Whether to transpose the data matrices\n",
    "        partial_corr (bool): Whether to calculate partial correlations\n",
    "        shrink_it (bool): Whether to apply shrinkage to correlation calculation\n",
    "    \"\"\"\n",
    "    # Initialize correlation dictionaries\n",
    "    correlation_dicts = {\n",
    "        'total': {},\n",
    "\n",
    "    }\n",
    "\n",
    "    first_index = 0\n",
    "    plt.figure(figsize=(len(correlation_dicts)*4,4))\n",
    "\n",
    "    # Define measures and their plot positions\n",
    "    measures = [\n",
    "        ('total', 'Total Thickness', 1),\n",
    "    ]\n",
    "\n",
    "    # Plot each measure\n",
    "    for measure, title, plot_pos in measures:\n",
    "        plt.subplot(1, len(measures), plot_pos)\n",
    "        data = hemisphere_data[measure]\n",
    "\n",
    "        if transpose:\n",
    "            data = data[first_index:].T\n",
    "        else:\n",
    "            data = data[first_index:]\n",
    "\n",
    "\n",
    "        #do zscore\n",
    "        #data = zscore(data)\n",
    "\n",
    "        # Yes, np.corrcoef calculates Pearson correlation coefficients\n",
    "        \n",
    "        corr = np.corrcoef(data)\n",
    "        '''\n",
    "        import dcor\n",
    "        # Compute a distance correlation matrix (n_samples x n_samples)\n",
    "        # Assume data.shape = (regions, subjects) if not transposed, else (subjects, regions)\n",
    "        if transpose:\n",
    "            n = data.shape[0]\n",
    "            corr = np.zeros((n, n))\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    corr[i, j] = dcor.distance_correlation(data[i, :], data[j, :])\n",
    "        else:\n",
    "            n = data.shape[0]\n",
    "            corr = np.zeros((n, n))\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    corr[i, j] = dcor.distance_correlation(data[i, :], data[j, :])\n",
    "        '''\n",
    "        '''\n",
    "        from scipy.stats import spearmanr\n",
    "\n",
    "        # Calculate Spearman's rank correlation matrix\n",
    "        # spearmanr returns both correlation matrix and p-values matrix\n",
    "        corr, _ = spearmanr(data, axis=0 if transpose else 1)\n",
    "        # If corr is a scalar, expand to 2D for consistency\n",
    "        if np.isscalar(corr):\n",
    "            corr = np.array([[corr]])\n",
    "        '''\n",
    "        # Store and plot correlation matrix\n",
    "        correlation_dicts[measure][0] = plot_correlation_matrix(\n",
    "            corr,\n",
    "            transpose,\n",
    "            title,\n",
    "            vmin,\n",
    "            vmax,\n",
    "            cmap\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return correlation_dicts['total']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load '/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm/at_pial_surface/output_960um_method0/output_120um_max_960um_dist_method0/I41_new_confidence/lh/pial_120um_method0_manual_raw_intensity.npz'\n",
    "tmp = np.load('/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm/at_pial_surface/output_960um_method0/output_120um_max_960um_dist_method0/I41_new_confidence/lh/pial_120um_method0_manual_raw_intensity.npz')\n",
    "print(tmp['all_values'].shape)\n",
    "# Get the value of all_values at the middle_index of dist_array\n",
    "dist_array = tmp['dist_array']\n",
    "middle_index = len(dist_array) // 2\n",
    "all_values_middle = tmp['all_values'][middle_index]\n",
    "print(\"Value of all_values at middle_index:\", all_values_middle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Main Analysis Block ===\n",
    "\n",
    "def clean_data(data_array):\n",
    "    \"\"\"Clean NaN and Inf values from array using masked arrays.\"\"\"\n",
    "    masked_array = np.ma.masked_invalid(data_array)\n",
    "    return np.array(masked_array.filled(0))\n",
    "\n",
    "def load_thickness_hcp_data(base_dir, subject_dir, hemi, suffix):\n",
    "    \"\"\"Load thickness measurements for HCP data.\"\"\"\n",
    "    file_stub = f\"{base_dir}{subject_dir}/{subject_dir}.\"\n",
    "    if hemi == 'lh':\n",
    "        file_path = file_stub + \"L.thickness.32k_6mm_fwhm_fs_LR.shape.gii\"\n",
    "    else:\n",
    "        file_path = file_stub + \"R.thickness.32k_6mm_fwhm_fs_LR.shape.gii\"\n",
    "    return {'total': nib.load(file_path).darrays[0].data}\n",
    "\n",
    "def load_thickness_exvivo_data(base_dir, subject_dir, hemi, suffix):\n",
    "    \"\"\"Load thickness measurements for ex vivo data.\"\"\"\n",
    "    return {\n",
    "        'infra': nib.load(f'{base_dir}{subject_dir}/{hemi}.thickness.wm.inf.{suffix}.shape.gii').darrays[0].data,\n",
    "        'supra': nib.load(f'{base_dir}{subject_dir}/{hemi}.thickness.inf.pial.{suffix}.shape.gii').darrays[0].data,\n",
    "        'total': nib.load(f'{base_dir}{subject_dir}/{hemi}.thickness.{suffix}.shape.gii').darrays[0].data\n",
    "    }\n",
    "\n",
    "def load_thickness_bigbrain_data(base_dir, hemi):\n",
    "    \"\"\"Load thickness measurements from BigBrain.\"\"\"\n",
    "    return {\n",
    "        'infra': nib.load(f'{base_dir}/{hemi}.3-6.32k.shape.gii').darrays[0].data,  # layers 4,5,6 (ex vivo style)\n",
    "        'supra': nib.load(f'{base_dir}/{hemi}.0-3.32k.shape.gii').darrays[0].data,  # layers 1,2,3\n",
    "        'total': nib.load(f'{base_dir}/{hemi}.0-6.32k.shape.gii').darrays[0].data   # all layers\n",
    "    }\n",
    "\n",
    "def calculate_derived_measurements(thickness_data):\n",
    "    \"\"\"Calculate derived thickness measurements.\"\"\"\n",
    "    return {\n",
    "        'relative': clean_data(np.divide(thickness_data['supra'], thickness_data['infra'],\n",
    "                                         out=np.zeros_like(thickness_data['supra']), where=thickness_data['infra'] != 0)),\n",
    "        'ratio_supra': clean_data(np.divide(thickness_data['supra'], thickness_data['total'],\n",
    "                                            out=np.zeros_like(thickness_data['supra']), where=thickness_data['total'] != 0)),\n",
    "        'ratio_infra': clean_data(np.divide(thickness_data['infra'], thickness_data['total'],\n",
    "                                            out=np.zeros_like(thickness_data['infra']), where=thickness_data['total'] != 0)),\n",
    "        'diff': clean_data((thickness_data['infra'] - thickness_data['supra']) /\n",
    "                           (thickness_data['infra'] + thickness_data['supra']))\n",
    "    }\n",
    "\n",
    "# --- Analysis Loop for HCP and Ex Vivo data ---\n",
    "for run_hcp_data in [True, False]:\n",
    "\n",
    "    # Set up directories\n",
    "    if run_hcp_data:\n",
    "        # For HCP\n",
    "        baseDir = '/Users/dennis.jungchildmind.org/OneDrive - Child Mind Institute/hcp1200-thickness/'\n",
    "        data_types = ['total']\n",
    "    else:\n",
    "        # For ex vivo\n",
    "        baseDir = '/Users/dennis.jungchildmind.org/Downloads/exvivo/'\n",
    "        # This setting is specifically for the ex vivo data\n",
    "        data_types = ['total', 'infra', 'supra', 'relative', 'ratio_supra', 'ratio_infra', 'diff']\n",
    "\n",
    "    hemispheres = {'lh': 'left', 'rh': 'right'}\n",
    "\n",
    "    # Get all subject directories in the baseDir\n",
    "    subject_dirs = [d for d in os.listdir(baseDir) if os.path.isdir(os.path.join(baseDir, d))]\n",
    "        \n",
    "    # Ex vivo-specific ordering of subjects\n",
    "    if not run_hcp_data:\n",
    "        # Move the subject containing 'I54' to the beginning of the list (should have both hemispheres)\n",
    "        l54_idx = next(i for i, s in enumerate(subject_dirs) if 'I54' in s)\n",
    "        print(f\"Index of L54: {l54_idx}\")\n",
    "        subject_dirs.insert(0, subject_dirs.pop(l54_idx))\n",
    "\n",
    "    # Initialize data dictionary for every hemisphere and type\n",
    "    data = {hemi: {dtype: [] for dtype in data_types} for hemi in hemispheres}\n",
    "\n",
    "    # Helper lists to keep track of included subject names\n",
    "    lh_subjects, rh_subjects = [], []\n",
    "    lh_subjects_name, rh_subjects_name = [], []\n",
    "\n",
    "    # Preallocate (optional usage)\n",
    "    all_values_middle_array = {hemi: {dtype: [] for dtype in data_types} for hemi in hemispheres}\n",
    "\n",
    "    # -------- Data Loading --------\n",
    "    for subjectDir in subject_dirs:\n",
    "        for hemi in hemispheres:\n",
    "            # Construct appropriate file path\n",
    "            if run_hcp_data:\n",
    "                surf_file = f\"{baseDir}{subjectDir}/{subjectDir}.{'L' if hemi == 'lh' else 'R'}.thickness.32k_6mm_fwhm_fs_LR.shape.gii\"\n",
    "            else:\n",
    "                surf_file = f'{baseDir}{subjectDir}/{hemi}.pial.32k_fs_LR.surf.gii'\n",
    "            \n",
    "            if os.path.exists(surf_file):\n",
    "                # Update subject lists\n",
    "                if hemi == 'lh':\n",
    "                    lh_subjects.append(subjectDir)\n",
    "                    lh_subjects_name.append(subjectDir.replace('_new_confidence', ''))\n",
    "                else:\n",
    "                    rh_subjects.append(subjectDir)\n",
    "                    rh_subjects_name.append(subjectDir.replace('_new_confidence', ''))\n",
    "\n",
    "                # Suffix is consistent for both\n",
    "                suffix = '32k_6mm_fwhm_fs_LR'\n",
    "\n",
    "                # Load and process thickness data for subject and hemisphere\n",
    "                if run_hcp_data:\n",
    "                    thickness_data = load_thickness_hcp_data(baseDir, subjectDir, hemi, suffix)\n",
    "                else:\n",
    "                    thickness_data = load_thickness_exvivo_data(baseDir, subjectDir, hemi, suffix)\n",
    "                    thickness_data.update(calculate_derived_measurements(thickness_data))\n",
    "\n",
    "                # Store data for each type efficiently using numpy\n",
    "                for dtype in data_types:\n",
    "                    reshaped = thickness_data[dtype].reshape(-1, 1)\n",
    "                    if len(data[hemi][dtype]) > 0:\n",
    "                        data[hemi][dtype] = np.concatenate((data[hemi][dtype], reshaped), axis=1)\n",
    "                    else:\n",
    "                        data[hemi][dtype] = reshaped\n",
    "\n",
    "    # -------- BigBrain Data (ex vivo only) --------\n",
    "    if add_bigbrain_data and not run_hcp_data:\n",
    "        for hemi in hemispheres:\n",
    "            thickness_data = load_thickness_bigbrain_data(baseDir_bigbrain, hemi)\n",
    "            thickness_data.update(calculate_derived_measurements(thickness_data))\n",
    "            for dtype in data_types:\n",
    "                reshaped = thickness_data[dtype].reshape(-1, 1)\n",
    "                if len(data[hemi][dtype]) > 0:\n",
    "                    data[hemi][dtype] = np.concatenate((data[hemi][dtype], reshaped), axis=1)\n",
    "                else:\n",
    "                    data[hemi][dtype] = reshaped\n",
    "        lh_subjects_name.append('bigbrain')\n",
    "        rh_subjects_name.append('bigbrain')\n",
    "\n",
    "    # -------- Combine Hemispheres if needed --------\n",
    "    # Combine before parcellation because left/right hemispheres may have different parcellations\n",
    "    if combine_hemispheres and not run_hcp_data:\n",
    "        for key in data['lh'].keys():\n",
    "            data['lh'][key] = np.concatenate((data['lh'][key], data['rh'][key]), axis=1)\n",
    "            print(data['lh'][key].shape)\n",
    "        data['rh'] = data['lh'].copy()\n",
    "        lh_subjects_name = lh_subjects_name + rh_subjects_name\n",
    "        rh_subjects_name = lh_subjects_name.copy()\n",
    "\n",
    "    # -------- Parcellation if specified --------\n",
    "    if do_parcellation:\n",
    "        for hemi in hemispheres:\n",
    "            # Choose the appropriate atlas_data for each hemisphere\n",
    "            atlas_data = atlas_data_lh if hemi == 'lh' else atlas_data_rh\n",
    "            for dtype in data_types:\n",
    "                print('hemi', hemi, 'dtype', dtype)\n",
    "                tmp_data = []\n",
    "                for i in range(data[hemi][dtype].shape[1]):\n",
    "                    print('i', i)\n",
    "                    parcellated = parcellate_data(data[hemi][dtype][:, i], atlas_data)\n",
    "                    tmp_data.append(parcellated)\n",
    "                data[hemi][dtype] = np.array(tmp_data).T\n",
    "\n",
    "    # -------- Assemble Final Data --------\n",
    "    lh_data_parc = data['lh']\n",
    "    rh_data_parc = data['rh']\n",
    "\n",
    "    # -------- Construct Structural Covariance Matrices --------\n",
    "    if run_hcp_data:\n",
    "        # For HCP data\n",
    "        total_hcp_corr_lh = plot_subject_similarity_hcp(lh_data_parc, transpose=False, vmin=-0.5, vmax=0.5, cmap='viridis')\n",
    "        total_hcp_corr_rh = plot_subject_similarity_hcp(rh_data_parc, transpose=False, vmin=-0.5, vmax=0.5, cmap='viridis')\n",
    "        # Save the correlation matrices\n",
    "        if not os.path.exists(hcp_save_subpath):\n",
    "            os.makedirs(hcp_save_subpath, exist_ok=True)\n",
    "        np.save(f'{hcp_save_subpath}/total_hcp_corr_lh.npy', total_hcp_corr_lh)\n",
    "        np.save(f'{hcp_save_subpath}/total_hcp_corr_rh.npy', total_hcp_corr_rh)\n",
    "    else:\n",
    "        # For ex vivo data\n",
    "        (total_corr_lh, infra_corr_lh, supra_corr_lh, relative_corr_lh,\n",
    "         ratio_supra_corr_lh, ratio_infra_corr_lh) = plot_subject_similarity(\n",
    "            lh_data_parc, transpose=False, vmin=-0.5, vmax=1, cmap='viridis'\n",
    "        )\n",
    "        (total_corr_rh, infra_corr_rh, supra_corr_rh, relative_corr_rh,\n",
    "         ratio_supra_corr_rh, ratio_infra_corr_rh) = plot_subject_similarity(\n",
    "            rh_data_parc, transpose=False, vmin=-0.5, vmax=1, cmap='viridis'\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(exvivo_save_subpath):\n",
    "            os.makedirs(exvivo_save_subpath, exist_ok=True)\n",
    "\n",
    "        # Save the correlation matrices\n",
    "        np.save(f'{exvivo_save_subpath}/total_corr_lh.npy', total_corr_lh)\n",
    "        np.save(f'{exvivo_save_subpath}/total_corr_rh.npy', total_corr_rh)\n",
    "        np.save(f'{exvivo_save_subpath}/infra_corr_lh.npy', infra_corr_lh)\n",
    "        np.save(f'{exvivo_save_subpath}/infra_corr_rh.npy', infra_corr_rh)\n",
    "        np.save(f'{exvivo_save_subpath}/supra_corr_lh.npy', supra_corr_lh)\n",
    "        np.save(f'{exvivo_save_subpath}/supra_corr_rh.npy', supra_corr_rh)\n",
    "        np.save(f'{exvivo_save_subpath}/relative_corr_lh.npy', relative_corr_lh)\n",
    "        np.save(f'{exvivo_save_subpath}/relative_corr_rh.npy', relative_corr_rh)\n",
    "        np.save(f'{exvivo_save_subpath}/ratio_supra_corr_lh.npy', ratio_supra_corr_lh)\n",
    "        np.save(f'{exvivo_save_subpath}/ratio_supra_corr_rh.npy', ratio_supra_corr_rh)\n",
    "        np.save(f'{exvivo_save_subpath}/ratio_infra_corr_lh.npy', ratio_infra_corr_lh)\n",
    "        np.save(f'{exvivo_save_subpath}/ratio_infra_corr_rh.npy', ratio_infra_corr_rh)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_gradients_from_brainspace(data2plot, mask_indices, atlas_data, hemisphere_mask, n_components, g_sparsity = 0.9):\n",
    "    \"\"\"Process gradient maps for one hemisphere\"\"\"\n",
    "    grad_all = []\n",
    "    \n",
    "    # Fit gradient maps\n",
    "    gm = GradientMaps(n_components, approach=G_dimension_reduction, kernel=G_kernel)\n",
    "    \n",
    "    if np.isnan(mask_indices).all():\n",
    "        print('no mask')\n",
    "        gm.fit(np.nan_to_num(data2plot, 0),sparsity = g_sparsity)#sparsity density is 0.9 by default\n",
    "    else:\n",
    "        mask = np.ones(data2plot.shape[0], dtype=bool)\n",
    "        mask[mask_indices] = False\n",
    "        gm.fit(data2plot, sparsity = g_sparsity)\n",
    "\n",
    "    # Process gradients\n",
    "    grad = []\n",
    "    \n",
    "    #for each gradient component..\n",
    "    for j in range(n_components):\n",
    "\n",
    "        data_len = len(data2plot)\n",
    "        if np.isnan(mask_indices).all():\n",
    "            tmp_gm = gm.gradients_[:,j]\n",
    "        else:\n",
    "            tmp_gm = np.full((data_len, 1), np.nan)\n",
    "            tmp_gm[mask] = gm.gradients_[:,j].reshape(-1,1)\n",
    "            tmp_gm = tmp_gm.ravel()\n",
    "            \n",
    "        atlas_slice = atlas_data\n",
    "        min_val = np.min(atlas_slice[atlas_slice != 0])\n",
    "        max_val = np.max(atlas_slice[atlas_slice != 0])\n",
    "        #print(f\"Atlas range: {min_val}-{max_val}\")\n",
    "\n",
    "        grad.append(map_to_labels(tmp_gm, atlas_slice, mask=hemisphere_mask, \n",
    "                                fill=np.nan))#, source_lab=np.arange(min_val,max_val+1)))\n",
    "\n",
    "    return gm, grad\n",
    "\n",
    "\n",
    "#this is the main funciton used for gradient alignment -092525 DJ\n",
    "def align_gradients(X,Y,reflection=False,rotation=False):\n",
    "    \"\"\"Align source gradients to target gradients\n",
    "        X: source gradients\n",
    "        Y: target gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(X) != len(Y):\n",
    "        raise ValueError(\"Lists must be same length\")\n",
    "    \n",
    "    #set nan to 0\n",
    "    X[np.isnan(X)] = 0\n",
    "    Y[np.isnan(Y)] = 0\n",
    "\n",
    "    print('X',X.shape)\n",
    "    print('Y',Y.shape)\n",
    "\n",
    "    #center the matrix\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    Y_centered = Y - np.mean(Y, axis=0)\n",
    "    R, _ = orthogonal_procrustes(X_centered, Y_centered)\n",
    "\n",
    "    if reflection == True and rotation == False:\n",
    "        sign_matrix = np.sign(np.diag(R))  \n",
    "        sign_transform = np.diag(sign_matrix)\n",
    "        X_sign_corrected = X_centered @ sign_transform + np.mean(Y, axis=0)\n",
    "        return X_sign_corrected, sign_transform\n",
    "    elif reflection == True and rotation == True:\n",
    "        X = X_centered @ R + np.mean(Y, axis=0)\n",
    "        return X, R\n",
    "    else:\n",
    "        return X, Y\n",
    "   \n",
    "\n",
    "def unmap_gradient(grad_all_aligned, mask_indices, atlas_data, hemisphere_mask, start_idx, end_idx):\n",
    "    \"\"\"Unmap gradients to atlas data\"\"\"\n",
    "    grad_all_unmapped = []\n",
    "    for i, gm in enumerate(gm_all):\n",
    "        grad = []\n",
    "        for j in range(N_components):\n",
    "            tmp_gm = gm.gradients_[:,j] if np.isnan(mask_indices).all() else \\\n",
    "                    np.full(len(data2plot[i]), np.nan)\n",
    "            \n",
    "            # Apply mask if needed\n",
    "            if not np.isnan(mask_indices).all():\n",
    "                mask = np.ones(data2plot[i].shape[0], dtype=bool)\n",
    "                mask[mask_indices] = False\n",
    "                tmp_gm[mask] = gm.gradients_[:,j]\n",
    "            \n",
    "            # Map to atlas labels\n",
    "            atlas_slice = atlas_data[start_idx:end_idx]\n",
    "            nonzero = atlas_slice[atlas_slice != 0]\n",
    "            grad.append(label_to_map(tmp_gm, atlas_slice, mask=hemisphere_mask,\n",
    "                                    fill=np.nan, source_lab=np.arange(nonzero.min(), nonzero.max()+1)))\n",
    "    return grad\n",
    "\n",
    "def map_gradients(gm_all, data2plot, mask_indices, atlas_data, hemisphere_mask, start_idx, end_idx):\n",
    "    \"\"\"Map gradients to atlas data\"\"\"\n",
    "    grad_all_aligned = []\n",
    "    for i, gm in enumerate(gm_all):\n",
    "        grad = []\n",
    "        for j in range(N_components):\n",
    "            # Get gradient data\n",
    "            tmp_gm = gm.gradients_[:,j] if np.isnan(mask_indices).all() else \\\n",
    "                    np.full(len(data2plot[i]), np.nan)\n",
    "            \n",
    "            # Apply mask if needed\n",
    "            if not np.isnan(mask_indices).all():\n",
    "                mask = np.ones(data2plot[i].shape[0], dtype=bool)\n",
    "                mask[mask_indices] = False\n",
    "                tmp_gm[mask] = gm.gradients_[:,j]\n",
    "            \n",
    "            # Map to atlas labels\n",
    "            atlas_slice = atlas_data[start_idx:end_idx]\n",
    "            nonzero = atlas_slice[atlas_slice != 0]\n",
    "            grad.append(map_to_labels(tmp_gm, atlas_slice, mask=hemisphere_mask,\n",
    "                                    fill=np.nan, source_lab=np.arange(nonzero.min(), nonzero.max()+1)))\n",
    "        grad_all_aligned.append(grad)\n",
    "    return grad_all_aligned\n",
    "\n",
    "\n",
    "def create_hemisphere_plots(grad_all_aligned, surf_file, hemi, N_components_plot,color_ranges):\n",
    "    \"\"\"Create plots for one hemisphere\"\"\"\n",
    "    plotters = []\n",
    "\n",
    "    for pc in range(N_components_plot):\n",
    "        plotter = yaspy.Plotter(surf_file, hemi=hemi)\n",
    "        #[0] is some radii parameter I used before.. \n",
    "        #m = np.max(np.abs(([grad_all_aligned[0][pc]])))\n",
    "        # Use percentiles but have the color scale centered at zero\n",
    "        data = grad_all_aligned[0][:,pc]\n",
    "        vmax = np.percentile(np.abs(data), 95)\n",
    "        vmin = -vmax\n",
    "        color_ranges[pc] = vmax\n",
    "        overlay = plotter.overlay(data, cmap='RdBu_r', alpha=1, vmin=vmin, vmax=vmax)\n",
    "        plotter.border(grad_all_aligned[0][:,pc], alpha=0)\n",
    "        plotters.append([plotter.screenshot(\"lateral\"), plotter.screenshot(\"medial\"), overlay])\n",
    "        \n",
    "    return plotters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient analysis parameters\n",
    "G_sparsity = 0.9#sparsity paramter for brainspace (Default is 0.9)\n",
    "global G_dimension_reduction, G_kernel\n",
    "G_dimension_reduction = 'dm'\n",
    "G_kernel = 'normalized_angle'\n",
    "\n",
    "lh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.L.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "rh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.R.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "\n",
    "N_components = 10\n",
    "N_components_plot = 10\n",
    "\n",
    "mask_lh = atlas_data_lh != 0\n",
    "mask_rh = atlas_data_rh != 0\n",
    "#egular_values, partial_values\n",
    "data2plot_lh = total_corr_lh[0]\n",
    "data2plot_rh = total_corr_rh[0]\n",
    "\n",
    "\n",
    "mask_index_lh = np.nan\n",
    "mask_index_rh = np.nan\n",
    "do_reflection = False\n",
    "do_rotation = False\n",
    "gm_all_lh, grad_lh = calculate_gradients_from_brainspace(data2plot_lh, mask_index_lh, atlas_data_lh, mask_lh, N_components,G_sparsity)\n",
    "gm_all_rh, grad_rh = calculate_gradients_from_brainspace(data2plot_rh, mask_index_rh, atlas_data_rh, mask_rh, N_components,G_sparsity)\n",
    "\n",
    "\n",
    "tmp_lh,R_lh = align_gradients(np.array(grad_lh).T, np.array(grad_lh).T, reflection=do_reflection, rotation=do_rotation)\n",
    "tmp_rh,R_rh = align_gradients(np.array(grad_rh).T, np.array(grad_lh).T, reflection=do_reflection, rotation=do_rotation)\n",
    "\n",
    "# Plot using Yaspy\n",
    "# Assume grad_all_aligned_hemi_rh (right hemi gradients) and grad_all_lh (left hemi gradients) are ready\n",
    "surf_file_lh = lh_surf  # Replace with correct surface file path for left hemi\n",
    "surf_file_rh = rh_surf  # Replace with correct surface file path for right hemi\n",
    "\n",
    "grad_this = {}\n",
    "\n",
    "grad_this[0] = tmp_lh\n",
    "# Plot left hemisphere gradients\n",
    "plotters_lh = create_hemisphere_plots(grad_this, surf_file_lh, hemi='lh', N_components_plot=N_components_plot, color_ranges=[0.1]*N_components_plot)\n",
    "# Plot right hemisphere gradients (use aligned gradients if desired)\n",
    "#plotters_rh, m_rh = create_hemisphere_plots(grad_all_aligned_hemi_rh, surf_file_rh, hemi='rh', N_components_plot=N_components_plot, color_ranges=[0.1]*N_components_plot)\n",
    "\n",
    "# Plot all PCAs (components) in two rows of subplots: first row lateral, second row medial\n",
    "num_pcas = N_components_plot  # or len(plotters_lh), as appropriate\n",
    "fig, axs = plt.subplots(2, num_pcas, figsize=(2*num_pcas, 4))  # two rows: lateral, medial\n",
    "\n",
    "for i in range(num_pcas):\n",
    "    # First row: lateral views\n",
    "    ax_lateral = axs[0, i] if num_pcas > 1 else axs[0]\n",
    "    ax_lateral.imshow(plotters_lh[i][0])  # 0 is lateral\n",
    "    ax_lateral.set_title(f'Component {i+1}', fontsize=11)\n",
    "    ax_lateral.axis('off')\n",
    "\n",
    "    # Second row: medial views\n",
    "    ax_medial = axs[1, i] if num_pcas > 1 else axs[1]\n",
    "    ax_medial.imshow(plotters_lh[i][1])  # 1 is medial\n",
    "  \n",
    "    ax_medial.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "grad_this[0] = tmp_rh\n",
    "# Plot left hemisphere gradients\n",
    "plotters_rh = create_hemisphere_plots(grad_this, surf_file_rh, hemi='rh', N_components_plot=N_components_plot, color_ranges=[0.1]*N_components_plot)\n",
    "# Plot right hemisphere gradients (use aligned gradients if desired)\n",
    "#plotters_rh, m_rh = create_hemisphere_plots(grad_all_aligned_hemi_rh, surf_file_rh, hemi='rh', N_components_plot=N_components_plot, color_ranges=[0.1]*N_components_plot)\n",
    "\n",
    "# Plot all PCAs (components) in a single row of subplots\n",
    "num_pcas = N_components_plot  # or len(plotters_lh), as appropriate\n",
    "fig, axs = plt.subplots(2, num_pcas, figsize=(2*num_pcas, 4))  # two rows: lateral, medial\n",
    "\n",
    "for i in range(num_pcas):\n",
    "    # First row: lateral views\n",
    "    ax_lateral = axs[0, i] if num_pcas > 1 else axs[0]\n",
    "    ax_lateral.imshow(plotters_rh[i][0])  # 0 is lateral\n",
    "    ax_lateral.set_title(f'Component {i+1}', fontsize=11)\n",
    "    ax_lateral.axis('off')\n",
    "    \n",
    "    # Second row: medial views\n",
    "    ax_medial = axs[1, i] if num_pcas > 1 else axs[1]\n",
    "    ax_medial.imshow(plotters_rh[i][1])  # 1 is medial\n",
    "    ax_medial.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#save tmp_lh and tmp_rh as the template gradient maps\n",
    "if run_hcp_data:\n",
    "    #save the template gradient maps\n",
    "    os.makedirs(hcp_save_subpath, exist_ok=True)\n",
    "    np.save(f'{hcp_save_subpath}/hcp_template_grad_lh_sparsity_{G_sparsity}.npy', tmp_lh)\n",
    "    np.save(f'{hcp_save_subpath}/hcp_template_grad_rh_sparsity_{G_sparsity}.npy', tmp_rh)\n",
    "    #save images of the template gradient maps\n",
    "    grad_this[0] = tmp_lh\n",
    "    plotters_lh = create_hemisphere_plots(grad_this, surf_file_lh, hemi='lh', N_components_plot=N_components_plot, color_ranges=[0.1]*N_components_plot)\n",
    "    grad_this[0] = tmp_rh\n",
    "    plotters_rh = create_hemisphere_plots(grad_this, surf_file_rh, hemi='rh', N_components_plot=N_components_plot, color_ranges=[0.1]*N_components_plot)\n",
    "    # Plot all PCAs (components) in a single row of subplots\n",
    "    num_pcas = N_components_plot  # or len(plotters_lh), as appropriate\n",
    "    fig, axs = plt.subplots(1, num_pcas, figsize=(2*num_pcas, 2))\n",
    "    for i in range(num_pcas):\n",
    "        ax = axs[i] if num_pcas > 1 else axs  # handle when only one axis\n",
    "        ax.imshow(plotters_lh[i][0])\n",
    "        ax.set_title(f'Component {i+1}', fontsize=11)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{hcp_save_subpath}/hcp_template_grad_lh_sparsity_{G_sparsity}.png')\n",
    "    plt.show()\n",
    "    #save right hemispere gradient maps\n",
    "    fig, axs = plt.subplots(1, num_pcas, figsize=(2*num_pcas, 2))\n",
    "    for i in range(num_pcas):\n",
    "        ax = axs[i] if num_pcas > 1 else axs  # handle when only one axis\n",
    "        ax.imshow(plotters_rh[i][0])\n",
    "        ax.set_title(f'Component {i+1}', fontsize=11)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{hcp_save_subpath}/hcp_template_grad_rh_sparsity_{G_sparsity}.png')\n",
    "    plt.show()\n",
    "    plot_eigenvalues(gm_all_lh, filename=f'hcp_template_grad_lh_eigenvalues_sparsity{G_sparsity}',save=True,SAVEFOLDER=hcp_save_subpath)\n",
    "    plot_eigenvalues(gm_all_rh, filename=f'hcp_template_grad_rh_eigenvalues_sparsity{G_sparsity}',save=True,SAVEFOLDER=hcp_save_subpath)\n",
    "    #close all plots\n",
    "    plt.close('all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed so code works for both LH and RH by ensuring correct subplot/metric assignment and variable names\n",
    "base_dir = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/schaefer400_gd'\n",
    "surf_type = 'mid'  # 'mid','pial','white'\n",
    "dist_type = 'gd'   # 'ed','gd'\n",
    "distance_label = 'Euclidean Distance' if dist_type == 'ed' else 'Geodesic Distance'\n",
    "cmap = 'gray_r'\n",
    "font_size = 20\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "# ---- Set hemisphere here ('lh' or 'rh')\n",
    "hemisphere = 'lh'  # <-- change this to 'rh' for right hemisphere\n",
    "dist_matrix = np.load(f'{base_dir}/{surf_type}_{hemisphere}/{dist_type}_matrix.npy')\n",
    "\n",
    "# List of (label, matrix, pretty_label) to plot -- for both left and right hemispheres\n",
    "metric_list_lh = [\n",
    "    ('Total', total_hcp_corr_lh[0], 'Total_HCP'),\n",
    "    ('Total', total_corr_lh[0], 'Total'),\n",
    "    ('Supra', supra_corr_lh[0], 'Supra'),\n",
    "    ('Infra', infra_corr_lh[0], 'Infra'),\n",
    "    ('Ratio Supra', ratio_supra_corr_lh[0], 'Ratio Supra'),\n",
    "    ('Ratio Infra', ratio_infra_corr_lh[0], 'Ratio Infra'),\n",
    "]\n",
    "metric_list_rh = [\n",
    "    ('Total', total_hcp_corr_rh[0], 'Total_HCP'),\n",
    "    ('Total', total_corr_rh[0], 'Total'),\n",
    "    ('Supra', supra_corr_rh[0], 'Supra'),\n",
    "    ('Infra', infra_corr_rh[0], 'Infra'),\n",
    "    ('Ratio Supra', ratio_supra_corr_rh[0], 'Ratio Supra'),\n",
    "    ('Ratio Infra', ratio_infra_corr_rh[0], 'Ratio Infra')\n",
    "]\n",
    "\n",
    "# Select metric list and axis holder based on hemisphere\n",
    "if hemisphere == 'lh':\n",
    "    n_metrics = len(metric_list_lh)\n",
    "    metric_list = metric_list_lh\n",
    "elif hemisphere == 'rh':\n",
    "    n_metrics = len(metric_list_rh)\n",
    "    metric_list = metric_list_rh\n",
    "else:\n",
    "    raise ValueError(\"hemisphere must be 'lh' or 'rh'\")\n",
    "\n",
    "fig, axs = plt.subplots(1, n_metrics, figsize=(5 * n_metrics, 6), squeeze=False)\n",
    "\n",
    "# --- COLLECT ALL HISTOGRAMS TO FIND GLOBAL VMAX ---\n",
    "all_hists = []\n",
    "xedges_all = []\n",
    "yedges_all = []\n",
    "structural_cov_flat_norm_list = []\n",
    "dist_matrix_flat_list = []\n",
    "\n",
    "for i, (metric_name, structural_covariance, pretty_label) in enumerate(metric_list):\n",
    "    mask = dist_matrix != 0\n",
    "    dist_matrix_flat = dist_matrix[mask]\n",
    "    structural_covariance_flat = structural_covariance[mask]\n",
    "    \n",
    "    # Normalize/clip to [-1, 1] as before\n",
    "    if np.nanmax(np.abs(structural_covariance_flat)) > 1.0:\n",
    "        structural_covariance_flat_norm = np.clip(\n",
    "            structural_covariance_flat / np.nanmax(np.abs(structural_covariance_flat)), -1, 1\n",
    "        )\n",
    "    else:\n",
    "        structural_covariance_flat_norm = np.clip(structural_covariance_flat, -1, 1)\n",
    "    \n",
    "    # Save for second loop\n",
    "    dist_matrix_flat_list.append(dist_matrix_flat)\n",
    "    structural_cov_flat_norm_list.append(structural_covariance_flat_norm)\n",
    "    \n",
    "    # Range for all plots (use same!)\n",
    "    range2d = [[np.min(dist_matrix[mask]), np.max(dist_matrix[mask])], [-1, 1]]\n",
    "    hist, xedges, yedges = np.histogram2d(\n",
    "        dist_matrix_flat, structural_covariance_flat_norm, bins=[100, 50], range=range2d\n",
    "    )\n",
    "    all_hists.append(hist)\n",
    "    xedges_all.append(xedges)\n",
    "    yedges_all.append(yedges)\n",
    "\n",
    "# Find global vmin/vmax for colorbar\n",
    "all_hist_values = np.concatenate([h.flatten() for h in all_hists])\n",
    "vmin = np.percentile(all_hist_values, 1)\n",
    "vmax = np.percentile(all_hist_values, 99)\n",
    "\n",
    "# --- PLOT WITH SHARED COLORBAR RANGE ---\n",
    "for i, (metric_name, structural_covariance, pretty_label) in enumerate(metric_list):\n",
    "    ax = axs[0, i]\n",
    "    X, Y = np.meshgrid(xedges_all[i], yedges_all[i])\n",
    "    pcm = ax.pcolormesh(X, Y, all_hists[i].T, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    cb = plt.colorbar(pcm, ax=ax)\n",
    "    \n",
    "\n",
    "    # --- Model fitting with linear vs exponential, AIC criterion, plus rho (Spearman) and significance ---\n",
    "    dist_flat = dist_matrix_flat_list[i]\n",
    "    sc_flat = structural_cov_flat_norm_list[i]\n",
    "    \n",
    "    # Remove any NaN or Inf values\n",
    "    valid_mask = np.isfinite(dist_flat) & np.isfinite(sc_flat)\n",
    "    dist_flat = dist_flat[valid_mask]\n",
    "    sc_flat = sc_flat[valid_mask]\n",
    "    \n",
    "    # Bin the data by distance for clearer fitting/plotting\n",
    "    bins = np.linspace(np.nanmin(dist_flat), np.nanmax(dist_flat), 40)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    digitized = np.digitize(dist_flat, bins)\n",
    "    means = np.array([np.nanmean(sc_flat[digitized == k]) for k in range(1, len(bins))])\n",
    "    valid = np.isfinite(means)\n",
    "    x_fit = bin_centers[valid]\n",
    "    y_fit = means[valid]\n",
    "    n = len(y_fit)\n",
    "    \n",
    "    if n < 4:  # Need at least 4 points to fit\n",
    "        print(f\"Not enough valid points for metric {metric_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Exponential decay: y = a * exp(-b * x) + c\n",
    "    # For negative decay, we expect a > 0, b > 0\n",
    "    def exp_func(x, a, b, c):\n",
    "        return a * np.exp(-b * x) + c\n",
    "    \n",
    "    # Linear: y = m*x + c\n",
    "    def lin_func(x, m, c):\n",
    "        return m * x + c\n",
    "    \n",
    "    # Try exponential fit with better initial guesses\n",
    "    exp_success = True\n",
    "    try:\n",
    "        # Better initial parameter estimates for exponential decay\n",
    "        # a: amplitude (difference between max and min)\n",
    "        # b: decay rate (estimate from data span)\n",
    "        # c: asymptote (approximate minimum value)\n",
    "        y_range = np.max(y_fit) - np.min(y_fit)\n",
    "        x_range = np.max(x_fit) - np.min(x_fit)\n",
    "        \n",
    "        # Initial guesses: a = range, b = 1/x_range (characteristic length), c = min value\n",
    "        p0_exp = [y_range, 1.0/x_range if x_range > 0 else 0.01, np.min(y_fit)]\n",
    "        \n",
    "        # Set bounds: a can be positive or negative, b should be positive for decay, c is free\n",
    "        bounds_exp = ([-np.inf, 0, -np.inf], [np.inf, np.inf, np.inf])\n",
    "        \n",
    "        popt_exp, pcov_exp = curve_fit(exp_func, x_fit, y_fit, p0=p0_exp, \n",
    "                                        bounds=bounds_exp, maxfev=10000)\n",
    "        y_pred_exp = exp_func(x_fit, *popt_exp)\n",
    "        residuals_exp = y_fit - y_pred_exp\n",
    "        rss_exp = np.sum(residuals_exp ** 2)\n",
    "        k_exp = 3\n",
    "        \n",
    "        if rss_exp == 0 or np.isnan(rss_exp) or rss_exp < 1e-10:\n",
    "            aic_exp = np.inf\n",
    "        else:\n",
    "            aic_exp = 2 * k_exp + n * np.log(rss_exp / n)\n",
    "            \n",
    "    except Exception as e:\n",
    "        exp_success = False\n",
    "        aic_exp = np.inf\n",
    "        y_pred_exp = np.full_like(y_fit, np.nan)\n",
    "        print(f\"Exponential fit failed for metric {metric_name}: {e}\")\n",
    "    \n",
    "    # Try linear fit\n",
    "    lin_success = True\n",
    "    try:\n",
    "        popt_lin, pcov_lin = curve_fit(lin_func, x_fit, y_fit)\n",
    "        y_pred_lin = lin_func(x_fit, *popt_lin)\n",
    "        residuals_lin = y_fit - y_pred_lin\n",
    "        rss_lin = np.sum(residuals_lin ** 2)\n",
    "        k_lin = 2\n",
    "        \n",
    "        if rss_lin == 0 or np.isnan(rss_lin) or rss_lin < 1e-10:\n",
    "            aic_lin = np.inf\n",
    "        else:\n",
    "            aic_lin = 2 * k_lin + n * np.log(rss_lin / n)\n",
    "            \n",
    "    except Exception as e:\n",
    "        lin_success = False\n",
    "        aic_lin = np.inf\n",
    "        y_pred_lin = np.full_like(y_fit, np.nan)\n",
    "        print(f\"Linear fit failed for metric {metric_name}: {e}\")\n",
    "    \n",
    "    # Choose better model based on AIC (lower is better)\n",
    "    # Also require AIC difference of at least 2 to be meaningful\n",
    "    aic_diff = np.abs(aic_exp - aic_lin)\n",
    "    \n",
    "    if not exp_success and not lin_success:\n",
    "        print(f\"Both fits failed for metric {metric_name}\")\n",
    "        continue\n",
    "    elif not exp_success:\n",
    "        fit_label = 'Linear fit'\n",
    "        fit_color = 'blue'\n",
    "        y_pred = y_pred_lin\n",
    "        chosen_aic = aic_lin\n",
    "    elif not lin_success:\n",
    "        fit_label = 'Exp fit'\n",
    "        fit_color = 'red'\n",
    "        y_pred = y_pred_exp\n",
    "        chosen_aic = aic_exp\n",
    "    elif aic_exp < aic_lin and aic_diff > 2:\n",
    "        fit_label = f'Exp fit (AIC={aic_exp:.1f})'\n",
    "        fit_color = 'red'\n",
    "        y_pred = y_pred_exp\n",
    "        chosen_aic = aic_exp\n",
    "    elif aic_lin < aic_exp and aic_diff > 2:\n",
    "        fit_label = f'Linear fit (AIC={aic_lin:.1f})'\n",
    "        fit_color = 'blue'\n",
    "        y_pred = y_pred_lin\n",
    "        chosen_aic = aic_lin\n",
    "    else:\n",
    "        # AIC difference too small, choose simpler model (linear)\n",
    "        fit_label = f'Linear fit (AIC~{aic_lin:.1f})'\n",
    "        fit_color = 'blue'\n",
    "        y_pred = y_pred_lin\n",
    "        chosen_aic = aic_lin\n",
    "    \n",
    "    \n",
    "    # Plot the binned means first\n",
    "    #ax.scatter(x_fit, y_fit, color='yellow', s=30, alpha=0.7, zorder=5, \n",
    "    #           edgecolors='black', linewidths=0.5, label='Binned means')\n",
    "    \n",
    "    # Plot the chosen fit\n",
    "    ax.plot(x_fit, y_pred, color=fit_color, lw=3, label=fit_label, zorder=6)\n",
    "    \n",
    "    # Calculate Spearman's rho and significance using ALL paired data (non-binned!)\n",
    "    rho, pval = spearmanr(dist_flat, sc_flat, nan_policy='omit')\n",
    "    \n",
    "    # Calculate R for the chosen fit\n",
    "    ss_res = np.sum((y_fit - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_fit - np.mean(y_fit)) ** 2)\n",
    "    r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "    \n",
    "    # Add legend with model selection and statistics\n",
    "    legend_str = f\"$R^2$={r_squared:.3f}\\n$\\\\rho$={rho:.3f}\\np={pval:.2e}\"\n",
    "    ax.text(0.05, 0.05, legend_str, transform=ax.transAxes, \n",
    "            fontsize=font_size-2, verticalalignment='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.2))\n",
    "    \n",
    "    # Labels and formatting\n",
    "    if i == 0:\n",
    "        cb.set_label('Counts', fontsize=font_size, rotation=270, labelpad=18)\n",
    "        ax.set_xlabel(distance_label, fontsize=font_size)\n",
    "        ax.set_ylabel(f'{hemisphere.upper()}\\n\\nStructural Covariance', fontsize=font_size)\n",
    "    else:\n",
    "        ax.set_xlabel(distance_label, fontsize=font_size)\n",
    "    \n",
    "    ax.set_title(pretty_label, fontsize=font_size)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='both', labelsize=font_size)\n",
    "    cb.ax.tick_params(labelsize=font_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient analysis parameters\n",
    "G_sparsity = [0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1]#sparsity paramter for brainspace (Default is 0.9)\n",
    "G_dimension_reduction = 'pca'\n",
    "G_kernel = 'normalized_angle'\n",
    "\n",
    "lh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.L.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "rh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.R.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "\n",
    "\n",
    "# Create dictionaries with simpler keys for data types\n",
    "metrics = ['total_hcp','total', 'supra', 'infra', 'ratio_supra', 'ratio_infra']\n",
    "N_components = 10\n",
    "N_components_plot = 3\n",
    "\n",
    "#paraemters for alignment\n",
    "align_across_hemi = False\n",
    "align_to_total = True\n",
    "#rotation \n",
    "do_rotation = True #says don't do rotation but for calculation of the \"reflection and reorder needt o be put True\"\n",
    "do_reflection = True\n",
    "\n",
    "#mask?\n",
    "mask_index_lh = np.nan\n",
    "mask_index_rh = np.nan\n",
    "\n",
    "# Initialize dictionaries for left and right hemispheres\n",
    "lh_data_all = {m: {} for m in metrics}\n",
    "rh_data_all = {m: {} for m in metrics}\n",
    "lh_aligned_all = {m: {} for m in metrics}\n",
    "rh_aligned_all = {m: {} for m in metrics}\n",
    "#rotation matrices\n",
    "R_lh_all = {m: {} for m in metrics}\n",
    "R_rh_all = {m: {} for m in metrics}\n",
    "R_lh_aligned_all = {m: {} for m in metrics}\n",
    "R_rh_aligned_all = {m: {} for m in metrics}\n",
    "\n",
    "\n",
    "for i,gs in enumerate(G_sparsity):\n",
    "    print('G_sparsity:',gs)\n",
    "    for metric in metrics:\n",
    "        print(metric)\n",
    "        # Get data for current metric\n",
    "        data2plot_lh = eval(f\"{metric}_corr_lh\")[0]\n",
    "        data2plot_rh = eval(f\"{metric}_corr_rh\")[0]\n",
    "        # Create hemisphere masks\n",
    "        atlasDat = atlas\n",
    "        mask_lh = atlas_data_lh != 0\n",
    "        mask_rh = atlas_data_rh != 0\n",
    "\n",
    "        # calculate gradients using brainspace toolbox\n",
    "        # make sure to adjust the G_sparsity parameter, which sets the sparsity for the affinity matrix\n",
    "        gm_all_lh, grad_all_lh = calculate_gradients_from_brainspace(data2plot_lh, mask_index_lh, atlas_data_lh, mask_lh, N_components, gs)\n",
    "        gm_all_rh, grad_all_rh = calculate_gradients_from_brainspace(data2plot_rh, mask_index_rh, atlas_data_rh, mask_rh, N_components, gs)\n",
    "        #def plot_eigenvalues(gm_aligned, color='#2E86C1',metrics=None,save=False,SAVEFOLDER='./figures/eigenvalues'):\n",
    "        #plot_eigenvalues(gm_all_lh, filename=f'{metric}_eigenvalues_lh_sparsity{G_sparsity}',save=True)\n",
    "        #plot_eigenvalues(gm_all_rh, filename=f'{metric}_eigenvalues_rh_sparsity{G_sparsity}',save=True)\n",
    "        \n",
    "    # if align_across_hemi:\n",
    "        if align_across_hemi:\n",
    "            print('aligned across hemi')\n",
    "            #align left hemisphere to itself (so that the data format is the same)\n",
    "            grad_all_aligned_hemi_lh, R_lh = align_gradients(np.array(grad_all_lh).T, np.array(grad_all_lh).T, reflection=do_reflection, rotation=do_rotation)\n",
    "            #align right hemisphere to left hemisphere\n",
    "            grad_all_aligned_hemi_rh, R_rh = align_gradients(np.array(grad_all_rh).T, np.array(grad_all_lh).T, reflection=do_reflection, rotation=do_rotation)\n",
    "            \n",
    "        else:\n",
    "            print('not aligned across hemi')\n",
    "            grad_all_aligned_hemi_lh, R_lh = align_gradients(np.array(grad_all_lh).T, np.array(grad_all_lh).T, reflection=do_reflection, rotation=do_rotation)\n",
    "            grad_all_aligned_hemi_rh, R_rh = align_gradients(np.array(grad_all_rh).T, np.array(grad_all_rh).T, reflection=do_reflection, rotation=do_rotation)\n",
    "\n",
    "        #declare variables\n",
    "        grad_all_aligned_lh = []\n",
    "        grad_all_aligned_rh = []\n",
    "        R_lh_hcp = []\n",
    "        R_rh_hcp = []\n",
    "\n",
    "        #align to total\n",
    "        if align_to_total:\n",
    "            #if align to total flag is on, make alignment for all other metrics, otherwise don't.\n",
    "            if metric != 'total' and metric != 'total_hcp':\n",
    "                print('aligning to total')\n",
    "                _ , R_lh = align_gradients(grad_all_aligned_hemi_lh, lh_data_all['total'][0],reflection=do_reflection, rotation=do_rotation)\n",
    "                _ , R_rh = align_gradients(grad_all_aligned_hemi_rh, rh_data_all['total'][0],reflection=do_reflection, rotation=do_rotation)\n",
    "                \n",
    "                #decompse the transformation matrix (R) to permultation index and signs 102725 DJ\n",
    "                perm_idx_lh, signs_lh = decompose_to_permutation_and_signs(R_lh)\n",
    "                perm_idx_rh, signs_rh = decompose_to_permutation_and_signs(R_rh)\n",
    "                \n",
    "                #reorder and flip the signs of the gradientmaps 102725 DJ\n",
    "                grad_all_aligned_lh = grad_all_aligned_hemi_lh[:,perm_idx_lh]*signs_lh\n",
    "                grad_all_aligned_rh = grad_all_aligned_hemi_rh[:,perm_idx_rh]*signs_rh\n",
    "        \n",
    "            else:\n",
    "        \n",
    "                grad_all_aligned_lh, R_lh = align_gradients(grad_all_aligned_hemi_lh, grad_all_aligned_hemi_lh,reflection=do_reflection, rotation=do_rotation)\n",
    "                #align righ hemishere to lh\n",
    "                grad_all_aligned_rh, R_rh = align_gradients(grad_all_aligned_hemi_rh, grad_all_aligned_hemi_rh,reflection=do_reflection, rotation=do_rotation)\n",
    "\n",
    "\n",
    "\n",
    "        # Store aligned to \"total\" (sign_flip + reordering components)\n",
    "        lh_aligned_all[metric][i] = grad_all_aligned_lh\n",
    "        rh_aligned_all[metric][i] = grad_all_aligned_rh\n",
    "        R_lh_aligned_all[metric][i] = R_lh_hcp\n",
    "        R_rh_aligned_all[metric][i] = R_rh_hcp\n",
    "\n",
    "\n",
    "        #Aligned across hemispehres\n",
    "        lh_data_all[metric][i] = grad_all_aligned_hemi_lh\n",
    "        rh_data_all[metric][i] = grad_all_aligned_hemi_rh\n",
    "        R_lh_all[metric][i] = R_lh\n",
    "        R_rh_all[metric][i] = R_rh\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_component_correlation(\n",
    "    data,\n",
    "    n_components=3,\n",
    "    sparsity_index=0,#first one in G_sparsity\n",
    "    vmin=-0.8,\n",
    "    vmax=0.8,\n",
    "    figsize_per_component=(3,3),\n",
    "    colormap='RdBu_r'\n",
    "):\n",
    "    \"\"\"\n",
    "    For each component up to n_components, collect the component for each metric in the aligned gradients dictionary,\n",
    "    compute their correlation matrix, and plot them in a single figure in columns (single row).\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary of aligned gradients; each key is a metric, each value is a list;\n",
    "                     the first item (index 0) is assumed to be an array with at least two dimensions,\n",
    "                     with columns representing components.\n",
    "        n_components (int): Number of components to plot.\n",
    "        vmin (float): Minimum value for color scale.\n",
    "        vmax (float): Maximum value for color scale.\n",
    "        figsize_per_component (tuple): Size of each subplot (width, height).\n",
    "        colormap (str): matplotlib colormap.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def label_format(key):\n",
    "        \"\"\"Format keys for LaTeX axis labels: main_subscript\"\"\"\n",
    "        if '_' in key:\n",
    "            main, sub = key.split('_', 1)\n",
    "            return fr\"${main.capitalize()}_{{\\mathrm{{{sub}}}}}$\"\n",
    "        else:\n",
    "            return fr\"${key.capitalize()}$\"\n",
    "\n",
    "    keys = list(data.keys())\n",
    "    labels = [label_format(key) for key in keys]\n",
    "    n_metrics = len(keys)\n",
    "\n",
    "    # Output the shape for debugging\n",
    "    sample_shape = data[keys[0]][0].shape\n",
    "    print(f\"Example aligned gradients array shape (should be n_vertices x n_components): {sample_shape}\")\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1, n_components,\n",
    "        figsize=(n_components * figsize_per_component[0], figsize_per_component[1]),\n",
    "        squeeze=False\n",
    "    )\n",
    "\n",
    "    # Get global vmin/vmax for all correlation matrices, if you want consistent colorbars (optional)\n",
    "    # Otherwise, you can just use the provided vmin/vmax for all\n",
    "    for component in range(n_components):\n",
    "        # Extract the specified component from each metric\n",
    "        try:\n",
    "            comp_matrix = np.column_stack([\n",
    "                data[key][sparsity_index][:, component] for key in keys\n",
    "            ])\n",
    "        except IndexError as e:\n",
    "            print(f\"Component {component} is out of bounds for available gradient array shape {sample_shape}.\")\n",
    "            raise e\n",
    "\n",
    "        print(f\"Component matrix shape for component {component}: {comp_matrix.shape}\")\n",
    "\n",
    "        corrmat = np.corrcoef(comp_matrix.T)\n",
    "        np.fill_diagonal(corrmat, np.nan)\n",
    "\n",
    "        ax = axes[0, component]\n",
    "        im = ax.imshow(corrmat, cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "        ax.set_xticks(np.arange(n_metrics))\n",
    "        ax.set_yticks(np.arange(n_metrics))\n",
    "        ax.set_xticklabels(labels, rotation=90, ha='center')\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.set_title(f'Component {component+1}')\n",
    "        \n",
    "        # Only add colorbar to each subplot (or just the last one, as preferred)\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage: plots the first `n_components` components in `lh_aligned_all` as a single row of subplots\n",
    "gs_sparsity = 0.9\n",
    "sparsity_index = [i for i, x in enumerate(G_sparsity) if x == gs_sparsity][0]\n",
    "plot_component_correlation(lh_aligned_all, n_components=3, sparsity_index=sparsity_index, colormap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this combines all pc components and pltos in the same figure (only one data type)\n",
    "def plot_component_correlation(\n",
    "    data,\n",
    "    layer_type='total',\n",
    "    component = 0,\n",
    "    G_sparsity=G_sparsity,\n",
    "    vmin=-0.9,\n",
    "    vmax=0.9,\n",
    "    figsize_per_component=(3,3),\n",
    "    colormap='RdBu_r'\n",
    "):\n",
    "    \"\"\"\n",
    "    For each component up to n_components, collect the component for each metric in the aligned gradients dictionary,\n",
    "    compute their correlation matrix, and plot them in a single figure in columns (single row).\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary of aligned gradients; each key is a metric, each value is a list;\n",
    "                     the first item (index 0) is assumed to be an array with at least two dimensions,\n",
    "                     with columns representing components.\n",
    "        n_components (int): Number of components to plot.\n",
    "        vmin (float): Minimum value for color scale.\n",
    "        vmax (float): Maximum value for color scale.\n",
    "        figsize_per_component (tuple): Size of each subplot (width, height).\n",
    "        colormap (str): matplotlib colormap.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def label_format(key):\n",
    "        \"\"\"Format keys for LaTeX axis labels: main_subscript\"\"\"\n",
    "        if '_' in key:\n",
    "            main, sub = key.split('_', 1)\n",
    "            return fr\"${main.capitalize()}_{{\\mathrm{{{sub}}}}}$\"\n",
    "        else:\n",
    "            return fr\"${key.capitalize()}$\"\n",
    "\n",
    "    keys = [layer_type]\n",
    "    labels = [layer_type]\n",
    "    n_metrics = len(keys)\n",
    "    \n",
    "    # Output the shape for debugging\n",
    "    #sample_shape = data[keys[0]][0].shape\n",
    "    #print(f\"Example aligned gradients array shape (should be n_vertices x n_components): {sample_shape}\")\n",
    "    fig, ax = plt.subplots(\n",
    "        1, 1,\n",
    "        figsize=( figsize_per_component[0], figsize_per_component[1])\n",
    "    )\n",
    "   \n",
    "    comp_matrix =[]#set this to HCP data\n",
    "    comp_matrix = data['total_hcp'][0][:,component]#this is the first sparsity (0.9)\n",
    "    layer_data = data[layer_type]\n",
    "\n",
    "    for sparsity_index in range(len(G_sparsity)):\n",
    "\n",
    "        # Extract the specified component from each *metric* for this sparsity and stack\n",
    "        try:\n",
    "            #if len(comp_matrix) == 0:\n",
    "                #this should be the first sparsity which is at 0.9 * \n",
    "            #    comp_matrix = layer_data[sparsity_index][:, component][:, None]\n",
    "\n",
    "            #else:\n",
    "\n",
    "            #for the sparsity other than 0.9 align the gradient maps to the first sparsity (0.9)\n",
    "            _ , R= align_gradients(layer_data[sparsity_index], layer_data[0],reflection=do_reflection, rotation=do_rotation)\n",
    "            \n",
    "            #decompse the transformation matrix (R) to permultation index and signs 102725 DJ\n",
    "            perm_idx, signs= decompose_to_permutation_and_signs(R)\n",
    "            \n",
    "            #reorder and flip the signs of the gradientmaps 102725 DJ\n",
    "            grad_aligned_ = layer_data[sparsity_index][:,perm_idx]*signs\n",
    "\n",
    "\n",
    "            #comp_matrix = np.column_stack((comp_matrix, layer_data[sparsity_index][:, component]))\n",
    "            comp_matrix = np.column_stack((comp_matrix, grad_aligned_[:,component]))\n",
    "                \n",
    "        except IndexError as e:\n",
    "            print(f\"Component {component} is out of bounds for available gradient arrays. Sample shape: {sample_shape}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    #align gradients\n",
    "    corrmat = np.corrcoef(comp_matrix.T)\n",
    "    np.fill_diagonal(corrmat, np.nan)\n",
    "\n",
    "    im = ax.imshow(corrmat, cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_xticks(np.arange(len(G_sparsity)+1))\n",
    "    ax.set_yticks(np.arange(len(G_sparsity)+1))\n",
    "    label_name_list = ['HCP 0.9'] + [f\"EX {g}\" for g in G_sparsity]\n",
    "    ax.set_xticklabels(label_name_list, rotation=90, ha='center')\n",
    "    ax.set_yticklabels(label_name_list)\n",
    "    ax.set_title(f'Component {component+1}')\n",
    "    \n",
    "    # Only add colorbar to each subplot (or just the last one, as preferred)\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.title(layer_type.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage: plots the first `n_components` components in `lh_aligned_all` as a single row of subplots\n",
    "LAYER_TYPE = 'total'\n",
    "plot_component_correlation(lh_aligned_all, layer_type=LAYER_TYPE,component = 0,G_sparsity=G_sparsity, colormap='RdBu_r')\n",
    "plot_component_correlation(lh_aligned_all, layer_type=LAYER_TYPE,component = 1,G_sparsity=G_sparsity, colormap='RdBu_r')\n",
    "plot_component_correlation(lh_aligned_all, layer_type=LAYER_TYPE,component = 2,G_sparsity=G_sparsity, colormap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lh = atlas_data_lh != 0\n",
    "mask_rh = atlas_data_rh != 0\n",
    "\n",
    "hemi = 'lh'# lh or rh\n",
    "N_components = 10\n",
    "\n",
    "do_rotation = True\n",
    "do_reflection = True\n",
    "\n",
    "if hemi == 'lh':\n",
    "    total_corr = total_corr_lh[0]\n",
    "    supra_corr = supra_corr_lh[0]\n",
    "    infra_corr = infra_corr_lh[0]\n",
    "    relative_corr = relative_corr_lh[0]\n",
    "    ratio_supra_corr = ratio_supra_corr_lh[0]\n",
    "    ratio_infra_corr = ratio_infra_corr_lh[0]\n",
    "    mask_index = mask_index_lh\n",
    "    atlas_data = atlas_data_lh\n",
    "    mask = mask_lh\n",
    "else:\n",
    "    total_corr = total_corr_rh[0]\n",
    "    supra_corr = supra_corr_rh[0]\n",
    "    infra_corr = infra_corr_rh[0]\n",
    "    relative_corr = relative_corr_rh[0]\n",
    "    ratio_supra_corr = ratio_supra_corr_rh[0]\n",
    "    ratio_infra_corr = ratio_infra_corr_rh[0]\n",
    "    mask_index = mask_index_rh\n",
    "    atlas_data = atlas_data_rh\n",
    "    mask = mask_rh\n",
    "\n",
    "gm_total, grad_total = calculate_gradients_from_brainspace(total_corr, mask_index, atlas_data, mask, N_components, G_sparsity)\n",
    "gm_supra, grad_supra = calculate_gradients_from_brainspace(supra_corr, mask_index, atlas_data, mask, N_components, G_sparsity)\n",
    "gm_infra, grad_infra = calculate_gradients_from_brainspace(infra_corr, mask_index, atlas_data, mask, N_components, G_sparsity)\n",
    "gm_relative, grad_relative = calculate_gradients_from_brainspace(relative_corr, mask_index, atlas_data, mask, N_components, G_sparsity)\n",
    "gm_ratio_supra, grad_ratio_supra  = calculate_gradients_from_brainspace(ratio_supra_corr, mask_index, atlas_data, mask, N_components, G_sparsity)\n",
    "gm_ratio_infra, grad_ratio_infra = calculate_gradients_from_brainspace(ratio_infra_corr, mask_index, atlas_data, mask, N_components, G_sparsity)\n",
    "\n",
    "#convert to numpy arrays and trasnpose\n",
    "supra_grad = np.array(grad_supra).T\n",
    "infra_grad = np.array(grad_infra).T\n",
    "total_grad = np.array(grad_total).T\n",
    "relative_grad = np.array(grad_relative).T\n",
    "ratio_supra_grad = np.array(grad_ratio_supra).T\n",
    "ratio_infra_grad = np.array(grad_ratio_infra).T\n",
    "\n",
    "#R is (A @ R ) - B, \n",
    "_, R_total = align_gradients(total_grad, total_grad, reflection=do_reflection, rotation=do_rotation)\n",
    "_, R_supra = align_gradients(supra_grad, total_grad, reflection=do_reflection, rotation=do_rotation)\n",
    "_, R_infra = align_gradients(infra_grad, total_grad, reflection=do_reflection, rotation=do_rotation)\n",
    "_, R_relative= align_gradients(relative_grad, total_grad, reflection=do_reflection, rotation=do_rotation)\n",
    "_, R_ratio_supra = align_gradients(ratio_supra_grad, total_grad, reflection=do_reflection, rotation=do_rotation)\n",
    "_, R_ratio_infra = align_gradients(ratio_infra_grad, total_grad, reflection=do_reflection, rotation=do_rotation)\n",
    "\n",
    "R_matrices = [\n",
    "    (\"Supra\", R_supra),\n",
    "    (\"Infra\", R_infra),\n",
    "    (\"Relative\", R_relative),\n",
    "    (\"Ratio Supra\", R_ratio_supra),\n",
    "    (\"Ratio Infra\", R_ratio_infra)\n",
    "]\n",
    "\n",
    "num_R = len(R_matrices)\n",
    "\n",
    "# Match cross-correlation subplot style: 1 row, num_R columns, (4*num_R, 4) figsize\n",
    "fig, axs = plt.subplots(1, num_R, figsize=(4*num_R, 4))\n",
    "if num_R == 1:\n",
    "    axs = [axs]\n",
    "for i, (ax, (title, R_mat)) in enumerate(zip(axs, R_matrices)):\n",
    "    im = ax.imshow(R_mat, vmin=-1, vmax=1, cmap='bwr')\n",
    "    n_comp_x = R_mat.shape[1]\n",
    "    n_comp_y = R_mat.shape[0]\n",
    "    ax.set_xlabel('Target PC', fontsize=15)\n",
    "    ax.set_ylabel('Source PC', fontsize=15)\n",
    "    ax.set_title(f'{title} alignment\\nrotation/reflection', fontsize=15)\n",
    "    # Index ticks from 1 for columns/rows\n",
    "    ax.set_xticks(np.arange(n_comp_x))\n",
    "    ax.set_xticklabels(np.arange(1, n_comp_x+1), fontsize=11)\n",
    "    ax.set_yticks(np.arange(n_comp_y))\n",
    "    ax.set_yticklabels(np.arange(1, n_comp_y+1), fontsize=11)\n",
    "    # Annotate each cell\n",
    "    for y in range(n_comp_y):\n",
    "        for x in range(n_comp_x):\n",
    "            val = R_mat[y, x]\n",
    "            text_color = 'w' if abs(val) > 0.5 else 'black'\n",
    "            ax.text(x, y, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=text_color, fontsize=8)\n",
    "    # Only add colorbar to the last subplot for a cleaner look\n",
    "    if i == num_R - 1:\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "perm_idx_supra, signs_supra = decompose_to_permutation_and_signs(R_supra)\n",
    "perm_idx_infra, signs_infra = decompose_to_permutation_and_signs(R_infra)\n",
    "perm_idx_relative, signs_relative = decompose_to_permutation_and_signs(R_relative)\n",
    "perm_idx_ratio_supra, signs_ratio_supra = decompose_to_permutation_and_signs(R_ratio_supra)\n",
    "perm_idx_ratio_infra, signs_ratio_infra = decompose_to_permutation_and_signs(R_ratio_infra)\n",
    "\n",
    "print('supra',  perm_idx_supra+1, signs_supra)\n",
    "print('infra', perm_idx_infra+1, signs_infra)\n",
    "print('relative', perm_idx_relative+1, signs_relative)\n",
    "print('ratio_supra', perm_idx_ratio_supra+1, signs_ratio_supra)\n",
    "print('ratio_infra', perm_idx_ratio_infra+1, signs_ratio_infra)\n",
    "\n",
    "source_aligned_supra = supra_grad[:,perm_idx_supra]*signs_supra\n",
    "source_aligned_infra = infra_grad[:,perm_idx_infra]*signs_infra\n",
    "source_aligned_relative = relative_grad[:,perm_idx_relative]*signs_relative\n",
    "source_aligned_ratio_supra = ratio_supra_grad[:,perm_idx_ratio_supra]*signs_ratio_supra\n",
    "source_aligned_ratio_infra = ratio_infra_grad[:,perm_idx_ratio_infra]*signs_ratio_infra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Succinct plotting routine for left hemisphere gradients: all in a single large figure\n",
    "if hemi == 'lh':\n",
    "    surf_file = lh_surf\n",
    "else:\n",
    "    surf_file = rh_surf\n",
    "\n",
    "num_pcas = 3\n",
    "\n",
    "# Prepare gradient sets and labels for iteration\n",
    "grads_and_labels = [\n",
    "    (total_grad, [f'{i+1}' for i in range(num_pcas)], 'total'),\n",
    "    (source_aligned_supra, [f'{perm_idx_supra[i]+1}' for i in range(num_pcas)], 'supra'),\n",
    "    (source_aligned_infra, [f'{perm_idx_infra[i]+1}' for i in range(num_pcas)], 'infra'),\n",
    "    (source_aligned_relative, [f'{perm_idx_relative[i]+1}' for i in range(num_pcas)], 'relative'),\n",
    "    (source_aligned_ratio_supra, [f'{perm_idx_ratio_supra[i]+1}' for i in range(num_pcas)], 'supra/total'),\n",
    "    (source_aligned_ratio_infra, [f'{perm_idx_ratio_infra[i]+1}' for i in range(num_pcas)], 'infra/total')\n",
    "]\n",
    "\n",
    "# Save the grads_and_labels arrays and metadata using numpy\n",
    "import numpy as np\n",
    "if not run_hcp_data:\n",
    "    os.makedirs(exvivo_save_subpath, exist_ok=True)\n",
    "    # grads_and_labels is a list of (array, label_list, metric)\n",
    "    # We'll save arrays and metadata separately.\n",
    "    arrays = [item[0] for item in grads_and_labels]\n",
    "    labels = [item[1] for item in grads_and_labels]\n",
    "    metrics = [item[2] for item in grads_and_labels]\n",
    "    np.savez(\n",
    "        f'{exvivo_save_subpath}/grad_algnd2_total_grad_{hemi}_sparsity_{G_sparsity}.npz',\n",
    "        arrays=arrays, labels=labels, metrics=metrics\n",
    "    )\n",
    "\n",
    "\n",
    "num_sets = len(grads_and_labels)  # rows\n",
    "n_cols = num_pcas                # columns\n",
    "\n",
    "# Create a large figure: rows = metrics, cols = PCs\n",
    "fig, axs = plt.subplots(num_sets, n_cols, figsize=(2*n_cols, 2*num_sets))\n",
    "if num_sets == 1:\n",
    "    axs = [axs]\n",
    "if n_cols == 1:\n",
    "    axs = np.expand_dims(axs, axis=1)\n",
    "\n",
    "for row_idx, (grad_set, label_list, metric) in enumerate(grads_and_labels):\n",
    "    grad_this = {0: grad_set}\n",
    "    plotters = create_hemisphere_plots(\n",
    "        grad_this, surf_file, hemi=hemi,\n",
    "        N_components_plot=N_components_plot,\n",
    "        color_ranges=[0.5] * N_components_plot\n",
    "    )\n",
    "    # Add the metric name to the center of the row at the top\n",
    "    center_col = n_cols // 2\n",
    "    axs[row_idx, center_col].set_title(metric, fontsize=18, pad=25, loc='center', color='black', fontweight='bold')\n",
    "    for col_idx in range(n_cols):\n",
    "        ax = axs[row_idx, col_idx]\n",
    "        ax.imshow(plotters[col_idx][0])\n",
    "        # Only add component # label if this column does NOT already have the row title\n",
    "        ax.set_title(f'Component {label_list[col_idx]}', fontsize=12)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Import grads_and_labels from .npz file (matching earlier code block)\n",
    "if not run_hcp_data:\n",
    "    exvivo_save_subpath = 'EXVIVO'\n",
    "    npz_path = f'{exvivo_save_subpath}/grad_algnd2_total_grad_{hemi}_sparsity_{G_sparsity}.npz'\n",
    "    with np.load(npz_path, allow_pickle=True) as data:\n",
    "        arrays = data['arrays']\n",
    "        labels = data['labels']\n",
    "        metrics = data['metrics']\n",
    "        grads_and_labels = list(zip(arrays, labels, metrics))\n",
    "\n",
    "# Print the 'metric' of the second grad_and_label entry (index 1)\n",
    "if 'grads_and_labels' in locals() and len(grads_and_labels) > 1:\n",
    "    print(grads_and_labels[1][2])\n",
    "else:\n",
    "    print(\"grads_and_labels is not properly loaded or too short.\")\n",
    "\n",
    "\n",
    "\n",
    "# Spatial similarity between ex vivo and hcp data, if the hcp gradient maps are already saved in the folder\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from enigmatoolbox.permutation_testing import spin_test, shuf_test\n",
    "\n",
    "# --- Parameters block: set all user-controlled parameters here ---\n",
    "hcp_grad_file = f'{hcp_save_subpath}/hcp_template_grad_{hemi}_sparsity_{G_sparsity}.npy'\n",
    "metric_labels = ['Total', 'Infra', 'Supra', 'Relative', 'Infra/Total','Supra/Total']  # Can be any length\n",
    "bar_width = 0.12\n",
    "colormap_name = 'tab20'\n",
    "ylim_min = -0.5\n",
    "ylim_max = 0.5\n",
    "font_size = 16\n",
    "ncomp = 3 #number of components to plot ; if not specified, all will be plotted\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Load HCP gradient maps and calculate spatial similarity\n",
    "if os.path.exists(hcp_grad_file):\n",
    "    hcp_tmpl = np.load(hcp_grad_file)\n",
    "\n",
    "    # Gather all available correlation vectors as a list so logic below can adapt to list length:\n",
    "    calc_corrs = [\n",
    "        ('Total', np.diag(np.corrcoef(total_grad.T, hcp_tmpl.T)[:total_grad.shape[1], total_grad.shape[1]:])),\n",
    "        ('Supra', np.diag(np.corrcoef(source_aligned_supra.T, hcp_tmpl.T)[:source_aligned_supra.shape[1], source_aligned_supra.shape[1]:])),\n",
    "        ('Infra', np.diag(np.corrcoef(source_aligned_infra.T, hcp_tmpl.T)[:source_aligned_infra.shape[1], source_aligned_infra.shape[1]:])),\n",
    "        ('Relative', np.diag(np.corrcoef(source_aligned_relative.T, hcp_tmpl.T)[:source_aligned_relative.shape[1], source_aligned_relative.shape[1]:])),\n",
    "        ('Infra/Total', np.diag(np.corrcoef(source_aligned_ratio_infra.T, hcp_tmpl.T)[:source_aligned_ratio_infra.shape[1], source_aligned_ratio_infra.shape[1]:])),\n",
    "        ('Supra/Total', np.diag(np.corrcoef(source_aligned_ratio_supra.T, hcp_tmpl.T)[:source_aligned_ratio_supra.shape[1], source_aligned_ratio_supra.shape[1]:])),\n",
    "       \n",
    "    ]\n",
    "    # Truncate to min(len(calc_corrs), len(metric_labels)) in case metric_labels or corrs is changed\n",
    "    max_len = min(len(metric_labels), len(calc_corrs))\n",
    "    corr_diags = [(metric_labels[i], calc_corrs[i][1]) for i in range(max_len)]\n",
    "\n",
    "    if len(corr_diags) == 0:\n",
    "        print(\"No correlation vectors specified, nothing to plot.\")\n",
    "    else:\n",
    "        # Determine the number of components actually present; use the minimum length of any values vector\n",
    "        if ncomp is not None:\n",
    "            num_components = ncomp\n",
    "        else:\n",
    "            num_components = min(len(values) for _, values in corr_diags)\n",
    "        x = np.arange(num_components)\n",
    "\n",
    "        fig_height = max(4, max_len)\n",
    "        fig, ax = plt.subplots(figsize=(num_components*3, fig_height))\n",
    "\n",
    "        n_groups = len(corr_diags)\n",
    "        offsets = (np.arange(n_groups) - (n_groups - 1) / 2) * bar_width\n",
    "\n",
    "        cmap = cm.get_cmap(colormap_name)\n",
    "        if n_groups > cmap.N:\n",
    "            colors = [cmap(i / n_groups) for i in range(n_groups)]\n",
    "        else:\n",
    "            colors = cmap(np.linspace(0, 1, n_groups))\n",
    "\n",
    "        for idx, ((label, values), color) in enumerate(zip(corr_diags, colors)):\n",
    "            ax.bar(x + offsets[idx], abs(values[:num_components]), bar_width, label=label, color=color)\n",
    "\n",
    "        ax.set_xlabel('Components', fontsize=font_size)\n",
    "        ax.set_ylabel('Spatial similarity (|$\\\\rho$|)', fontsize=font_size)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([f'{i+1}' for i in x], fontsize=font_size)\n",
    "        ax.tick_params(axis='y', labelsize=font_size, left=False)\n",
    "        # Put the legend outside the plot to the right\n",
    "        ax.legend(fontsize=13, loc='center left', bbox_to_anchor=(1.15, 0.5), ncol=1, frameon=False)\n",
    "        ax.set_title('Spatial similarity between \\nex vivo and HCP (total) gradient maps', fontsize=font_size)\n",
    "        ax.set_ylim(0, ylim_max)\n",
    "\n",
    "        # Remove all spines (box around plot)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        ax.tick_params(axis='y', which='both', left=True, right=False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cross correlations between total_grad and all aligned sources\n",
    "aligned_sources = [\n",
    "    (\"Supra (aligned)\", source_aligned_supra),\n",
    "    (\"Infra (aligned)\", source_aligned_infra),\n",
    "    (\"Relative (aligned)\", source_aligned_relative),\n",
    "    (\"Supra/Total (aligned)\", source_aligned_ratio_supra),\n",
    "    (\"Infra/Total (aligned)\", source_aligned_ratio_infra)\n",
    "]\n",
    "\n",
    "unaligned_sources = [\n",
    "    (\"Supra\", supra_grad),\n",
    "    (\"Infra\", infra_grad),\n",
    "    (\"Relative\", relative_grad),\n",
    "    (\"Supra/Total\", ratio_supra_grad),\n",
    "    (\"Infra/Total\", ratio_infra_grad)\n",
    "]\n",
    "\n",
    "sources_to_plot = aligned_sources\n",
    "num_sources = len(sources_to_plot)\n",
    "corr_mats = []\n",
    "labels = []\n",
    "\n",
    "for label, aligned in sources_to_plot:\n",
    "    cross_corr = np.corrcoef(total_grad.T, aligned.T)[:total_grad.shape[1], total_grad.shape[1]:]\n",
    "    corr_mats.append(cross_corr)\n",
    "    labels.append(label)\n",
    "\n",
    "# Put all correlation matrices in a single row figure\n",
    "fig, axs = plt.subplots(1, num_sources, figsize=(4*num_sources, 4))\n",
    "if num_sources == 1:\n",
    "    axs = [axs]\n",
    "for i, (ax, mat, label) in enumerate(zip(axs, corr_mats, labels)):\n",
    "    im = ax.imshow(mat, cmap='bwr', vmin=-1, vmax=1)\n",
    "    n_comp_x = mat.shape[1]\n",
    "    n_comp_y = mat.shape[0]\n",
    "    ax.set_xlabel(f'{label} PC',fontsize=15)\n",
    "    ax.set_ylabel('Total PC',fontsize=15)\n",
    "    ax.set_title(f'Correlation:\\nTotal vs {label}', fontsize=15)\n",
    "    # Index ticks from 1, use slightly larger ticklabel font size (e.g., 17)\n",
    "    ax.set_xticks(np.arange(n_comp_x))\n",
    "    ax.set_xticklabels(np.arange(1, n_comp_x+1), fontsize=11)\n",
    "    ax.set_yticks(np.arange(n_comp_y))\n",
    "    ax.set_yticklabels(np.arange(1, n_comp_y+1), fontsize=11)\n",
    "    # Annotate each cell with value (2 decimals), using a bit smaller font size (e.g., 8)\n",
    "    for y in range(n_comp_y):\n",
    "        for x in range(n_comp_x):\n",
    "            val = mat[y, x]\n",
    "            text_color = 'w' if abs(val) > 0.5 else 'black'\n",
    "            ax.text(x, y, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=text_color, fontsize=8)\n",
    "    # Only add colorbar to the last subplot for a cleaner look\n",
    "    if i == num_sources - 1:\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrices across data types for first 3 PCs\n",
    "data_types = ['total','supra', 'infra',  'ratio_supra', 'ratio_infra']#,'diff']\n",
    "title_labels = ['total','supra', 'infra', 'supra/total', 'infra/total']#,'diff']\n",
    "\n",
    "plot_dat_lh = []\n",
    "plot_dat_rh = []\n",
    "\n",
    "\n",
    "print('Aligned across hemi reflection and/or rotation matrix being plotted')\n",
    "plot_dat_lh = R_lh_all\n",
    "plot_dat_rh = R_rh_all\n",
    "\n",
    "\n",
    "# Plot both hemispheres\n",
    "for hemi_name, plot_dat in [('left hemisphere', plot_dat_lh), ('right hemisphere', plot_dat_rh)]:\n",
    "    print(f'Plotting {hemi_name}')\n",
    "    fig, axes = plt.subplots(1, len(data_types), figsize=(5*len(data_types), 10))\n",
    "\n",
    "    for i,dtype in enumerate(data_types):\n",
    "        ax = axes[i]\n",
    "        im = ax.imshow(plot_dat[dtype][0],cmap='RdBu_r',vmin=-1.5,vmax=1.5)\n",
    "        ax.set_title(title_labels[i], fontsize=17, fontweight='bold')\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xticks(np.arange(0,10))\n",
    "        ax.set_xticklabels(np.arange(1,11),fontsize=16)\n",
    "        ax.set_xlabel('Principal Component',fontsize=16)\n",
    "        ax.set_yticks(np.arange(0,10))\n",
    "        ax.set_yticklabels(np.arange(1,11),fontsize=16)\n",
    "        ax.set_ylabel('Principal Component',fontsize=16)\n",
    "        # Add colorbar\n",
    "        #cbar = plt.colorbar(im, ax=ax)\n",
    "        #cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrices across data types for first 3 PCs\n",
    "data_types = ['total','supra', 'infra',  'ratio_supra', 'ratio_infra']#,'diff',]\n",
    "tick_labels = ['total','supra', 'infra', 'supra/total', 'infra/total']\n",
    "\n",
    "num_pca_plot = 3\n",
    "#save subfolder \n",
    "SAVEFOLDER = './figures/gradient_corr_mat'\n",
    "# Helper function to process and plot correlation matrices\n",
    "def plot_correlation_matrix(data_dict, pc_index, ax, hemisphere):\n",
    "    # Concatenate data for all types\n",
    "    tmp_data = None\n",
    "    for data_type in data_types:\n",
    "        data = data_dict[data_type][0][:,pc_index].reshape(-1,1)\n",
    "        tmp_data = data if tmp_data is None else np.concatenate((tmp_data, data), axis=1)\n",
    "    \n",
    "    # Calculate and plot correlations\n",
    "    corr_mat = np.corrcoef(tmp_data.T)\n",
    "    im = ax.imshow(corr_mat, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax.set_title(f'PC {pc_index+1}', fontsize=17, fontweight='bold')\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(np.arange(len(tick_labels)))\n",
    "    ax.set_yticks(np.arange(len(tick_labels)))\n",
    "    ax.set_xticklabels(tick_labels, rotation=45, ha='right', fontsize=16)\n",
    "    ax.set_yticklabels(tick_labels, fontsize=16)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    return im\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot correlation matrices for the unaligned\n",
    "fig, axes = plt.subplots(1, num_pca_plot*2, figsize=(10*num_pca_plot, 4))\n",
    "print('displaying unaligned data')\n",
    "for pc_index in range(num_pca_plot):\n",
    "    # Left hemisphere\n",
    "    plot_correlation_matrix(lh_data_all, pc_index, axes[pc_index], 'LH')\n",
    "    \n",
    "    # Right hemisphere \n",
    "    plot_correlation_matrix(rh_data_all, pc_index, axes[pc_index + num_pca_plot], 'RH')\n",
    "\n",
    "plt.tight_layout()\n",
    "#if the save folder exists, save the figure\n",
    "if not os.path.exists(SAVEFOLDER):\n",
    "    os.makedirs(SAVEFOLDER)\n",
    "plt.savefig(os.path.join(SAVEFOLDER, f'unaligned_corr_mat_sparsity{G_sparsity}.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "#Unaligned data \n",
    "\n",
    "print('displaying unaligned data (not reordered)')\n",
    "N_components_plot = 3\n",
    "labelsize = 10\n",
    "SAVEFOLDER = f'./figures/gradients/sparsity{G_sparsity}'\n",
    "#for data_type in lh_data_all.keys():\n",
    "for data_type in data_types:\n",
    "    print(data_type)\n",
    "    grad2plot_lh = lh_data_all[data_type]\n",
    "    grad2plot_rh = rh_data_all[data_type]\n",
    "    #color_ranges_lh = [np.nanmax(np.abs([grad2plot_lh[0][:,pc]])) for pc in range(N_components_plot)]\n",
    "    #color_ranges_rh = [np.nanmax(np.abs([grad2plot_rh[0][:,pc]])) for pc in range(N_components_plot)]\n",
    "    color_ranges_lh = [np.percentile(([grad2plot_lh[0][:,pc]]),95) for pc in range(N_components_plot)]\n",
    "    color_ranges_rh = [np.percentile(([grad2plot_rh[0][:,pc]]),95) for pc in range(N_components_plot)]\n",
    "\n",
    "    plotters_lh = create_hemisphere_plots(\n",
    "        grad2plot_lh,\n",
    "        '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.L.white_MSMAll.32k_fs_LR.surf.gii',\n",
    "        'lh',\n",
    "        N_components_plot,\n",
    "        color_ranges_lh\n",
    "    )\n",
    "\n",
    "    plotters_rh = create_hemisphere_plots(\n",
    "        grad2plot_rh,\n",
    "        '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.R.white_MSMAll.32k_fs_LR.surf.gii',\n",
    "        'rh',\n",
    "        N_components_plot,\n",
    "        color_ranges_rh\n",
    "    )\n",
    "\n",
    "    # Plot both hemispheres\n",
    "    for hemi, plotters, color_ranges in [('LH', plotters_lh, color_ranges_lh), ('RH', plotters_rh, color_ranges_rh)]:\n",
    "        f, (ax1, ax2) = plt.subplots(2, N_components_plot, figsize=(2*N_components_plot, 5))\n",
    "\n",
    "        for pc in range(N_components_plot):\n",
    "            # Create mappable object for colorbar\n",
    "            im1 = ax1[pc].imshow(plotters[pc][0], cmap='RdBu_r', \n",
    "                                vmin=-color_ranges[pc], vmax=color_ranges[pc])\n",
    "            ax1[pc].axis('off')\n",
    "            ax1[pc].set_title(f'PC{pc+1}', fontsize=labelsize*2, fontweight='bold')\n",
    "            \n",
    "            im2 = ax2[pc].imshow(plotters[pc][1], cmap='RdBu_r',\n",
    "                                vmin=-color_ranges[pc], vmax=color_ranges[pc])\n",
    "            ax2[pc].axis('off')\n",
    "            \n",
    "            # Add colorbar between the plots\n",
    "            divider = make_axes_locatable(ax1[pc])\n",
    "            cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "            cbar = plt.colorbar(im1, cax=cax, orientation='horizontal')\n",
    "            cbar.ax.tick_params(labelsize=labelsize)  # Increase colorbar tick label size\n",
    "\n",
    "        plt.tight_layout()\n",
    "        #save figure and the save folder, make the save folder if it doesn't exist\n",
    "        if not os.path.exists(SAVEFOLDER):\n",
    "            os.makedirs(SAVEFOLDER)\n",
    "        plt.savefig(os.path.join(SAVEFOLDER, f'{data_type}_{hemi.lower()}_unaligned_unordered.png'), dpi=300)\n",
    "        plt.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aligned data \n",
    "print('displaying aligned data (Reordered)')\n",
    "#for data_type in lh_data_all.keys():\n",
    "for data_type in data_types:\n",
    "    print(data_type)\n",
    "    grad2plot_lh = lh_aligned_all[data_type]\n",
    "    grad2plot_rh = rh_aligned_all[data_type]\n",
    "    color_ranges_lh = [np.nanmax(np.abs([grad2plot_lh[0][:,pc]])) for pc in range(N_components_plot)]\n",
    "    color_ranges_rh = [np.nanmax(np.abs([grad2plot_rh[0][:,pc]])) for pc in range(N_components_plot)]\n",
    "\n",
    "    plotters_lh = create_hemisphere_plots(\n",
    "        grad2plot_lh,\n",
    "        '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.L.white_MSMAll.32k_fs_LR.surf.gii',\n",
    "        'lh',\n",
    "        N_components_plot,\n",
    "        color_ranges_lh\n",
    "    )\n",
    "\n",
    "    plotters_rh = create_hemisphere_plots(\n",
    "        grad2plot_rh,\n",
    "        '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.R.white_MSMAll.32k_fs_LR.surf.gii',\n",
    "        'rh',\n",
    "        N_components_plot,\n",
    "        color_ranges_rh\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "      # Plot both hemispheres\n",
    "    for hemi, plotters, color_ranges in [('LH', plotters_lh, color_ranges_lh), ('RH', plotters_rh, color_ranges_rh)]:\n",
    "        f, (ax1, ax2) = plt.subplots(2, N_components_plot, figsize=(2*N_components_plot, 5))\n",
    "\n",
    "        for pc in range(N_components_plot):\n",
    "            # Create mappable object for colorbar\n",
    "            im1 = ax1[pc].imshow(plotters[pc][0], cmap='RdBu_r', \n",
    "                                vmin=-color_ranges[pc], vmax=color_ranges[pc])\n",
    "            ax1[pc].axis('off')\n",
    "            ax1[pc].set_title(f'PC{pc+1}', fontsize=labelsize*2, fontweight='bold')\n",
    "            \n",
    "            im2 = ax2[pc].imshow(plotters[pc][1], cmap='RdBu_r',\n",
    "                                vmin=-color_ranges[pc], vmax=color_ranges[pc])\n",
    "            ax2[pc].axis('off')\n",
    "            \n",
    "            # Add colorbar between the plots\n",
    "            divider = make_axes_locatable(ax1[pc])\n",
    "            cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.5)\n",
    "            cbar = plt.colorbar(im1, cax=cax, orientation='horizontal')\n",
    "            cbar.ax.tick_params(labelsize=labelsize)  # Increase colorbar tick label size\n",
    "\n",
    "        plt.tight_layout()\n",
    "        #save figure and the save folder\n",
    "        #make the save folder if it doesn't exist\n",
    "        if not os.path.exists(SAVEFOLDER):\n",
    "            os.makedirs(SAVEFOLDER)\n",
    "        plt.savefig(os.path.join(SAVEFOLDER, f'{data_type}_{hemi.lower()}_aligned_reordered.png'), dpi=300)\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in a large figure we are going to combine all figures to compare the notreordered and reordere figures\n",
    "num_cols = len(data_types)\n",
    "num_rows = 2\n",
    "title_fontsize = 10\n",
    "\n",
    "for HEMI in ['lh','rh']:\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols*3, 3*num_rows))\n",
    "\n",
    "    for i, data_type in enumerate(data_types):\n",
    "        # Load the unreordered figure\n",
    "        unordered_path = os.path.join(SAVEFOLDER, f'{data_type}_{HEMI}_unaligned_unordered.png')\n",
    "        unordered_img = plt.imread(unordered_path)\n",
    "        \n",
    "        # Load the reordered figure\n",
    "        reordered_path = os.path.join(SAVEFOLDER, f'{data_type}_{HEMI}_aligned_reordered.png')\n",
    "        reordered_img = plt.imread(reordered_path)\n",
    "\n",
    "        # Display unreordered image in top row\n",
    "        axes[0, i].imshow(unordered_img)\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f'{data_type} (unordered)', fontsize=title_fontsize, fontweight='bold')\n",
    "        \n",
    "        # Display reordered image in bottom row\n",
    "        axes[1, i].imshow(reordered_img)\n",
    "        axes[1, i].axis('off')\n",
    "        axes[1, i].set_title(f'{data_type} (reordered)', fontsize=title_fontsize, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVEFOLDER, f'{HEMI}_combined_unordered_vs_reordered.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmap gradient data to parcel level for both hemispheres\n",
    "lh_data_all_unmapped = {}\n",
    "rh_data_all_unmapped = {}\n",
    "\n",
    "\n",
    "sparsity_idx = 0#0 is the 0.9 in the G_sparsity variable\n",
    "# Process left hemisphere\n",
    "for data_type in lh_data_all.keys():\n",
    "    mapped_grad = lh_data_all[data_type][sparsity_idx]# this is unaligned (meaning that not reorderd and flipped)\n",
    "    #mapped_grad = lh_aligned_all[data_type][sparsity_idx]# this is aligned (meaning that reorderd and not flipped)\n",
    "    atlasDat_lh = atlasDat[:32492]\n",
    "    unmapped_grad = np.zeros((np.max(atlasDat_lh)-np.min(atlasDat_lh[atlasDat_lh != 0])+1, mapped_grad.shape[1]))\n",
    "    for i, parcel in enumerate(range(np.min(atlasDat_lh[atlasDat_lh != 0]), np.max(atlasDat_lh)+1)):\n",
    "        unmapped_grad[i,:] = np.median(mapped_grad[atlasDat_lh == parcel,:], axis=0)\n",
    "    lh_data_all_unmapped[data_type] = unmapped_grad\n",
    "\n",
    "# Process right hemisphere \n",
    "for data_type in rh_data_all.keys():\n",
    "    mapped_grad = rh_data_all[data_type][sparsity_idx] # this is unaligned (meaning that not reorderd and flipped)\n",
    "   # mapped_grad = rh_aligned_all[data_type][sparsity_idx]# this is aligned (meaning that reorderd and not flipped)\n",
    "    atlasDat_rh = atlasDat[32492:]\n",
    "    unmapped_grad = np.zeros((np.max(atlasDat_rh)-np.min(atlasDat_rh[atlasDat_rh != 0])+1, mapped_grad.shape[1]))\n",
    "    for i, parcel in enumerate(range(np.min(atlasDat_rh[atlasDat_rh != 0]), np.max(atlasDat_rh)+1)):\n",
    "        unmapped_grad[i,:] = np.median(mapped_grad[atlasDat_rh == parcel,:], axis=0)\n",
    "    rh_data_all_unmapped[data_type] = unmapped_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradients versus Gradients (but jt seems like they want in parcel level not vertex level)\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "nPC = 0# 0 is the first PC\n",
    "font_size = 24\n",
    "tick_size = 20\n",
    "data_type_ref_x = 'total'\n",
    "\n",
    "for hemi in ['lh','rh']:\n",
    "\n",
    "    if hemi == 'lh':\n",
    "        gradient_data = lh_data_all_unmapped\n",
    "    else:\n",
    "        gradient_data = rh_data_all_unmapped\n",
    "\n",
    "   \n",
    "    data_types = [key for key in gradient_data.keys() if key != 'diff' and key != 'ratio_supra' and key != 'ratio_infra' and key != 'relative' and key != data_type_ref_x]\n",
    "    n_plots = len(data_types)\n",
    "\n",
    "    # Plot settings\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "    \n",
    "\n",
    "    # Create figure and grid\n",
    "    fig = plt.figure(figsize=(8*n_plots, 8))\n",
    "    # Increase spacing between subplots\n",
    "    gs = fig.add_gridspec(3, 3*n_plots, hspace=0.6, wspace=0.4)\n",
    "\n",
    "\n",
    "\n",
    "    def get_title_name(data_type):\n",
    "        titles = {\n",
    "            'diff': f'G{nPC+1}(Symmetry)',\n",
    "            'ratio_supra': f'G{nPC+1}(Supra/Total Ratio)',\n",
    "            'ratio_infra': f'G{nPC+1}(Infra/Total Ratio)', \n",
    "            'relative': f'G{nPC+1}(Relative)',\n",
    "            'he': f'G{nPC+1}(Hurst Exponent)',\n",
    "            'supra': f'G{nPC+1}(Supra Thickness)',\n",
    "            'infra': f'G{nPC+1}(Infra Thickness)',\n",
    "            'total': f'G{nPC+1}(Total Thickness)'\n",
    "        }\n",
    "        return titles.get(data_type, '')\n",
    "\n",
    "\n",
    "    def get_data_for_hemisphere(hemi, data_type_ref_x, gradient_data, data_type, nPC, atlas, atlas_to_network):\n",
    "        if hemi == 'lh':\n",
    "            #mask = atlas[0:32492].astype(bool)\n",
    "            gradient_ref = gradient_data[data_type_ref_x][:,nPC]\n",
    "            gradient = gradient_data[data_type][:,nPC]\n",
    "            data_label_color = np.array(atlas_to_network[1:int(len(atlas_to_network)/2+1)])\n",
    "            atlas_dat = atlas[:32492]\n",
    "            atlas_min = np.min(atlas_dat[atlas_dat != 0])\n",
    "            atlas_max = np.max(atlas_dat[atlas_dat != 0])\n",
    "            new_data_label_color = []\n",
    "\n",
    "            print(data_label_color.shape)\n",
    "            for i,parcel in enumerate(range(atlas_min,atlas_max+1)):\n",
    "                print(parcel)\n",
    "                median_color = np.median(data_label_color[atlas_dat == parcel,:],axis=0)\n",
    "                print(median_color)\n",
    "                new_data_label_color.append(tuple(median_color.tolist()))\n",
    "            data_label_color = new_data_label_color\n",
    "        else:\n",
    "            #mask = atlas[32492:].astype(bool)\n",
    "            gradient_ref = gradient_data[data_type_ref_x][:,nPC]\n",
    "            gradient = gradient_data[data_type][:,nPC]\n",
    "            data_label_color = np.array(atlas_to_network[int(len(atlas_to_network)/2):])\n",
    "            atlas_dat = atlas[32492:]\n",
    "            atlas_min = np.min(atlas_dat[atlas_dat != 0])\n",
    "            atlas_max = np.max(atlas_dat[atlas_dat != 0])\n",
    "            new_data_label_color = []\n",
    "\n",
    "            for i,parcel in enumerate(range(atlas_min,atlas_max+1)):\n",
    "                median_color = np.median(data_label_color[atlas_dat == parcel,:],axis=0)\n",
    "                new_data_label_color.append(tuple(median_color.tolist()))\n",
    "            data_label_color = new_data_label_color\n",
    "\n",
    "\n",
    "        return gradient_ref, gradient, data_label_color\n",
    "    \n",
    "    # Create network color mapping\n",
    "    atlas_to_network = [yeo_network_colors[atlas[i]] for i in range(len(atlas))]\n",
    "\n",
    "    # Plot each data type\n",
    "    for plot_idx, data_type in enumerate(data_types):\n",
    "        # Get data\n",
    "        gradient_ref, gradient, data_label_color =get_data_for_hemisphere(\n",
    "            hemi, data_type_ref_x, gradient_data, data_type, nPC, atlas, atlas_to_network\n",
    "        )\n",
    "        \n",
    "        # Calculate correlation\n",
    "        valid_mask = ~np.isnan(gradient_ref) & ~np.isnan(gradient)\n",
    "        r, p = stats.pearsonr(gradient_ref[valid_mask], gradient[valid_mask])\n",
    "        print(f\"{data_type}: r={r:.2f}, p={p:.2e}\")\n",
    "        \n",
    "        # Plot layout\n",
    "        col_start = 3 * plot_idx\n",
    "        col_end = col_start + 2\n",
    "        \n",
    "        # Main scatter plot\n",
    "        ax_scatter = fig.add_subplot(gs[1:, col_start:col_end])\n",
    "        sns.scatterplot(x=gradient_ref, y=gradient, alpha=1, s=60,\n",
    "                        c=data_label_color,edgecolor='None', ax=ax_scatter)\n",
    "        \n",
    "        # Add regression line and correlation text\n",
    "        sns.regplot(x=gradient_ref, y=gradient, scatter=False, color='black',\n",
    "                    line_kws={'linestyle': '-', 'linewidth': 8}, ax=ax_scatter)\n",
    "        ax_scatter.text(0.05, 0.95, f'r = {r:.2f}\\np = {p:.2e}',\n",
    "                        transform=ax_scatter.transAxes, va='top',\n",
    "                        fontsize=font_size, weight='bold', color='black')\n",
    "        \n",
    "        # Format scatter plot\n",
    "        ax_scatter.set_xlim(np.nanmin(gradient_ref), np.nanmax(gradient_ref))\n",
    "        ax_scatter.set_ylim(np.nanmin(gradient), np.nanmax(gradient))\n",
    "        ax_scatter.set_ylabel(f'G{nPC+1} ({data_type})', \n",
    "                            fontsize=font_size, labelpad=12, weight='bold')\n",
    "        ax_scatter.set_xlabel(f'G{nPC+1} ({data_type_ref_x})', fontsize=font_size, labelpad=12, weight='bold')\n",
    "        ax_scatter.set_title(get_title_name(data_type), fontsize=font_size+2, pad=20, weight='bold')\n",
    "        ax_scatter.tick_params(axis='both', which='major', labelsize=tick_size)\n",
    "        ax_scatter.grid(True, linestyle='--', alpha=0)\n",
    "        \n",
    "        for spine in ax_scatter.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(2.0)\n",
    "        \n",
    "        # Top histogram\n",
    "        ax_histx = fig.add_subplot(gs[0, col_start:col_end])\n",
    "        sns.histplot(data=np.flip(gradient_ref), bins=50, kde=True, color='#8E44AD',\n",
    "                    ax=ax_histx, stat='density', alpha=1)\n",
    "        ax_histx.set(xlabel='', ylabel='')\n",
    "        ax_histx.set_yticks([])\n",
    "        ax_histx.tick_params(labelbottom=False, labelsize=tick_size)\n",
    "        for spine in ax_histx.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        \n",
    "        # Right histogram\n",
    "        ax_histy = fig.add_subplot(gs[1:, col_end])\n",
    "        sns.histplot(y=np.flip(gradient), bins=50, kde=True, color='#8E44AD',\n",
    "                    ax=ax_histy, stat='density', alpha=1)\n",
    "        ax_histy.set(xlabel='', ylabel='')\n",
    "        ax_histy.set_xticks([])\n",
    "        ax_histy.tick_params(labelleft=False, labelsize=tick_size)\n",
    "        for spine in ax_histy.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_plot.png', dpi=300, bbox_inches='tight',\n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_with_histograms(\n",
    "    x, y, xlabel_name, ylabel_name, marker_size=30, data_label_color=None, ax=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a scatterplot with histograms on a given Axes (ax). \n",
    "    If ax is not provided, create a new figure with marginal histograms.\n",
    "    Returns: fig, axes_dict, stats_dict\n",
    "    \"\"\"\n",
    "    from scipy import stats as sp_stats\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Helper function to format p-values\n",
    "    def format_pvalue(p):\n",
    "        \"\"\"Format p-value: show actual value if >= 0.001, otherwise show '< 0.001'\"\"\"\n",
    "        if p < 0.001:\n",
    "            return \"< 0.001\"\n",
    "        else:\n",
    "            return f\"= {p:.3f}\"\n",
    "    \n",
    "    # Compute correlation\n",
    "    r, p = sp_stats.spearmanr(x, y)\n",
    "    print(f\"{ylabel_name}: r={r:.2f}, p={p:.2e}\")\n",
    "    \n",
    "    # Format p-value for display\n",
    "    p_formatted = format_pvalue(p)\n",
    "    \n",
    "    # Colors\n",
    "    if data_label_color is None:\n",
    "        data_label_color = ['#8E44AD' for _ in x]\n",
    "    font_size = max(int(marker_size * 0.50), 10)\n",
    "    tick_size = max(int(marker_size * 0.50), 8)\n",
    "    \n",
    "    # If an axis is passed, just draw scatter (no histograms, as in a multi-panel layout)\n",
    "    if ax is not None:\n",
    "        fig = ax.figure\n",
    "        axes = {'scatter': ax}\n",
    "        sns.scatterplot(x=x, y=y, alpha=1, s=marker_size, \n",
    "                        c=data_label_color, edgecolor='black', ax=ax)\n",
    "        # Regression line\n",
    "        if len(x) > 1:\n",
    "            try:\n",
    "                slope, intercept = np.polyfit(x, y, 1)\n",
    "            except np.linalg.LinAlgError:\n",
    "                slope, intercept = 0, np.nan\n",
    "            x_mean, x_std = np.nanmean(x), np.nanstd(x)\n",
    "            x_window = 3.5 * x_std\n",
    "            x_lim = (x_mean - x_window, x_mean + x_window)\n",
    "            x_line = np.linspace(x_lim[0], x_lim[1], 100)\n",
    "            y_line = slope * x_line + intercept\n",
    "            ax.plot(x_line, y_line, color='black', linestyle='--', linewidth=max(1, marker_size // 10))\n",
    "        else:\n",
    "            x_lim = (np.nanmin(x), np.nanmax(x))\n",
    "            y_lim = (np.nanmin(y), np.nanmax(y))\n",
    "        \n",
    "        # Annotate correlation with formatted p-value\n",
    "        ax.text(0.04, 0.97, f'r={r:.2f}, p {p_formatted}',\n",
    "                   transform=ax.transAxes, va='top',\n",
    "                   fontsize=font_size, weight='bold', color='black')\n",
    "        \n",
    "        # Axis limits\n",
    "        if len(x) > 1:\n",
    "            y_mean, y_std = np.nanmean(y), np.nanstd(y)\n",
    "            y_window = 3.5 * y_std\n",
    "            y_lim = (y_mean - y_window, y_mean + y_window)\n",
    "        ax.set_xlim(x_lim)\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.set_xlabel(xlabel_name, fontsize=font_size, labelpad=2, weight='bold')\n",
    "        ax.set_ylabel(ylabel_name, fontsize=font_size, labelpad=2, weight='bold')\n",
    "        ax.tick_params(axis='both', which='major', labelsize=tick_size)\n",
    "        ax.grid(True, linestyle='--', alpha=0)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(2.0)\n",
    "        stats_dict = {'r': r, 'p': p}\n",
    "        return fig, axes, stats_dict\n",
    "    \n",
    "    # If ax is None, draw as standalone figure with marginal histograms\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.12, wspace=0.12)\n",
    "    axes = {}\n",
    "    \n",
    "    # Main scatter plot\n",
    "    ax_scatter = fig.add_subplot(gs[1:, 0:2])\n",
    "    axes['scatter'] = ax_scatter\n",
    "    sns.scatterplot(x=x, y=y, alpha=1, s=marker_size, c=data_label_color, edgecolor='black', ax=ax_scatter)\n",
    "    \n",
    "    # Regression line\n",
    "    if len(x) > 1:\n",
    "        try:\n",
    "            slope, intercept = np.polyfit(x, y, 1)\n",
    "        except np.linalg.LinAlgError:\n",
    "            slope, intercept = 0, np.nan\n",
    "        x_mean, x_std = np.nanmean(x), np.nanstd(x)\n",
    "        x_window = 3.5 * x_std\n",
    "        x_lim = (x_mean - x_window, x_mean + x_window)\n",
    "        x_line = np.linspace(x_lim[0], x_lim[1], 100)\n",
    "        y_line = slope * x_line + intercept\n",
    "        ax_scatter.plot(x_line, y_line, color='black', linestyle='--', linewidth=max(1, marker_size // 10))\n",
    "    else:\n",
    "        x_lim = (np.nanmin(x), np.nanmax(x))\n",
    "        y_lim = (np.nanmin(y), np.nanmax(y))\n",
    "    \n",
    "    # Annotate correlation with formatted p-value\n",
    "    ax_scatter.text(0.04, 0.97, f'r={r:.2f}, p {p_formatted}',\n",
    "                    transform=ax_scatter.transAxes, va='top',\n",
    "                    fontsize=font_size, weight='bold', color='black')\n",
    "    \n",
    "    # Axis limits\n",
    "    if len(x) > 1:\n",
    "        y_mean, y_std = np.nanmean(y), np.nanstd(y)\n",
    "        y_window = 3.5 * y_std\n",
    "        y_lim = (y_mean - y_window, y_mean + y_window)\n",
    "    ax_scatter.set_xlim(x_lim)\n",
    "    ax_scatter.set_ylim(y_lim)\n",
    "    ax_scatter.set_xlabel(xlabel_name, fontsize=font_size, labelpad=1, weight='bold')\n",
    "    ax_scatter.set_ylabel(ylabel_name, fontsize=font_size, labelpad=1, weight='bold')\n",
    "    ax_scatter.tick_params(axis='both', which='major', labelsize=tick_size)\n",
    "    ax_scatter.grid(True, linestyle='--', alpha=0)\n",
    "    for spine in ax_scatter.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(2.0)\n",
    "    \n",
    "    # Top histogram\n",
    "    ax_histx = fig.add_subplot(gs[0, 0:2])\n",
    "    axes['histx'] = ax_histx\n",
    "    sns.histplot(x=x, bins=50, kde=True, color='#8E44AD', ax=ax_histx, stat='density', alpha=1)\n",
    "    ax_histx.set_xlim(x_lim)\n",
    "    hist_ylim = ax_histx.get_ylim()\n",
    "    ax_histx.set_ylim(0, hist_ylim[1]*3)\n",
    "    ax_histx.set(xlabel='', ylabel='')\n",
    "    ax_histx.set_yticks([])\n",
    "    ax_histx.tick_params(labelbottom=False, labelsize=tick_size)\n",
    "    for spine in ax_histx.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    \n",
    "    # Right histogram\n",
    "    ax_histy = fig.add_subplot(gs[1:, 2])\n",
    "    axes['histy'] = ax_histy\n",
    "    sns.histplot(y=y, bins=50, kde=True, color='#8E44AD', ax=ax_histy, stat='density', alpha=1)\n",
    "    ax_histy.set_ylim(y_lim)\n",
    "    histx_lim = ax_histy.get_xlim()\n",
    "    ax_histy.set_xlim(0, histx_lim[1]*3)\n",
    "    ax_histy.set(xlabel='', ylabel='')\n",
    "    ax_histy.set_xticks([])\n",
    "    ax_histy.tick_params(labelleft=False, labelsize=tick_size)\n",
    "    for spine in ax_histy.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    stats_dict = {'r': r, 'p': p}\n",
    "    return fig, axes, stats_dict\n",
    "\n",
    "def _darker_color(color, factor=0.7):\n",
    "    # Accept color in hex string or RGB; convert to RGB, darken, return as tuple\n",
    "    rgb = mcolors.to_rgb(color)\n",
    "    return tuple(factor * c for c in rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is aperiod exponent from the sensor-space data\n",
    "from scipy.stats import ranksums, spearmanr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "exp_data_path = '/Users/dennis.jungchildmind.org/Downloads/exponent_result1.shape.gii'\n",
    "#exp_data_path = '/Users/dennis.jungchildmind.org/Downloads/beta_power.shape.gii'\n",
    "exp_data = nib.load(exp_data_path)\n",
    "exp_data = exp_data.darrays[2]#0 is vertices, 1 is faces, 2 is exponent values\n",
    "\n",
    "exp_data_lh = exp_data.data[0:32492]\n",
    "exp_data_rh = exp_data.data[32492:]\n",
    "\n",
    "\n",
    "hemi = 'lh'\n",
    "YLABEL_NAME = 'Aperiodic Exponent'\n",
    "LAYER_TYPE = 'total'\n",
    "grad2plot = lh_data_all_unmapped[LAYER_TYPE]#total_grad #this is gradient for the hemi specific in the earlier section\n",
    "\n",
    "if hemi == 'lh':\n",
    "    atlas_data = atlas_data_lh  # Use LH for now\n",
    "    exp_parcellated = parcellate_data(exp_data_lh, atlas_data)\n",
    "   # yeo_color = yeo_network_colors[1:201]\n",
    "else:\n",
    "    atlas_data = atlas_data_rh\n",
    "    exp_parcellated = parcellate_data(exp_data_rh, atlas_data)\n",
    "  #  yeo_color = yeo_network_colors[201,:]\n",
    "\n",
    "num_pcs_to_plot = 3\n",
    "\n",
    "#generate a figure with num_pcs_to_plot subplots\n",
    "main_fig, main_axes = plt.subplots(1, num_pcs_to_plot, figsize=(num_pcs_to_plot*3, num_pcs_to_plot))\n",
    "\n",
    "for pc in range(num_pcs_to_plot):\n",
    "    grad_parcellated = grad2plot[:,pc]#parcellate_data(grad2plot[:, pc], atlas_data)\n",
    "    nonzero_mask = grad_parcellated != 0\n",
    "    # Use correct colors for scatter plot based on hemisphere and nonzero_mask.\n",
    "    #if hemi == 'lh':\n",
    "        # Use first 200 colors, skip yeo_network_colors[0] (assume background/unknown), length should match parcellation.\n",
    "       # color_subset = yeo_network_colors[1:201]\n",
    "   # else:\n",
    "        # Use next 200 colors\n",
    "       # color_subset = yeo_network_colors[201:]\n",
    "\n",
    "    # Filter to match nonzero_mask (scatter points only where data is present)\n",
    "    #yeo_color = [c for c, keep in zip(color_subset, nonzero_mask) if keep]\n",
    "    #edge_colors = [_darker_color(c, 0.7) for c in yeo_color]\n",
    "    x = grad_parcellated[nonzero_mask]\n",
    "    y = exp_parcellated[nonzero_mask]\n",
    "\n",
    "    # Rank sum test: split y by above/below median x (not a \"correlation\", but a group difference)\n",
    "    median_x = np.median(x)\n",
    "    group1 = y[x >= median_x]\n",
    "    group2 = y[x < median_x]\n",
    " \n",
    "\n",
    "\n",
    "    # Instead of creating a new figure inside scatter_with_histograms, \n",
    "    # pass the target axis from main_axes[pc] so all drawing happens on your desired axes.\n",
    "    # This means you need to make sure scatter_with_histograms allows passing an ax argument.\n",
    "    _, _, stats_dict = scatter_with_histograms(\n",
    "        x, y, 'Gradient '+str(pc+1)+ ' score', YLABEL_NAME,\n",
    "        marker_size=30, data_label_color=None, ax=main_axes[pc]\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums, spearmanr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "hemi = 'lh'\n",
    "\n",
    "#for gradient (x-axis)\n",
    "LAYER_TYPE = 'infra'\n",
    "\n",
    "#for DFA (y-axis)\n",
    "YLABEL_NAME = 'DFA Exponent'\n",
    "FREQ_RANGES = [[0.1,4.0],[4.0,8.0],[8.0,15.0],[15.0,30.0],[30.0,60.0]]\n",
    "num_pcs_to_plot = 3\n",
    "n_freq = len(FREQ_RANGES)\n",
    "\n",
    "# Make a big figure: each row = freq range, each col = one PC\n",
    "# Use constrained_layout for better spacing\n",
    "main_fig, main_axes = plt.subplots(\n",
    "    n_freq, num_pcs_to_plot, \n",
    "    figsize=(num_pcs_to_plot*3, n_freq*3),  # Increased size for better visibility\n",
    "    squeeze=False,\n",
    "    constrained_layout=True\n",
    ")\n",
    "\n",
    "for fi, freq_range in enumerate(FREQ_RANGES):\n",
    "    print(fi,freq_range)\n",
    "    # Load DFA (Y-axis) data for this frequency\n",
    "    dfa_data_path = f'/Users/dennis.jungchildmind.org/Desktop/MEG/112225_dfa_outputs/dfa_restin_f{freq_range[0]}_to_{freq_range[1]}_parcelonly.pkl'\n",
    "    dfa_data = pickle.load(open(dfa_data_path, 'rb'))\n",
    "    dfa_data_mean = np.nanmean(dfa_data, axis=0)\n",
    "    dfa_data_mean_lh = dfa_data_mean[0:int(num_parcels/2)]\n",
    "    dfa_data_mean_rh = dfa_data_mean[int(num_parcels/2):]\n",
    "    \n",
    "    # pick LH/RH for gradient and exp_parcellated\n",
    "    grad2plot = lh_data_all_unmapped[LAYER_TYPE] if hemi=='lh' else rh_data_all_unmapped[LAYER_TYPE]\n",
    "    \n",
    "    if hemi == 'lh':\n",
    "        atlas_data = atlas_data_lh\n",
    "        exp_parcellated = dfa_data_mean_lh\n",
    "        color_subset = yeo_network_colors[1:201]\n",
    "    else:\n",
    "        atlas_data = atlas_data_rh\n",
    "        exp_parcellated = dfa_data_mean_rh\n",
    "        color_subset = yeo_network_colors[201:]\n",
    "    \n",
    "    for pc in range(num_pcs_to_plot):\n",
    "        ax = main_axes[fi, pc]\n",
    "        grad_parcellated = grad2plot[:,pc] #parcellate_data(grad2plot[:, pc], atlas_data)\n",
    "        nonzero_mask = grad_parcellated != 0\n",
    "        x = grad_parcellated[nonzero_mask]\n",
    "        y = exp_parcellated[nonzero_mask]\n",
    "        \n",
    "        # Optionally could test and annotate with stats, not required for plotting\n",
    "        # median_x = np.median(x)\n",
    "        # group1 = y[x >= median_x]\n",
    "        # group2 = y[x < median_x]\n",
    "        \n",
    "        # Plot\n",
    "        _, _, stats_dict = scatter_with_histograms(\n",
    "            x, y, f'Gradient {pc+1} score', YLABEL_NAME,\n",
    "            marker_size=30, data_label_color=color_subset, ax=ax)\n",
    "        \n",
    "        # Force the subplot box to be square, but keep independent axis scales\n",
    "        ax.set_box_aspect(1)\n",
    "        \n",
    "        # Optionally annotate stats here: e.g. correlation, p-value from stats_dict\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break HERE for now, since we don't wanna do the gene stuff yet\n",
    "import abagen\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import zscore\n",
    "#https://github.com/ThomasYeoLab/CBIG/blob/master/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal/Parcellations/MNI/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii.gz\n",
    "atlas_path = '/Users/dennis.jungchildmind.org/Downloads/Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.nii.gz'\n",
    "#need to use abagen 0.1.3 and pandas needs to be 1.5.3 to avoid inplace error 11/25/2025\n",
    "expression_cache_file = 'ahba_expression.pkl'\n",
    "if os.path.exists(expression_cache_file):\n",
    "    with open(expression_cache_file, 'rb') as f:\n",
    "        expression = pickle.load(f)\n",
    "else:\n",
    "    expression = abagen.get_expression_data(atlas_path, lr_mirror=True, return_donors=True)\n",
    "    with open(expression_cache_file, 'wb') as f:\n",
    "        pickle.dump(expression, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of keys: {len(expression['9861'].keys())}\")\n",
    "gene_keys = list(expression['9861'].keys())\n",
    "htr_keys = [k for k in gene_keys if k.lower().startswith('htr')]\n",
    "print(f\"Keys that start with 'htr': {htr_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gene_expression(expression, target_gene,do_zscore=True):\n",
    "    \"\"\"Get z-scored gene expression values across all donors for a target gene.\n",
    "    \n",
    "    Args:\n",
    "        expression: Dictionary mapping donor IDs to expression DataFrames\n",
    "        target_gene: Name of gene to extract expression for\n",
    "        \n",
    "    Returns:\n",
    "        Array of z-scored expression values concatenated across donors\n",
    "    \"\"\"\n",
    "    all_exp_data = []\n",
    "    \n",
    "    for donor, donor_data in expression.items():\n",
    "        # Find columns containing target gene\n",
    "        matching_cols = donor_data.columns[donor_data.columns==target_gene]\n",
    "        \n",
    "        if matching_cols.size > 0:\n",
    "            # Extract and z-score expression values\n",
    "            exp_values = donor_data[matching_cols].values\n",
    "            if do_zscore:   \n",
    "                exp_values = zscore(exp_values, axis=0, nan_policy='omit')\n",
    "            all_exp_data.append(exp_values)\n",
    "\n",
    "\n",
    "    if not all_exp_data:\n",
    "        return None\n",
    "    return np.concatenate(all_exp_data, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Gradients versus Gene Expression data at the Parcel Level\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "correlations_file = 'gene_correlations.npy'\n",
    "pvalues_file = 'gene_pvalues.npy'\n",
    "gene_names_file = 'gene_names.npy'\n",
    "data_types_file = 'data_types.npy'\n",
    "\n",
    "if (os.path.exists(correlations_file) and os.path.exists(pvalues_file)\n",
    "    and os.path.exists(gene_names_file) and os.path.exists(data_types_file)):\n",
    "    correlations = np.load(correlations_file)\n",
    "    p_values = np.load(pvalues_file)\n",
    "    ALL_GENES = np.load(gene_names_file, allow_pickle=True)\n",
    "    data_types = np.load(data_types_file, allow_pickle=True)\n",
    "else:\n",
    "    # Initialize arrays to store correlation results\n",
    "\n",
    "    ALL_GENES = expression['9861'].columns.tolist()\n",
    "    print(ALL_GENES)\n",
    "\n",
    "    n_genes = len(ALL_GENES)\n",
    "    data_types = [key for key in gradient_data.keys() if key != 'diff']\n",
    "    n_data_types = len(data_types)\n",
    "\n",
    "    # Arrays to store correlation coefficients and p-values\n",
    "    correlations = np.zeros((n_genes, n_data_types, 2)) # 2 for lh and rh\n",
    "    p_values = np.zeros((n_genes, n_data_types, 2))\n",
    "\n",
    "    nPC = 1 #0 is the first PC\n",
    "\n",
    "    # Loop through each gene\n",
    "    for gene_idx, gene in enumerate(ALL_GENES):\n",
    "        #print(f\"\\rProcessing gene {gene} ({gene_idx+1}/{n_genes})\", end='', flush=True)\n",
    "        \n",
    "        # Get gene expression data\n",
    "        gene_exp_data = get_gene_expression(expression, gene)\n",
    "        the_gene_data = np.nanmean(gene_exp_data, axis=-1)\n",
    "\n",
    "        # Process each hemisphere\n",
    "        for hemi_idx, hemi in enumerate(['lh', 'rh']):\n",
    "            if hemi == 'lh':\n",
    "                gradient_data_hem = lh_data_all_unmapped\n",
    "                atlas = atlas_data_lh\n",
    "            else:\n",
    "                gradient_data_hem = rh_data_all_unmapped\n",
    "                atlas = atlas_data_rh\n",
    "\n",
    "            # Create network color mapping\n",
    "            atlas_to_network = [yeo_network_colors[atlas[i]] for i in range(len(atlas))]\n",
    "\n",
    "            # Process each data type\n",
    "            for data_type_idx, data_type in enumerate(data_types):\n",
    "                # Get data for hemisphere\n",
    "                if hemi == 'lh':\n",
    "                    gradient = gradient_data_hem[data_type][:,nPC]\n",
    "                    gene_ref = np.nan_to_num(the_gene_data[:int(num_parcels/2)])\n",
    "                else:\n",
    "                    gradient = gradient_data_hem[data_type][:,nPC]\n",
    "                    gene_ref = np.nan_to_num(the_gene_data[int(num_parcels/2):])\n",
    "\n",
    "                # Calculate correlation\n",
    "                valid_mask = ~np.isnan(gene_ref) & ~np.isnan(gradient)\n",
    "                r, p = stats.spearmanr(gene_ref[valid_mask], gradient[valid_mask])\n",
    "\n",
    "                # Store results\n",
    "                correlations[gene_idx, data_type_idx, hemi_idx] = r\n",
    "                p_values[gene_idx, data_type_idx, hemi_idx] = p\n",
    "\n",
    "    # Save results\n",
    "    np.save('gene_correlations.npy', correlations)\n",
    "    np.save('gene_pvalues.npy', p_values)\n",
    "\n",
    "    # Also save metadata for interpreting results\n",
    "    np.save('gene_names.npy', np.array(ALL_GENES))\n",
    "    np.save('data_types.npy', np.array(data_types))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene,layer type,hemi (hemi means gradient1,graident2 sincne both hemisphere have the same level of gene expression level as they are reflected*)\n",
    "from scipy.stats import false_discovery_control\n",
    "# Using scipy (newer method)\n",
    "alpha = 0.05\n",
    "#the second dimension is layer type [['total', 'supra', 'infra', 'ratio_supra', 'ratio_infra', 'relative']]\n",
    "#the third dimension is hemisphere [lh,rh]\n",
    "corrected_pvals = false_discovery_control(p_values[:,0,0], method='bh')\n",
    "\n",
    "significant_indices = np.where(corrected_pvals < alpha)[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSE_GENES = ['BEND5','C1QL2','CACNA1E','COL24A1','COL6A1','CRYM','KCNC3',\n",
    "             'KCNH4','LGALS1','MFGE8','NEFH','SCN3B','SCN4B',\n",
    "             'SNCG','SV2C','SYT2','TPBG','VAMP1']\n",
    "hse_gene_data = []\n",
    "for gene in HSE_GENES:\n",
    "    gene_exp = get_gene_expression(expression, gene)\n",
    "    hse_gene_data.append(gene_exp)\n",
    "\n",
    "\n",
    "hse_gene_data = np.stack(hse_gene_data, axis=0)\n",
    "print(hse_gene_data.shape)\n",
    "the_gene_data = np.nanmean(hse_gene_data, axis=-1)\n",
    "the_gene_data = np.sum(the_gene_data, axis=0)\n",
    "xlabel_name = 'HSE_GENES (mean)'\n",
    "print(the_gene_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradients versus Gene Expression data at the Parcel Level\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "pvalb_exp_data = np.load('073125_gene_expression/pvalb_exp_data.npy')\n",
    "sst_exp_data = np.load('073125_gene_expression/sst_exp_data.npy')\n",
    "vip_exp_data = np.load('073125_gene_expression/vip_exp_data.npy')\n",
    "ndnf_exp_data = np.load('073125_gene_expression/ndnf_exp_data.npy')\n",
    "\n",
    "rasgrf2_exp_data = np.load('073125_gene_expression/rasgrf2_exp_data.npy')\n",
    "cux2_exp_data = np.load('073125_gene_expression/cux2_exp_data.npy')\n",
    "rorb_exp_data = np.load('073125_gene_expression/rorb_exp_data.npy')\n",
    "trib2_exp_data = np.load('073125_gene_expression/trib2_exp_data.npy')\n",
    "b3galt2_exp_data = np.load('073125_gene_expression/b3galt2_exp_data.npy')\n",
    "ntng2_exp_data = np.load('073125_gene_expression/ntng2_exp_data.npy')\n",
    "tle4_exp_data = np.load('073125_gene_expression/tle4_exp_data.npy')\n",
    "ctgf_exp_data = np.load('073125_gene_expression/ctgf_exp_data.npy')\n",
    "\n",
    "HSE_GENES = ['BEND5','C1QL2','CACNA1E','COL24A1','COL6A1','CRYM','KCNC3',\n",
    "             'KCNH4','LGALS1','MFGE8','NEFH','SCN3B','SCN4B',\n",
    "             'SNCG','SV2C','SYT2','TPBG','VAMP1']\n",
    "\n",
    "bend5_exp_data = np.load('073125_gene_expression/bend5_exp_data.npy')\n",
    "c1ql2_exp_data = np.load('073125_gene_expression/c1ql2_exp_data.npy')\n",
    "cacna1e_exp_data = np.load('073125_gene_expression/cacna1e_exp_data.npy')\n",
    "col24a1_exp_data = np.load('073125_gene_expression/col24a1_exp_data.npy')\n",
    "col6a1_exp_data = np.load('073125_gene_expression/col6a1_exp_data.npy')\n",
    "kcnc3_exp_data = np.load('073125_gene_expression/kcnc3_exp_data.npy')\n",
    "grin2b_exp_data = np.load('073125_gene_expression/grin2b_exp_data.npy')\n",
    "'''\n",
    "\n",
    "# Compute the nanmean across all genes in HSE_GENES\n",
    "HSE_GENES = ['BEND5','C1QL2','CACNA1E','COL24A1','COL6A1','CRYM','KCNC3',\n",
    "             'KCNH4','LGALS1','MFGE8','NEFH','SCN3B','SCN4B',\n",
    "             'SNCG','SV2C','SYT2','TPBG','VAMP1']\n",
    "hse_gene_data = []\n",
    "for gene in HSE_GENES:\n",
    "    gene_exp = get_gene_expression(expression, gene)\n",
    "    hse_gene_data.append(gene_exp)\n",
    "\n",
    "\n",
    "hse_gene_data = np.stack(hse_gene_data, axis=0)\n",
    "print(hse_gene_data.shape)\n",
    "the_gene_data = np.nanmean(hse_gene_data, axis=-1)\n",
    "the_gene_data = np.sum(the_gene_data, axis=0)\n",
    "xlabel_name = 'HSE_GENES (mean)'\n",
    "print(the_gene_data.shape)\n",
    "\n",
    "\n",
    "the_gene_data = np.nanmean(get_gene_expression(expression, \"SST\")-get_gene_expression(expression, \"PVALB\"), axis=-1)\n",
    "xlabel_name = 'SST - PVALB'\n",
    "#xlabel_name ='PVALB'\n",
    "#the_gene_data = np.nanmean(get_gene_expression(expression, xlabel_name), axis=-1)\n",
    "\n",
    "# Configuration\n",
    "nPC = 0# 0 is the first PC\n",
    "font_size = 24\n",
    "tick_size = 20\n",
    "#data_type_ref_x = 'ratio_supra'\n",
    "\n",
    "for hemi in ['lh','rh']:\n",
    "\n",
    "    if hemi == 'lh':\n",
    "        gradient_data = lh_data_all_unmapped\n",
    "    else:\n",
    "        gradient_data = rh_data_all_unmapped\n",
    "\n",
    "   \n",
    "    data_types = [key for key in gradient_data.keys() if key != 'diff']# if key != 'diff' and key != 'ratio_supra' and key != 'ratio_infra' and key != 'relative' and key != data_type_ref_x]\n",
    "    n_plots = len(data_types)\n",
    "\n",
    "    # Plot settings\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_context(\"paper\", font_scale=1.2)\n",
    "    \n",
    "\n",
    "    # Create figure and grid\n",
    "    fig = plt.figure(figsize=(8*n_plots, 8))\n",
    "    # Increase spacing between subplots\n",
    "    gs = fig.add_gridspec(3, 3*n_plots, hspace=0.1, wspace=0.1)\n",
    "\n",
    "\n",
    "\n",
    "    def get_title_name(data_type):\n",
    "        titles = {\n",
    "            'diff': f'G{nPC+1}(Symmetry)',\n",
    "            'ratio_supra': f'G{nPC+1}(Supra/Total Ratio)',\n",
    "            'ratio_infra': f'G{nPC+1}(Infra/Total Ratio)', \n",
    "            'relative': f'G{nPC+1}(Relative)',\n",
    "            'he': f'G{nPC+1}(Hurst Exponent)',\n",
    "            'supra': f'G{nPC+1}(Supra Thickness)',\n",
    "            'infra': f'G{nPC+1}(Infra Thickness)',\n",
    "            'total': f'G{nPC+1}(Total Thickness)',\n",
    "            'total_hcp': f'G{nPC+1}(Total Thickness(HCP))',\n",
    "        }\n",
    "        return titles.get(data_type, '')\n",
    "\n",
    "\n",
    "    def get_data_for_hemisphere(hemi, data_type_ref_x, gradient_data, data_type, nPC, atlas, atlas_to_network):\n",
    "        if hemi == 'lh':\n",
    "            gradient = gradient_data[data_type][:,nPC]\n",
    "            gradient_ref = np.nan_to_num(the_gene_data[:200])\n",
    "            data_label_color = np.array(atlas_to_network)\n",
    "            atlas_min = np.min(atlas[atlas != 0])\n",
    "            atlas_max = np.max(atlas[atlas != 0])\n",
    "            new_data_label_color = []\n",
    "\n",
    "            print(data_label_color.shape)\n",
    "            for i,parcel in enumerate(range(atlas_min,atlas_max+1)):\n",
    "               \n",
    "                median_color = np.median(data_label_color[atlas == parcel,:],axis=0)\n",
    "       \n",
    "                new_data_label_color.append(tuple(median_color.tolist()))\n",
    "            data_label_color = new_data_label_color\n",
    "        else:\n",
    "            gradient = gradient_data[data_type][:,nPC]\n",
    "            gradient_ref = np.nan_to_num(the_gene_data[200:])\n",
    "            data_label_color = np.array(atlas_to_network)   \n",
    "            atlas_min = np.min(atlas[atlas != 0])\n",
    "            atlas_max = np.max(atlas[atlas != 0])\n",
    "            new_data_label_color = []\n",
    "\n",
    "            for i,parcel in enumerate(range(atlas_min,atlas_max+1)):\n",
    "                median_color = np.median(data_label_color[atlas == parcel,:],axis=0)\n",
    "                new_data_label_color.append(tuple(median_color.tolist()))\n",
    "            data_label_color = new_data_label_color\n",
    "\n",
    "        return gradient_ref, gradient, data_label_color\n",
    "    \n",
    "    \n",
    "\n",
    "    # Plot each data type\n",
    "    for plot_idx, data_type in enumerate(data_types):\n",
    "        # Get data\n",
    "        if hemi == 'lh':\n",
    "            atlas = atlas_data_lh\n",
    "        else:\n",
    "            atlas = atlas_data_rh\n",
    "\n",
    "        # Create network color mapping\n",
    "        atlas_to_network = [yeo_network_colors[atlas[i]] for i in range(len(atlas))]\n",
    "        gradient_ref, gradient, data_label_color = get_data_for_hemisphere(\n",
    "            hemi, data_type_ref_x, gradient_data, data_type, nPC, atlas, atlas_to_network\n",
    "        )\n",
    "        \n",
    "        # Calculate correlation\n",
    "        valid_mask = ~np.isnan(gradient_ref) & ~np.isnan(gradient)\n",
    "        r, p = stats.spearmanr(gradient_ref[valid_mask], gradient[valid_mask])\n",
    "        print(f\"{data_type}: r={r:.2f}, p={p:.2e}\")\n",
    "        \n",
    "        # Plot layout\n",
    "        col_start = 3 * plot_idx\n",
    "        col_end = col_start + 2\n",
    "        \n",
    "        # Main scatter plot\n",
    "        ax_scatter = fig.add_subplot(gs[1:, col_start:col_end])\n",
    "        sns.scatterplot(x=gradient_ref, y=gradient, alpha=1, s=150,\n",
    "                        c=data_label_color,edgecolor='black', ax=ax_scatter)\n",
    "        \n",
    "        # Add regression line and correlation text\n",
    "        x_range = np.array([gradient_ref.min(), gradient_ref.max()])\n",
    "        x_extended = np.array([x_range[0]*3.5, x_range[1]*3.5]) \n",
    "        slope, intercept = np.polyfit(gradient_ref[~np.isnan(gradient_ref)], \n",
    "                                    gradient[~np.isnan(gradient)], 1)\n",
    "        y_extended = slope * x_extended + intercept\n",
    "        ax_scatter.plot(x_extended, y_extended, color='black', linestyle='--', linewidth=4)\n",
    "        ax_scatter.text(0.05, 0.95, f'r={r:.2f}, p={p:.2e}',\n",
    "                        transform=ax_scatter.transAxes, va='top',\n",
    "                        fontsize=font_size, weight='bold', color='black')\n",
    "        \n",
    "        # Center and format scatter plot\n",
    "        x_mean = np.nanmean(gradient_ref)\n",
    "        x_std = np.nanstd(gradient_ref)\n",
    "        x_window = 3.5 * x_std\n",
    "        x_lim = (x_mean - x_window, x_mean + x_window)\n",
    "        ax_scatter.set_xlim(x_lim)\n",
    "        \n",
    "        y_mean = np.nanmean(gradient)\n",
    "        y_std = np.nanstd(gradient)\n",
    "        y_window = 3.5 * y_std\n",
    "        y_lim = (y_mean - y_window, y_mean + y_window)\n",
    "        ax_scatter.set_ylim(y_lim)\n",
    "        \n",
    "        ax_scatter.set_ylabel(f'{get_title_name(data_type)}', \n",
    "                            fontsize=font_size, labelpad=12, weight='bold')\n",
    "        ax_scatter.set_xlabel(xlabel_name, fontsize=font_size, labelpad=12, weight='bold')\n",
    "        ax_scatter.set_title(get_title_name(data_type), fontsize=font_size+2, pad=20, weight='bold')\n",
    "        ax_scatter.tick_params(axis='both', which='major', labelsize=tick_size)\n",
    "        ax_scatter.grid(True, linestyle='--', alpha=0)\n",
    "        \n",
    "        for spine in ax_scatter.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(2.0)\n",
    "        \n",
    "        # Top histogram - align with scatter plot x-axis\n",
    "        ax_histx = fig.add_subplot(gs[0, col_start:col_end])\n",
    "        sns.histplot(data=gradient_ref, bins=50, kde=True, color='#8E44AD',\n",
    "                    ax=ax_histx, stat='density', alpha=1)\n",
    "        ax_histx.set_xlim(x_lim)\n",
    "        ylim = ax_histx.get_ylim()\n",
    "        ax_histx.set_ylim(0, ylim[1]*3)\n",
    "        ax_histx.set(xlabel='', ylabel='')\n",
    "        ax_histx.set_yticks([])\n",
    "        ax_histx.tick_params(labelbottom=False, labelsize=tick_size)\n",
    "        for spine in ax_histx.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        \n",
    "        # Right histogram - align with scatter plot y-axis\n",
    "        ax_histy = fig.add_subplot(gs[1:, col_end])\n",
    "        sns.histplot(y=gradient, bins=50, kde=True, color='#8E44AD',\n",
    "                    ax=ax_histy, stat='density', alpha=1)\n",
    "        ax_histy.set_ylim(y_lim)\n",
    "        xlim = ax_histy.get_xlim()\n",
    "        ax_histy.set_xlim(0, xlim[1]*3)\n",
    "        ax_histy.set(xlabel='', ylabel='')\n",
    "        ax_histy.set_xticks([])\n",
    "        ax_histy.tick_params(labelleft=False, labelsize=tick_size)\n",
    "        for spine in ax_histy.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_plot.png', dpi=300, bbox_inches='tight',\n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This rewrite matches the plotting/layout conventions of @file_context_0 (cells 23-60):\n",
    "# - Creates subplots (fig,axes) in advance\n",
    "# - Loops over genes, passing `ax` to scatter_with_histograms as main_axes[pc] is used before\n",
    "# - Respects the same width/height ratio and tight_layout details\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "GENES2PLOT = ['LAMP5','SNCG','VIP','SST','PVALB']\n",
    "#GENES2PLOT = [('SST', 'PVALB'),('VIP', 'SST'),('VIP','PVALB')]\n",
    "\n",
    "\n",
    "hemi = 'lh'\n",
    "\n",
    "exp_type = 'dfa'#'ae' for aperiodic exponent or 'dfa' detrended fluctuation analysis\n",
    "\n",
    "if exp_type == 'ae':\n",
    "    YLABEL_NAME = 'Aperiodic Exponent' #'Aperiodic Exponent'\n",
    "    exp_data_path = '/Users/dennis.jungchildmind.org/Desktop/MEG/112325_fooof_exponent/fooof_brainic_parcelonly.pkl'\n",
    "elif exp_type == 'dfa':\n",
    "    YLABEL_NAME = 'DFA exponent'\n",
    "    # Load DFA (Y-axis) data for this frequency\n",
    "    freq_range = [8.0, 15.0]\n",
    "    exp_data_path = f'/Users/dennis.jungchildmind.org/Desktop/MEG/112225_dfa_outputs/dfa_restin_f{freq_range[0]}_to_{freq_range[1]}_parcelonly.pkl'\n",
    "\n",
    "exp_data = pickle.load(open(exp_data_path, 'rb'))\n",
    "exp_data_mean = np.nanmean(exp_data, axis=0)\n",
    "exp_data_mean_lh = exp_data_mean[0:int(num_parcels/2)]\n",
    "exp_data_mean_rh = exp_data_mean[int(num_parcels/2):]\n",
    "\n",
    "# --- Handle tuple for subtraction, otherwise just plot each gene in list ---\n",
    "processed_genes = []\n",
    "xlabel_names = []\n",
    "\n",
    "for gene in GENES2PLOT:\n",
    "    if isinstance(gene, tuple):\n",
    "        # Do subtraction between first and second entry (gene1 - gene2)\n",
    "        data1 = np.nanmean(get_gene_expression(expression, gene[0]), axis=-1)\n",
    "        data2 = np.nanmean(get_gene_expression(expression, gene[1]), axis=-1)\n",
    "        gene_data = data1 - data2\n",
    "        processed_genes.append(gene_data)\n",
    "        xlabel_names.append(f\"{gene[0]} - {gene[1]}\")\n",
    "    else:\n",
    "        gene_data = np.nanmean(get_gene_expression(expression, gene), axis=-1)\n",
    "        processed_genes.append(gene_data)\n",
    "        xlabel_names.append(gene)\n",
    "\n",
    "num_genes_to_plot = len(processed_genes)\n",
    "fig, main_axes = plt.subplots(1, num_genes_to_plot, figsize=(num_genes_to_plot**2, num_genes_to_plot))\n",
    "\n",
    "if num_genes_to_plot == 1:\n",
    "    main_axes = [main_axes]  # Ensure iterable if single plot\n",
    "\n",
    "\n",
    "for idx, (the_gene_data, xlabel_name) in enumerate(zip(processed_genes, xlabel_names)):\n",
    "    ax = main_axes[idx]\n",
    "    ylabel_name = YLABEL_NAME\n",
    "    if hemi == 'lh':\n",
    "        the_gene_data = the_gene_data[:200]\n",
    "       # exponent_parcellated = parcellate_data(exp_data_lh, atlas_data)\n",
    "        exponent_parcellated = exp_data_mean_lh\n",
    "    else:\n",
    "        the_gene_data = the_gene_data[200:]\n",
    "        #exponent_parcellated = parcellate_data(exp_data_rh, atlas_data)\n",
    "        exponent_parcellated = exp_data_mean_rh\n",
    "    valid_mask = ~np.isnan(the_gene_data) & ~np.isnan(exponent_parcellated)\n",
    "    x = the_gene_data[valid_mask]\n",
    "    y = exponent_parcellated[valid_mask]\n",
    "    # Use color convention: first 200 for lh, second 200 for rh. +1 offset for matched color scheme.\n",
    "    if hemi == 'lh':\n",
    "        color_subset = yeo_network_colors[1:201]\n",
    "    else:\n",
    "        color_subset = yeo_network_colors[201:]\n",
    "    data_label_color = [c for c, k in zip(color_subset, valid_mask) if k]\n",
    "\n",
    "    _, _, stats_dict = scatter_with_histograms(\n",
    "        x, y, xlabel_name, ylabel_name,\n",
    "        marker_size=len(GENES2PLOT)*10, data_label_color=data_label_color, ax=ax\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "corr_file = 'meg_exp_correlations.npy'\n",
    "pval_file = 'meg_exp_pvalues.npy'\n",
    "genes_file = 'meg_exp_gene_names.npy'\n",
    "\n",
    "# Try to load precomputed data if available\n",
    "if os.path.exists(corr_file) and os.path.exists(pval_file) and os.path.exists(genes_file):\n",
    "    correlations = np.load(corr_file)\n",
    "    p_values = np.load(pval_file)\n",
    "    ALL_GENES = np.load(genes_file, allow_pickle=True).tolist()\n",
    "    n_genes = len(ALL_GENES)\n",
    "    print(\"Loaded correlation results from files.\")\n",
    "else:\n",
    "    # Initialize arrays to store correlation results\n",
    "    ALL_GENES = expression['9861'].columns.tolist()\n",
    "    n_genes = len(ALL_GENES)\n",
    "\n",
    "    # Arrays to store correlation coefficients and p-values\n",
    "    correlations = np.zeros((n_genes, 2)) # 2 for lh and rh\n",
    "    p_values = np.zeros((n_genes, 2))\n",
    "\n",
    "    # Loop through each gene\n",
    "    for gene_idx, gene in enumerate(ALL_GENES):\n",
    "        #print(f\"\\rProcessing gene {gene} ({gene_idx+1}/{n_genes})\", end='', flush=True)\n",
    "        \n",
    "        # Get gene expression data\n",
    "        gene_exp_data = get_gene_expression(expression, gene)\n",
    "        the_gene_data = np.nanmean(gene_exp_data, axis=-1)\n",
    "\n",
    "        # Process each hemisphere\n",
    "        for hemi_idx, hemi in enumerate(['lh', 'rh']):\n",
    "            if hemi == 'lh':\n",
    "                exp_data = parcellate_data(exp_data_lh, atlas_data_lh)\n",
    "                gene_ref = np.nan_to_num(the_gene_data[:int(num_parcels/2)])\n",
    "            else:\n",
    "                exp_data = parcellate_data(exp_data_rh, atlas_data_rh)\n",
    "                gene_ref = np.nan_to_num(the_gene_data[int(num_parcels/2):])\n",
    "\n",
    "            # Calculate correlation\n",
    "            valid_mask = ~np.isnan(gene_ref) & ~np.isnan(exp_data)\n",
    "            r, p = stats.spearmanr(gene_ref[valid_mask], exp_data[valid_mask])\n",
    "\n",
    "            # Store results\n",
    "            correlations[gene_idx, hemi_idx] = r\n",
    "            p_values[gene_idx, hemi_idx] = p\n",
    "\n",
    "    # Save results\n",
    "    np.save(corr_file, correlations)\n",
    "    np.save(pval_file, p_values)\n",
    "    np.save(genes_file, np.array(ALL_GENES))\n",
    "    print(\"Correlation results computed and saved to files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene,layer type,hemi (hemi means gradient1,graident2 sincne both hemisphere have the same level of gene expression level as they are reflected*)\n",
    "from scipy.stats import false_discovery_control\n",
    "target_gene_index = ALL_GENES.index('LAMP5')\n",
    "pvalb_corr = correlations[target_gene_index, 0]  # LH correlation for PVALB\n",
    "\n",
    "# Plot using a density curve instead of histogram\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Kernel density estimation\n",
    "corr_data = correlations[:,0]\n",
    "corr_data = corr_data[~np.isnan(corr_data)]  # remove NaNs if any\n",
    "\n",
    "fig = plt.figure(figsize=(4,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(corr_data - np.mean(corr_data), bins=30, color='black', alpha=0.8)\n",
    "ax.axvline(x=pvalb_corr - np.mean(corr_data), color='red', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel(\"Spatial Correlation (r)\", fontsize=18)\n",
    "ax.set_xlim(-1,1)\n",
    "ax.legend(fontsize=16)\n",
    "# Make tick labels bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "# Remove top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "alpha = 0.05\n",
    "#the second dimension is layer type [['total', 'supra', 'infra', 'ratio_supra', 'ratio_infra', 'relative']]\n",
    "#the third dimension is hemisphere [lh,rh]\n",
    "corrected_pvals = false_discovery_control(p_values[:,0], method='bh')\n",
    "\n",
    "significant_indices = np.where(corrected_pvals < alpha)[0]\n",
    "print(significant_indices)\n",
    "print(p_values[significant_indices])\n",
    "print(correlations[significant_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "PC_INDEX = 0#of cortical thickness gradient\n",
    "LAYER_TYPE = 'ratio_supra'\n",
    "GENE_AXIS_LABEL = \"SST-PVALB\"\n",
    "Y_AXIS_LABEL = \"DFA Exponent\"\n",
    "the_gene_data = np.nanmean(get_gene_expression(expression, \"SST\")-get_gene_expression(expression, \"PVALB\"), axis=-1)\n",
    "\n",
    "#GENE_AXIS_LABEL = \"PVALB\"\n",
    "#the_gene_data = np.nanmean(get_gene_expression(expression, GENE_AXIS_LABEL), axis=-1)\n",
    "the_gene_data_lh = the_gene_data[:int(num_parcels/2)]\n",
    "the_gene_data_lh = the_gene_data[:int(num_parcels/2)]\n",
    "\n",
    "'''\n",
    "#MEG sensor space \n",
    "exp_data_path = '/Users/dennis.jungchildmind.org/Downloads/exponent_result1.shape.gii'\n",
    "#exp_data_path = '/Users/dennis.jungchildmind.org/Downloads/offset_result1.shape.gii'\n",
    "exp_data = nib.load(exp_data_path)\n",
    "exp_data = exp_data.darrays[2]#0 is vertices, 1 is faces, 2 is exponent values\n",
    "\n",
    "exp_data_lh = exp_data.data[0:32492]\n",
    "exp_data_rh = exp_data.data[32492:]\n",
    "\n",
    "exp_dat_lh_parc = parcellate_data(exp_data_lh, atlas_data_lh)\n",
    "exp_dat_lh_parc = parcellate_data(exp_data_lh, atlas_data_lh)\n",
    "'''\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define your list of frequency ranges\n",
    "freq_ranges = [\n",
    "    [0.1, 4.0],\n",
    "    [4.0, 8.0],\n",
    "    [8.0, 15.0],\n",
    "    [15.0, 30.0],\n",
    "    [30.0, 60.0]\n",
    "]\n",
    "\n",
    "font_size = 17\n",
    "\n",
    "# Store correlation stats to print all at end, if desired\n",
    "corr_stats = []\n",
    "\n",
    "# Prepare figure with one row per freq_range, 3 columns\n",
    "fig, axes = plt.subplots(len(freq_ranges), 3, figsize=(3*3, 3*len(freq_ranges)), squeeze=False)\n",
    "\n",
    "for i, freq_range in enumerate(freq_ranges):\n",
    "    # --- Load E/I data for this freq_range ---\n",
    "    dfa_data_path = f'/Users/dennis.jungchildmind.org/Desktop/MEG/112225_dfa_outputs/dfa_restin_f{freq_range[0]}_to_{freq_range[1]}_parcelonly.pkl'\n",
    "    dfa_data = pickle.load(open(dfa_data_path, 'rb'))\n",
    "    dfa_data_mean = np.nanmean(dfa_data, axis=0)\n",
    "    exp_dat_lh_parc = dfa_data_mean[0:int(num_parcels/2)]\n",
    "    exp_dat_rh_parc = dfa_data_mean[int(num_parcels/2):]\n",
    "\n",
    "    # --- Compose predictors ---\n",
    "    X = np.column_stack([exp_dat_lh_parc, the_gene_data_lh])\n",
    "\n",
    "    # --- Gradient axis and plotting window ---\n",
    "    y = lh_data_all_unmapped[LAYER_TYPE][:, PC_INDEX]\n",
    "    y_mean = np.nanmean(y)\n",
    "    y_std = np.nanstd(y)\n",
    "    y_window = 3.5 * y_std\n",
    "    y_lim = (y_mean - y_window, y_mean + y_window)\n",
    "\n",
    "    # Optionally check/print data for this freq_range\n",
    "    print(f\"\\n[Freq {freq_range[0]}-{freq_range[1]} Hz]\")\n",
    "    print(f\"Original gradient shape: {lh_data_all_unmapped[LAYER_TYPE].shape}\")\n",
    "    print(f\"X (LIMITED) shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"Aperiodic Exponent min/max after clip: {X[:,0].min():.3f}, {X[:,0].max():.3f}\")\n",
    "    print(f\"Gene SST-PVALB min/max after clip: {X[:,1].min():.3f}, {X[:,1].max():.3f}\")\n",
    "\n",
    "    # --- Remove NaNs ---\n",
    "    index = ~np.isnan(X).any(axis=1) & ~np.isnan(y)\n",
    "    X_clean = X[index]\n",
    "    y_clean = y[index]\n",
    "\n",
    "    print(f\"N regions after removing NaNs: {len(y_clean)}\")\n",
    "\n",
    "    # --- Correlations (Step 1) ---\n",
    "    r_EI, p_EI = spearmanr(X_clean[:, 0], y_clean)\n",
    "    r_gene, p_gene = spearmanr(X_clean[:, 1], y_clean)\n",
    "\n",
    "    # --- Partial \"multiple regression\" (Step 2) ---\n",
    "    model_ei_gene = LinearRegression().fit(X_clean[:, [1]], X_clean[:, 0])\n",
    "    ei_resid = X_clean[:, 0] - model_ei_gene.predict(X_clean[:, [1]])\n",
    "    model_gene_ei = LinearRegression().fit(X_clean[:, [0]], X_clean[:, 1])\n",
    "    gene_resid = X_clean[:, 1] - model_gene_ei.predict(X_clean[:, [0]])\n",
    "    r_partial_ei, p_partial_ei = spearmanr(ei_resid, y_clean)\n",
    "    r_partial_gene, p_partial_gene = spearmanr(gene_resid, y_clean)\n",
    "\n",
    "    # Print to console for each band:\n",
    "    print(f\"\\nUnivariate correlations (Spearman):\")\n",
    "    print(f\"  E/I:  r={r_EI:.3f}, p={p_EI:.4f}\")\n",
    "    print(f\"  Gene: r={r_gene:.3f}, p={p_gene:.4f}\")\n",
    "    print(f\"\\nPartial Spearman correlations:\")\n",
    "    print(f\"  Gradient vs E/I (controlling for gene):    r={r_partial_ei:.3f}, p={p_partial_ei:.4f}\")\n",
    "    print(f\"  Gradient vs Gene (controlling for E/I):    r={r_partial_gene:.3f}, p={p_partial_gene:.4f}\")\n",
    "    print(\"\\nVariance explained (approximated by r):\")\n",
    "    print(f\"  E/I only:      r={r_EI**2:.3f}\")\n",
    "    print(f\"  Gene only:     r={r_gene**2:.3f}\")\n",
    "    print(f\"  E/I|Gene:      r={r_partial_ei**2:.3f}\")\n",
    "    print(f\"  Gene|E/I:      r={r_partial_gene**2:.3f}\")\n",
    "\n",
    "    # --- Store stats if desired ---\n",
    "    corr_stats.append(dict(\n",
    "        freq_range=f\"{freq_range[0]}-{freq_range[1]}Hz\",\n",
    "        r_EI=r_EI, p_EI=p_EI,\n",
    "        r_gene=r_gene, p_gene=p_gene,\n",
    "        r_partial_ei=r_partial_ei, p_partial_ei=p_partial_ei,\n",
    "        r_partial_gene=r_partial_gene, p_partial_gene=p_partial_gene\n",
    "    ))\n",
    "\n",
    "    # --- Plotting (Step 3) ---\n",
    "    ax_row = axes[i]\n",
    "    # E/I vs gradient\n",
    "    ax_row[0].scatter(y_clean, X_clean[:, 0], alpha=0.5, s=30)\n",
    "    ax_row[0].set_xlabel(f'Gradient {PC_INDEX+1} Scores', fontsize=16, fontweight='bold')\n",
    "    ax_row[0].set_ylabel(Y_AXIS_LABEL, fontsize=16, fontweight='bold')\n",
    "    ax_row[0].set_xlim(y_lim)\n",
    "    ax_row[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_row[0].tick_params(axis='both', which='minor', labelsize=14)\n",
    "    ax_row[0].spines['top'].set_visible(False)\n",
    "    ax_row[0].spines['right'].set_visible(False)\n",
    "    ax_row[0].text(\n",
    "        0.99, 0.99, f\"r={r_EI:.2f}, p={p_EI:.2e}\",\n",
    "        ha=\"right\", va=\"top\", transform=ax_row[0].transAxes,\n",
    "        fontsize=font_size*0.7, weight='bold', color='black'\n",
    "    )\n",
    "    ax_row[0].set_box_aspect(1)\n",
    "    ax_row[0].set_title(f'{freq_range[0]}-{freq_range[1]}Hz: Grad vs. E/I', fontsize=15)\n",
    "\n",
    "    # Gene vs gradient\n",
    "    ax_row[1].scatter(y_clean, X_clean[:, 1], alpha=0.5, s=30, color='orange')\n",
    "    ax_row[1].set_xlabel(f'Gradient {PC_INDEX+1} Scores', fontsize=16, fontweight='bold')\n",
    "    ax_row[1].set_ylabel(GENE_AXIS_LABEL, fontsize=16, fontweight='bold')\n",
    "    ax_row[1].set_xlim(y_lim)\n",
    "    ax_row[1].set_ylim(-3, 3)\n",
    "    ax_row[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_row[1].tick_params(axis='both', which='minor', labelsize=14)\n",
    "    ax_row[1].spines['top'].set_visible(False)\n",
    "    ax_row[1].spines['right'].set_visible(False)\n",
    "    ax_row[1].text(\n",
    "        0.99, 0.99, f\"r={r_gene:.2f}, p={p_gene:.2e}\",\n",
    "        ha=\"right\", va=\"top\", transform=ax_row[1].transAxes,\n",
    "        fontsize=font_size*0.7, weight='bold', color='black'\n",
    "    )\n",
    "    ax_row[1].set_box_aspect(1)\n",
    "    ax_row[1].set_title('Grad vs. Gene', fontsize=15)\n",
    "\n",
    "    # Residualized/pseudo-multivariate fit\n",
    "    ax_row[2].scatter(y_clean, ei_resid, alpha=0.5, s=30, color='green')\n",
    "    ax_row[2].set_xlabel(f'Gradient {PC_INDEX+1} Scores', fontsize=16, fontweight='bold')\n",
    "    ax_row[2].set_ylabel(f'{Y_AXIS_LABEL}\\n(resid E/I | Gene)', fontsize=16, fontweight='bold')\n",
    "    ax_row[2].set_xlim(y_lim)\n",
    "    ax_row[2].tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax_row[2].tick_params(axis='both', which='minor', labelsize=14)\n",
    "    ax_row[2].spines['top'].set_visible(False)\n",
    "    ax_row[2].spines['right'].set_visible(False)\n",
    "    ax_row[2].text(\n",
    "        0.99, 0.99, f\"r={r_partial_ei:.2f}, p={p_partial_ei:.2e}\",\n",
    "        ha=\"right\", va=\"top\", transform=ax_row[2].transAxes,\n",
    "        fontsize=font_size*0.7, weight='bold', color='black'\n",
    "    )\n",
    "    ax_row[2].set_box_aspect(1)\n",
    "    ax_row[2].set_title('Grad vs. E/I (control gene)', fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Organized analysis:\n",
    "1. Prepare gene data matrix for defined genes & combinations.\n",
    "2. Prepare DFA feature matrix for all frequency ranges.\n",
    "3. Prepare cortical gradient matrices for all layer types.\n",
    "4. Concatenate gene and DFA features.\n",
    "5. Compute Spearman correlations with gradients.\n",
    "6. Plot and display.\n",
    "\"\"\"\n",
    "\n",
    "# ==== Imports and Config ====\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "ALPHA = 0.0001\n",
    "\n",
    "# ---- CONFIGURATION ----\n",
    "#GENES\n",
    "TARGET_GENES = ['PVALB', 'SST', 'VIP','LAMP5','SNCG']\n",
    "#TARGET_GENES = sorted((list(expression['9861'].keys())))\n",
    "def _label_from_gene(g):\n",
    "    if isinstance(g, tuple):\n",
    "        if len(g) == 2:\n",
    "            return f\"{g[0]}-{g[1]}\"\n",
    "        else:\n",
    "            return \"-\".join(g)\n",
    "    else:\n",
    "        return g\n",
    "TARGET_GENES_LABELS = [_label_from_gene(g) for g in TARGET_GENES]\n",
    "#DFA EXPONENTS\n",
    "TARGET_FREQ_RANGES = [[0.1, 4.0], [4.0, 8.0], [8.0, 15.0], [15.0, 30.0], [30.0, 60.0]]\n",
    "TARGET_FREQ_LABELS = ['DF(Delta)', 'DF(Theta)', 'DF(Alpha)', 'DF(Beta)', 'DF(Gamma)']\n",
    "\n",
    "#RSS EXPONENTS\n",
    "TARGET_RS_LABELS = ['RS(Delta)', 'RS(Theta)', 'RS(Alpha)', 'RS(Beta)', 'RS(Gamma)']\n",
    "\n",
    "#APERIOD EXPONENTS\n",
    "TARGET_APERIOD_EXPONENTS = ['AE(brain)']#, 'AE(all)', 'AO(brain)', 'AO(all)']\n",
    "#GRADIENTS\n",
    "TARGET_LAYER_TYPES = ['total_hcp','total','supra', 'infra', 'ratio_supra', 'ratio_infra']\n",
    "TARGET_LAYER_TYPES_LABELS = ['HCP: Total','Ex: Total', 'Ex: Supra', 'Ex: Infra', 'Ex: Supra/Total', 'Ex: Infra/Total']\n",
    "\n",
    "PC_INDEX = 1   # Principal component for gradients\n",
    "hemi = 'lh'    # Hemisphere selection; assumes 200 parcels per hemisphere\n",
    "\n",
    "# ==== 1. PREPARE GENE DATA MATRIX ====\n",
    "def get_parcel_gene_data(gene, hemi):\n",
    "    \"\"\"Return mean (across donors) gene expression for the selected hemisphere.\"\"\"\n",
    "    if isinstance(gene, tuple):\n",
    "        if len(gene) == 2:\n",
    "            g1 = np.nanmean(get_gene_expression(expression, gene[0]), axis=-1)\n",
    "            g2 = np.nanmean(get_gene_expression(expression, gene[1]), axis=-1)\n",
    "            vals = g1 - g2\n",
    "            label = f\"{gene[0]}-{gene[1]}\"\n",
    "        elif len(gene) > 2:\n",
    "            print(gene)\n",
    "            vals = np.zeros_like(np.nanmean(get_gene_expression(expression, gene[0]), axis=-1))\n",
    "            label = \"-\".join(gene)\n",
    "            for g in gene:\n",
    "                vals += np.nanmean(get_gene_expression(expression, g), axis=-1)\n",
    "    else:\n",
    "        vals = np.nanmean(get_gene_expression(expression, gene), axis=-1)\n",
    "        label = gene\n",
    "    # Split by hemisphere\n",
    "    if hemi == 'lh':\n",
    "        vals = vals[:200]\n",
    "    else:\n",
    "        vals = vals[200:]\n",
    "    return vals, label\n",
    "\n",
    "gene_data_matrix = []\n",
    "gene_data_labels = []\n",
    "for gene in TARGET_GENES:\n",
    "    vals, label = get_parcel_gene_data(gene, hemi)\n",
    "    gene_data_matrix.append(vals)\n",
    "    gene_data_labels.append(label)\n",
    "gene_data_matrix = np.stack(gene_data_matrix, axis=1)  # shape: (n_parcels, n_genes)\n",
    "\n",
    "# ==== 2. PREPARE DFA FEATURE MATRIX ====\n",
    "def get_dfa_matrix(freq_ranges, hemi):\n",
    "    dfa_matrix = []\n",
    "    for freq_range in freq_ranges:\n",
    "        dfa_data_path = f'/Users/dennis.jungchildmind.org/Desktop/MEG/112225_dfa_outputs/dfa_restin_f{freq_range[0]}_to_{freq_range[1]}_parcelonly.pkl'\n",
    "        dfa_data = pickle.load(open(dfa_data_path, 'rb'))\n",
    "        dfa_data_mean = np.nanmean(dfa_data, axis=0)\n",
    "        if hemi == 'lh':\n",
    "            dfa_vals = dfa_data_mean[0:int(num_parcels/2)]\n",
    "        else:\n",
    "            dfa_vals = dfa_data_mean[int(num_parcels/2):]\n",
    "        dfa_matrix.append(dfa_vals)\n",
    "    dfa_matrix = np.array(dfa_matrix).T  # parcels x freq\n",
    "    return dfa_matrix\n",
    "\n",
    "dfa_matrix_lh = get_dfa_matrix(TARGET_FREQ_RANGES, \"lh\")\n",
    "dfa_matrix_rh = get_dfa_matrix(TARGET_FREQ_RANGES, \"rh\")\n",
    "\n",
    "# ==== 2. PREPARE RRS FEATURE MATRIX ====\n",
    "def get_rrs_matrix(freq_ranges, hemi):\n",
    "    rrs_matrix = []\n",
    "    for freq_range in freq_ranges:\n",
    "        rrs_data_path = f'/Users/dennis.jungchildmind.org/Desktop/MEG/112625_rrs_outputs/rrs_restin_f{freq_range[0]}_to_{freq_range[1]}_parcelonly.pkl'\n",
    "        rrs_data = pickle.load(open(rrs_data_path, 'rb'))\n",
    "        rrs_data_mean = np.nanmean(rrs_data, axis=0)\n",
    "        if hemi == 'lh':\n",
    "            rrs_vals = rrs_data_mean[0:int(num_parcels/2)]\n",
    "        else:\n",
    "            rrs_vals = rrs_data_mean[int(num_parcels/2):]\n",
    "        rrs_matrix.append(rrs_vals)\n",
    "    rrs_matrix = np.array(rrs_matrix).T  # parcels x freq\n",
    "    return rrs_matrix\n",
    "\n",
    "rrs_matrix_lh = get_rrs_matrix(TARGET_FREQ_RANGES, \"lh\")\n",
    "rrs_matrix_rh = get_rrs_matrix(TARGET_FREQ_RANGES, \"rh\")\n",
    "\n",
    "# ==== 2.5. PREPARE APERIODIC EXPONENT MATRIX ====\n",
    "def get_aperiodic_exponent_matrix(hemi, data_type):\n",
    "    ae_matrix = []\n",
    "    ae_data_path = f'/Users/dennis.jungchildmind.org/Desktop/MEG/112325_fooof_exponent/fooof_{data_type}_parcelonly.pkl'\n",
    "    ae_data = pickle.load(open(ae_data_path, 'rb'))\n",
    "    ae_data_mean = np.nanmean(ae_data, axis=0)\n",
    "    if hemi == 'lh':\n",
    "        ae_vals = ae_data_mean[0:int(num_parcels/2)]\n",
    "    else:\n",
    "        ae_vals = ae_data_mean[int(num_parcels/2):]\n",
    "    ae_matrix.append(ae_vals)\n",
    "    ae_matrix = np.array(ae_matrix).T  # parcels x freq\n",
    "    return ae_matrix\n",
    "\n",
    "ae_brainic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='brainic')\n",
    "ae_brainic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='brainic')\n",
    "ae_allic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='allic')\n",
    "ae_allic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='allic')\n",
    "\n",
    "ao_brainic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='offset_brainic')\n",
    "ao_brainic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='offset_brainic')\n",
    "ao_allic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='offset_allic')\n",
    "ao_allic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='offset_allic')\n",
    "\n",
    "\n",
    "# ==== 3. PREPARE GRADIENT MATRIX ====\n",
    "def get_gradient_matrix(layer_types, PC_INDEX, hemi):\n",
    "    mat = []\n",
    "    data_dict = lh_data_all_unmapped if hemi == 'lh' else rh_data_all_unmapped\n",
    "    for layer_type in layer_types:\n",
    "        mat.append(data_dict[layer_type][:, PC_INDEX])\n",
    "    return np.array(mat).T  # shape: (n_parcels, n_layers)\n",
    "\n",
    "gradient_matrix_lh = get_gradient_matrix(TARGET_LAYER_TYPES, PC_INDEX, \"lh\")\n",
    "gradient_matrix_rh = get_gradient_matrix(TARGET_LAYER_TYPES, PC_INDEX, \"rh\")\n",
    "print(\"Gradient matrix (lh) shape:\", gradient_matrix_lh.shape)\n",
    "\n",
    "# ==== 4. CONCATENATE GENE AND DFA FEATURES ====\n",
    "gene_dfa_matrix_lh = np.concatenate([gene_data_matrix, dfa_matrix_lh, rrs_matrix_lh, ae_brainic_matrix_lh], axis=1)\n",
    "gene_dfa_matrix_rh = np.concatenate([gene_data_matrix, dfa_matrix_rh, rrs_matrix_rh, ae_brainic_matrix_rh], axis=1)\n",
    "print(\"Gene/DFA matrix (lh) shape:\", gene_dfa_matrix_lh.shape)\n",
    "print(\"Gene/DFA matrix (rh) shape:\", gene_dfa_matrix_rh.shape)\n",
    "\n",
    "# ==== 5. COMPUTE SPEARMAN'S CORRELATION ====\n",
    "def compute_spearman_matrix(X, Y):\n",
    "    \"\"\"\n",
    "    X: shape (n_parcels, n_x_features)\n",
    "    Y: shape (n_parcels, n_y_features)\n",
    "    Returns: corr & p-value matrices: shape (n_x_features, n_y_features)\n",
    "    \"\"\"\n",
    "    n_x, n_y = X.shape[1], Y.shape[1]\n",
    "    corrs = np.zeros((n_x, n_y))\n",
    "    ps = np.zeros_like(corrs)\n",
    "    for i in range(n_x):\n",
    "        for j in range(n_y):\n",
    "            r, p = spearmanr(X[:, i], Y[:, j], nan_policy='omit')\n",
    "            corrs[i, j] = r\n",
    "            ps[i, j] = p\n",
    "    return corrs, ps\n",
    "\n",
    "# Compute Spearman's correlation and p-values\n",
    "spearman_corrs_lh, spearman_ps_lh = compute_spearman_matrix(gene_dfa_matrix_lh, gradient_matrix_lh)\n",
    "spearman_corrs_rh, spearman_ps_rh = compute_spearman_matrix(gene_dfa_matrix_rh, gradient_matrix_rh)\n",
    "\n",
    "# Apply multiple comparisons correction (FDR) across all p-values (lh)\n",
    "pvals_lh_flat = spearman_ps_lh.flatten()\n",
    "rej_lh, pvals_fdr_lh, _, _ = multipletests(pvals_lh_flat, alpha=ALPHA, method='fdr_bh')\n",
    "spearman_ps_fdr_lh = pvals_fdr_lh.reshape(spearman_ps_lh.shape)\n",
    "# NOTE: In statsmodels' multipletests, \"rej_lh\" is True (1) for *significant* (not rejected) null hypothesis, i.e., significant correlations.\n",
    "# So, spearman_signif_lh == 1 means significant (null rejected), 0 means not significant (null not rejected).\n",
    "spearman_signif_lh = rej_lh.reshape(spearman_ps_lh.shape)\n",
    "\n",
    "# Same for rh\n",
    "pvals_rh_flat = spearman_ps_rh.flatten()\n",
    "rej_rh, pvals_fdr_rh, _, _ = multipletests(pvals_rh_flat, alpha=ALPHA, method='fdr_bh')\n",
    "spearman_ps_fdr_rh = pvals_fdr_rh.reshape(spearman_ps_rh.shape)\n",
    "spearman_signif_rh = rej_rh.reshape(spearman_ps_rh.shape)\n",
    "\n",
    "print(\"LH Spearman correlation matrix shape:\", spearman_corrs_lh.shape)\n",
    "print(\"RH Spearman correlation matrix shape:\", spearman_corrs_rh.shape)\n",
    "\n",
    "# ==== 6. PLOTTING ====\n",
    "\n",
    "# ---- Plotting helpers ----\n",
    "small_font = 12\n",
    "smaller_font = 10\n",
    "\n",
    "def plot_fixed_size_imshow(ax, data, vmin=None, vmax=None, cmap='viridis', aspect_ratio=1, fixed_width=3, fixed_height=6, **imshow_kwargs):\n",
    "    \"\"\"\n",
    "    Plots an image on 'ax' such that the Axes size fits the requested fixed pixel size for the image portion.\n",
    "    Args:\n",
    "        ax: Matplotlib Axes.\n",
    "        data: Image array.\n",
    "        vmin, vmax: for imshow.\n",
    "        cmap: Colormap.\n",
    "        aspect_ratio: Height/Width; if None defaults to data shape.\n",
    "        fixed_width, fixed_height: in inches.\n",
    "        imshow_kwargs: Additional kwargs for imshow.\n",
    "    Returns:\n",
    "        The image object (from imshow)\n",
    "    \"\"\"\n",
    "    fig = ax.figure\n",
    "    # Compute aspect to get correct physical extent\n",
    "    if aspect_ratio is None:\n",
    "        aspect_ratio = data.shape[0] / data.shape[1]\n",
    "    bbox = ax.get_position()\n",
    "    # Set Axes location in figure to fit fixed_width  fixed_height (in inches)\n",
    "    figw, figh = fig.get_size_inches()\n",
    "    # full figure spans (0,0)-(1,1); left, bottom, width, height\n",
    "    width_fig_frac = fixed_width / figw\n",
    "    height_fig_frac = fixed_height / figh\n",
    "    ax.set_position([bbox.x0, bbox.y0, width_fig_frac, height_fig_frac])\n",
    "    im = ax.imshow(data, vmin=vmin, vmax=vmax, cmap=cmap, aspect='auto', **imshow_kwargs)\n",
    "    return im\n",
    "\n",
    "# ---- Plot: Spearman correlation matrix ----\n",
    "fig0 = plt.figure(figsize=(3,3)) # Large enough so Axes can be fixed, but the image will remain fixed size\n",
    "ax0 = fig0.add_subplot(111)\n",
    "fixed_im_width = (0.2*len(TARGET_LAYER_TYPES_LABELS)+0.2) # inches\n",
    "fixed_im_height = 2.2 # inches, taller for more y items\n",
    "\n",
    "im0 = plot_fixed_size_imshow(\n",
    "    ax0,\n",
    "    spearman_corrs_lh,\n",
    "    vmin=-0.5, vmax=0.5,\n",
    "    cmap='coolwarm',\n",
    "    fixed_width=fixed_im_width,\n",
    "    fixed_height=fixed_im_height\n",
    ")\n",
    "#ax0.set_xlabel('Cortical Gradients', fontsize=small_font)\n",
    "ax0.set_xticks(np.arange(len(TARGET_LAYER_TYPES_LABELS)))\n",
    "ax0.set_xticklabels(TARGET_LAYER_TYPES_LABELS, rotation=90, fontsize=smaller_font)\n",
    "ax0.set_yticks(np.arange(len(TARGET_GENES_LABELS + TARGET_FREQ_LABELS + TARGET_RS_LABELS + TARGET_APERIOD_EXPONENTS)))\n",
    "ax0.set_yticklabels(TARGET_GENES_LABELS + TARGET_FREQ_LABELS + TARGET_RS_LABELS + TARGET_APERIOD_EXPONENTS, fontsize=smaller_font)\n",
    "divider0 = make_axes_locatable(ax0)\n",
    "cax0 = divider0.append_axes(\"right\", size=0.1, pad=0.08)\n",
    "cbar0 = plt.colorbar(im0, cax=cax0, label='Spearman Correlation')\n",
    "cbar0.ax.set_ylabel('Spearman Correlation', rotation=270, labelpad=15, fontsize=small_font)\n",
    "cbar0.ax.tick_params(labelsize=smaller_font)\n",
    "plt.show()\n",
    "\n",
    "# ---- Plot: p-values (FDR) as bins ----\n",
    "# Bins and corresponding colorbar labels (monochrome for significance)\n",
    "p_bins = [0, 0.00001, 0.001, 0.01, 1.0001]  # set upper limit as slightly above 1\n",
    "p_bin_labels = ['<0.00001', '<0.001', '<0.01', 'n.s.']\n",
    "# Shades of gray: black for most significant, light gray for n.s.\n",
    "p_colors = ['#222222', '#555555', '#bbbbbb', '#eeeeee']\n",
    "binned_ps = np.digitize(spearman_ps_fdr_lh, p_bins) - 1\n",
    "cmap = mcolors.ListedColormap(p_colors)\n",
    "bounds = np.arange(len(p_bin_labels)+1)-0.5\n",
    "norm = mcolors.BoundaryNorm(np.arange(len(p_bin_labels)+1), cmap.N)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "im = plot_fixed_size_imshow(\n",
    "    ax,\n",
    "    binned_ps,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    fixed_width=fixed_im_width,\n",
    "    fixed_height=fixed_im_height\n",
    ")\n",
    "#ax.set_xlabel('Cortical Gradients', fontsize=small_font)\n",
    "ax.set_xticks(np.arange(len(TARGET_LAYER_TYPES_LABELS)))\n",
    "ax.set_xticklabels(TARGET_LAYER_TYPES_LABELS, rotation=90, fontsize=smaller_font)\n",
    "ax.set_yticks(np.arange(len(TARGET_GENES_LABELS + TARGET_FREQ_LABELS + TARGET_RS_LABELS + TARGET_APERIOD_EXPONENTS )))\n",
    "ax.set_yticklabels(TARGET_GENES_LABELS + TARGET_FREQ_LABELS + TARGET_RS_LABELS + TARGET_APERIOD_EXPONENTS, fontsize=smaller_font)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=0.1, pad=0.08)\n",
    "tick_locs = (np.array(p_bins[:-1]) + np.array(p_bins[1:])) / 2\n",
    "cbar = plt.colorbar(im, cax=cax, boundaries=bounds, values=np.arange(len(p_bin_labels)))\n",
    "cbar.set_ticks(np.arange(len(p_bin_labels)))\n",
    "cbar.set_ticklabels(p_bin_labels)\n",
    "cbar.ax.set_yticks(np.arange(len(p_bin_labels)))\n",
    "cbar.ax.set_yticklabels(p_bin_labels)\n",
    "cbar.ax.set_ylabel('p-value', rotation=270, labelpad=15, fontsize=small_font)\n",
    "cbar.ax.tick_params(labelsize=smaller_font)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "sort_order = np.argsort(spearman_corrs_lh[:, 0])\n",
    "sorted_spearman_corrs_lh = spearman_corrs_lh[sort_order]\n",
    "im0 = plot_fixed_size_imshow(\n",
    "    ax,\n",
    "    sorted_spearman_corrs_lh,\n",
    "    vmin=-0.5, vmax=0.5,\n",
    "    cmap='coolwarm',\n",
    "    fixed_width=fixed_im_width,\n",
    "    fixed_height=fixed_im_height\n",
    ")\n",
    "ax.set_xticks(np.arange(len(TARGET_LAYER_TYPES_LABELS)))\n",
    "ax.set_xticklabels(TARGET_LAYER_TYPES_LABELS, rotation=90, fontsize=smaller_font)\n",
    "ax.set_yticks(np.arange(len(TARGET_GENES_LABELS + TARGET_FREQ_LABELS + TARGET_RS_LABELS + TARGET_APERIOD_EXPONENTS)))\n",
    "ytick_labels = TARGET_GENES_LABELS + TARGET_FREQ_LABELS + TARGET_RS_LABELS + TARGET_APERIOD_EXPONENTS\n",
    "ytick_labels = np.array(ytick_labels)[sort_order]\n",
    "ax.set_yticklabels(ytick_labels, fontsize=smaller_font)\n",
    "divider0 = make_axes_locatable(ax)\n",
    "cax0 = divider0.append_axes(\"right\", size=0.1, pad=0.08)\n",
    "cbar0 = plt.colorbar(im0, cax=cax0, label='Spearman Correlation')\n",
    "cbar0.ax.set_ylabel('Spearman Correlation', rotation=270, labelpad=15, fontsize=small_font)\n",
    "cbar0.ax.tick_params(labelsize=smaller_font)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "p_bins = [0, 0.0001, 0.001, 0.01, 1.0001]  # set upper limit as slightly above 1\n",
    "p_bin_labels = ['<0.0001', '<0.001', '<0.01', 'n.s.']\n",
    "# Shades of gray: black for most significant, light gray for n.s.\n",
    "p_colors = ['#222222', '#555555', '#bbbbbb', '#eeeeee']\n",
    "binned_ps = np.digitize(spearman_ps_fdr_lh, p_bins) - 1\n",
    "cmap = mcolors.ListedColormap(p_colors)\n",
    "bounds = np.arange(len(p_bin_labels)+1)-0.5\n",
    "norm = mcolors.BoundaryNorm(np.arange(len(p_bin_labels)+1), cmap.N)\n",
    "binned_ps = binned_ps[sort_order]\n",
    "\n",
    "im1 = plot_fixed_size_imshow(\n",
    "    ax0,\n",
    "    binned_ps,\n",
    "    cmap=cmap,\n",
    "    fixed_width=fixed_im_width,\n",
    "    fixed_height=fixed_im_height\n",
    ")\n",
    "ax0.set_xticks(np.arange(len(TARGET_LAYER_TYPES_LABELS)))\n",
    "ax0.set_xticklabels(TARGET_LAYER_TYPES_LABELS, rotation=90, fontsize=smaller_font)\n",
    "ax0.set_yticks(np.arange(len(TARGET_GENES_LABELS + TARGET_FREQ_LABELS + TARGET_RS_LABELS + TARGET_APERIOD_EXPONENTS)))\n",
    "ax0.set_yticklabels(ytick_labels, fontsize=smaller_font)\n",
    "divider0 = make_axes_locatable(ax0)\n",
    "cax0 = divider0.append_axes(\"right\", size=0.1, pad=0.08)\n",
    "cbar0 = plt.colorbar(im1, cax=cax0, label='p-value')\n",
    "cbar0.ax.tick_params(labelsize=smaller_font)\n",
    "cbar0.ax.set_yticks(np.arange(len(p_bin_labels)))\n",
    "cbar0.ax.set_yticklabels(p_bin_labels)\n",
    "cbar0.ax.set_ylabel('p-value', rotation=270, labelpad=15, fontsize=small_font)\n",
    "cbar0.ax.tick_params(labelsize=smaller_font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Organized analysis:\n",
    "1. Prepare gene data matrix for defined genes & combinations.\n",
    "2. Prepare DFA feature matrix for all frequency ranges.\n",
    "3. Prepare cortical gradient matrices for all layer types.\n",
    "4. Concatenate gene and DFA features.\n",
    "5. Compute Spearman correlations with gradients.\n",
    "6. Plot and display.\n",
    "\"\"\"\n",
    "\n",
    "# ==== Imports and Config ====\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "ALPHA = 0.0001\n",
    "\n",
    "# ---- CONFIGURATION ----\n",
    "#GENES\n",
    "TARGET_GENES = ['PVALB', 'SST', 'VIP','LAMP5','SNCG']\n",
    "#TARGET_GENES = sorted((list(expression['9861'].keys())))\n",
    "\n",
    "TARGET_GENES_LABELS = [_label_from_gene(g) for g in TARGET_GENES]\n",
    "#DFA EXPONENTS\n",
    "TARGET_FREQ_RANGES = [[0.1, 4.0], [4.0, 8.0], [8.0, 15.0], [15.0, 30.0], [30.0, 60.0]]\n",
    "TARGET_FREQ_LABELS = ['DFA(Delta)', 'DFA(Theta)', 'DFA(Alpha)', 'DFA(Beta)', 'DFA(Gamma)']\n",
    "#APERIOD EXPONENTS\n",
    "TARGET_APERIOD_EXPONENTS = ['AE(brain)']\n",
    "#GRADIENTS\n",
    "if run_hcp_data:\n",
    "    #HCP has total only\n",
    "    TARGET_LAYER_TYPES = ['total']#     , 'supra', 'infra', 'ratio_supra', 'ratio_infra']\n",
    "    TARGET_LAYER_TYPES_LABELS = ['Total']#, 'Supra', 'Infra', 'Supra/Total', 'Infra/Total']\n",
    "else:\n",
    "    TARGET_LAYER_TYPES = ['total','supra', 'infra', 'ratio_supra', 'ratio_infra']\n",
    "    TARGET_LAYER_TYPES_LABELS = ['Total','Supra', 'Infra', 'Supra/Total', 'Infra/Total']\n",
    "\n",
    "PC_INDEX = 0   # Principal component for gradients\n",
    "hemi = 'lh'    # Hemisphere selection; assumes 200 parcels per hemisphere\n",
    "\n",
    "\n",
    "\n",
    "gene_data_matrix = []\n",
    "gene_data_labels = []\n",
    "for gene in TARGET_GENES:\n",
    "    vals, label = get_parcel_gene_data(gene, hemi)\n",
    "    gene_data_matrix.append(vals)\n",
    "    gene_data_labels.append(label)\n",
    "gene_data_matrix = np.stack(gene_data_matrix, axis=1)  # shape: (n_parcels, n_genes)\n",
    "\n",
    "# ==== 2. PREPARE DFA FEATURE MATRIX ====\n",
    "\n",
    "\n",
    "dfa_matrix_lh = get_dfa_matrix(TARGET_FREQ_RANGES, \"lh\")\n",
    "dfa_matrix_rh = get_dfa_matrix(TARGET_FREQ_RANGES, \"rh\")\n",
    "rss_matrix_lh = get_rrs_matrix(TARGET_FREQ_RANGES, \"lh\")\n",
    "rss_matrix_rh = get_rrs_matrix(TARGET_FREQ_RANGES, \"rh\")\n",
    "\n",
    "ae_brainic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='brainic')\n",
    "ae_brainic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='brainic')\n",
    "ae_allic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='allic')\n",
    "ae_allic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='allic')\n",
    "\n",
    "ao_brainic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='offset_brainic')\n",
    "ao_brainic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='offset_brainic')\n",
    "ao_allic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='offset_allic')\n",
    "ao_allic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='offset_allic')\n",
    "\n",
    "#gradient_matrix_lh = get_gradient_matrix(TARGET_LAYER_TYPES, PC_INDEX, \"lh\")\n",
    "#gradient_matrix_rh = get_gradient_matrix(TARGET_LAYER_TYPES, PC_INDEX, \"rh\")\n",
    "\n",
    "# ==== 4. CONCATENATE GENE AND DFA FEATURES ====\n",
    "gene_dfa_matrix_lh = np.concatenate([dfa_matrix_lh, rss_matrix_lh, ae_brainic_matrix_lh], axis=1)\n",
    "gene_dfa_matrix_rh = np.concatenate([dfa_matrix_rh, rss_matrix_rh, ae_brainic_matrix_rh], axis=1)\n",
    "\n",
    "\n",
    "# ==== 5. COMPUTE SPEARMAN'S CORRELATION ====\n",
    "# Compute Spearman's correlation and p-values\n",
    "spearman_corrs_lh, spearman_ps_lh = compute_spearman_matrix(gene_dfa_matrix_lh, gene_data_matrix)\n",
    "spearman_corrs_rh, spearman_ps_rh = compute_spearman_matrix(gene_dfa_matrix_rh, gene_data_matrix)\n",
    "\n",
    "# Apply multiple comparisons correction (FDR) across all p-values (lh)\n",
    "pvals_lh_flat = spearman_ps_lh.flatten()\n",
    "rej_lh, pvals_fdr_lh, _, _ = multipletests(pvals_lh_flat, alpha=ALPHA, method='fdr_bh')\n",
    "spearman_ps_fdr_lh = pvals_fdr_lh.reshape(spearman_ps_lh.shape)\n",
    "# NOTE: In statsmodels' multipletests, \"rej_lh\" is True (1) for *significant* (not rejected) null hypothesis, i.e., significant correlations.\n",
    "# So, spearman_signif_lh == 1 means significant (null rejected), 0 means not significant (null not rejected).\n",
    "spearman_signif_lh = rej_lh.reshape(spearman_ps_lh.shape)\n",
    "\n",
    "# Same for rh\n",
    "pvals_rh_flat = spearman_ps_rh.flatten()\n",
    "rej_rh, pvals_fdr_rh, _, _ = multipletests(pvals_rh_flat, alpha=ALPHA, method='fdr_bh')\n",
    "spearman_ps_fdr_rh = pvals_fdr_rh.reshape(spearman_ps_rh.shape)\n",
    "spearman_signif_rh = rej_rh.reshape(spearman_ps_rh.shape)\n",
    "\n",
    "print(\"LH Spearman correlation matrix shape:\", spearman_corrs_lh.shape)\n",
    "print(\"RH Spearman correlation matrix shape:\", spearman_corrs_rh.shape)\n",
    "\n",
    "# ==== 6. PLOTTING ====\n",
    "\n",
    "# ---- Plotting helpers ----\n",
    "small_font = 12\n",
    "smaller_font = 10\n",
    "YLABEL_NAME = TARGET_FREQ_LABELS + TARGET_RS_LABELS + TARGET_APERIOD_EXPONENTS\n",
    "# ---- Plot: Spearman correlation matrix ----\n",
    "fig0 = plt.figure(figsize=(3,3)) # Large enough so Axes can be fixed, but the image will remain fixed size\n",
    "ax0 = fig0.add_subplot(111)\n",
    "fixed_im_width = (0.2*len(TARGET_LAYER_TYPES_LABELS)+0.2) # inches\n",
    "fixed_im_height = (0.2*len(YLABEL_NAME)+0.2)\n",
    "\n",
    "im0 = plot_fixed_size_imshow(\n",
    "    ax0,\n",
    "    spearman_corrs_lh,\n",
    "    vmin=-0.5, vmax=0.5,\n",
    "    cmap='coolwarm',\n",
    "    fixed_width=fixed_im_width,\n",
    "    fixed_height=fixed_im_height\n",
    ")\n",
    "#ax0.set_xlabel('Cortical Gradients', fontsize=small_font)\n",
    "ax0.set_xticks(np.arange(len(TARGET_GENES_LABELS)))\n",
    "ax0.set_xticklabels(TARGET_GENES_LABELS, rotation=90, fontsize=smaller_font)\n",
    "ax0.set_yticks(np.arange(len(YLABEL_NAME)))\n",
    "ax0.set_yticklabels(YLABEL_NAME, fontsize=smaller_font)\n",
    "divider0 = make_axes_locatable(ax0)\n",
    "cax0 = divider0.append_axes(\"right\", size=0.1, pad=0.08)\n",
    "cbar0 = plt.colorbar(im0, cax=cax0, label='Spearman Correlation')\n",
    "cbar0.ax.set_ylabel('Spearman Correlation', rotation=270, labelpad=15, fontsize=small_font)\n",
    "cbar0.ax.tick_params(labelsize=smaller_font)\n",
    "plt.show()\n",
    "\n",
    "# ---- Plot: p-values (FDR) as bins ----\n",
    "# Bins and corresponding colorbar labels (monochrome for significance)\n",
    "p_bins = [0, 0.0001, 0.001, 0.01, 1.0001]  # set upper limit as slightly above 1\n",
    "p_bin_labels = ['<0.0001', '<0.001', '<0.01', 'n.s.']\n",
    "# Shades of gray: black for most significant, light gray for n.s.\n",
    "p_colors = ['#222222', '#555555', '#bbbbbb', '#eeeeee']\n",
    "binned_ps = np.digitize(spearman_ps_fdr_lh, p_bins) - 1\n",
    "cmap = mcolors.ListedColormap(p_colors)\n",
    "bounds = np.arange(len(p_bin_labels)+1)-0.5\n",
    "norm = mcolors.BoundaryNorm(np.arange(len(p_bin_labels)+1), cmap.N)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "im = plot_fixed_size_imshow(\n",
    "    ax,\n",
    "    binned_ps,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    fixed_width=fixed_im_width,\n",
    "    fixed_height=fixed_im_height\n",
    ")\n",
    "#ax.set_xlabel('Cortical Gradients', fontsize=small_font)\n",
    "ax.set_xticks(np.arange(len(TARGET_GENES_LABELS)))\n",
    "ax.set_xticklabels(TARGET_GENES_LABELS, rotation=90, fontsize=smaller_font)\n",
    "ax.set_yticks(np.arange(len(YLABEL_NAME)))\n",
    "ax.set_yticklabels(YLABEL_NAME, fontsize=smaller_font)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=0.1, pad=0.08)\n",
    "tick_locs = (np.array(p_bins[:-1]) + np.array(p_bins[1:])) / 2\n",
    "cbar = plt.colorbar(im, cax=cax, boundaries=bounds, values=np.arange(len(p_bin_labels)))\n",
    "cbar.set_ticks(np.arange(len(p_bin_labels)))\n",
    "cbar.set_ticklabels(p_bin_labels)\n",
    "cbar.ax.set_yticks(np.arange(len(p_bin_labels)))\n",
    "cbar.ax.set_yticklabels(p_bin_labels)\n",
    "cbar.ax.set_ylabel('p-value', rotation=270, labelpad=15, fontsize=small_font)\n",
    "cbar.ax.tick_params(labelsize=smaller_font)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Organized analysis:\n",
    "1. Prepare gene data matrix for defined genes & combinations.\n",
    "2. Prepare DFA feature matrix for all frequency ranges.\n",
    "3. Prepare cortical gradient matrices for all layer types.\n",
    "4. Concatenate gene and DFA features.\n",
    "5. Compute Spearman correlations with gradients.\n",
    "6. Plot and display.\n",
    "\"\"\"\n",
    "\n",
    "# ==== Imports and Config ====\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "ALPHA = 0.0001\n",
    "\n",
    "# ---- CONFIGURATION ----\n",
    "#GENES\n",
    "TARGET_GENES = ['PVALB', 'SST', 'VIP','LAMP5','SNCG']\n",
    "#TARGET_GENES = sorted((list(expression['9861'].keys())))\n",
    "\n",
    "TARGET_GENES_LABELS = [_label_from_gene(g) for g in TARGET_GENES]\n",
    "#DFA EXPONENTS\n",
    "TARGET_FREQ_RANGES = [[0.1, 4.0], [4.0, 8.0], [8.0, 15.0], [15.0, 30.0], [30.0, 60.0]]\n",
    "TARGET_FREQ_LABELS = ['DFA(Delta)', 'DFA(Theta)', 'DFA(Alpha)', 'DFA(Beta)', 'DFA(Gamma)']\n",
    "#APERIOD EXPONENTS\n",
    "TARGET_APERIOD_EXPONENTS = ['AE(brain)','AE(all)','AO(brain)','AO(all)']\n",
    "#GRADIENTS\n",
    "if run_hcp_data:\n",
    "    #HCP has total only\n",
    "    TARGET_LAYER_TYPES = ['total']#     , 'supra', 'infra', 'ratio_supra', 'ratio_infra']\n",
    "    TARGET_LAYER_TYPES_LABELS = ['Total']#, 'Supra', 'Infra', 'Supra/Total', 'Infra/Total']\n",
    "else:\n",
    "    TARGET_LAYER_TYPES = ['total','supra', 'infra', 'ratio_supra', 'ratio_infra']\n",
    "    TARGET_LAYER_TYPES_LABELS = ['Total','Supra', 'Infra', 'Supra/Total', 'Infra/Total']\n",
    "\n",
    "PC_INDEX = 0   # Principal component for gradients\n",
    "hemi = 'lh'    # Hemisphere selection; assumes 200 parcels per hemisphere\n",
    "\n",
    "\n",
    "\n",
    "gene_data_matrix = []\n",
    "gene_data_labels = []\n",
    "for gene in TARGET_GENES:\n",
    "    vals, label = get_parcel_gene_data(gene, hemi)\n",
    "    gene_data_matrix.append(vals)\n",
    "    gene_data_labels.append(label)\n",
    "gene_data_matrix = np.stack(gene_data_matrix, axis=1)  # shape: (n_parcels, n_genes)\n",
    "\n",
    "# ==== 2. PREPARE DFA FEATURE MATRIX ====\n",
    "\n",
    "\n",
    "dfa_matrix_lh = get_dfa_matrix(TARGET_FREQ_RANGES, \"lh\")\n",
    "dfa_matrix_rh = get_dfa_matrix(TARGET_FREQ_RANGES, \"rh\")\n",
    "rrs_matrix_lh = get_rrs_matrix(TARGET_FREQ_RANGES, \"lh\")\n",
    "rrs_matrix_rh = get_rrs_matrix(TARGET_FREQ_RANGES, \"rh\")\n",
    "\n",
    "ae_brainic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='brainic')\n",
    "ae_brainic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='brainic')\n",
    "ae_allic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='allic')\n",
    "ae_allic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='allic')\n",
    "\n",
    "ao_brainic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='offset_brainic')\n",
    "ao_brainic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='offset_brainic')\n",
    "ao_allic_matrix_lh = get_aperiodic_exponent_matrix(\"lh\",data_type='offset_allic')\n",
    "ao_allic_matrix_rh = get_aperiodic_exponent_matrix(\"rh\",data_type='offset_allic')\n",
    "\n",
    "#gradient_matrix_lh = get_gradient_matrix(TARGET_LAYER_TYPES, PC_INDEX, \"lh\")\n",
    "#gradient_matrix_rh = get_gradient_matrix(TARGET_LAYER_TYPES, PC_INDEX, \"rh\")\n",
    "\n",
    "# ==== 4. CONCATENATE GENE AND DFA FEATURES ====\n",
    "\n",
    "exp_matrix_lh = np.concatenate([ae_brainic_matrix_lh, ae_allic_matrix_lh, ao_brainic_matrix_lh, ao_allic_matrix_lh], axis=1)\n",
    "exp_matrix_rh = np.concatenate([ae_brainic_matrix_rh, ae_allic_matrix_rh, ao_brainic_matrix_rh, ao_allic_matrix_rh], axis=1)\n",
    "\n",
    "# ==== 5. COMPUTE SPEARMAN'S CORRELATION ====\n",
    "# Compute Spearman's correlation and p-values\n",
    "spearman_corrs_lh, spearman_ps_lh = compute_spearman_matrix(np.concatenate([dfa_matrix_lh, rrs_matrix_lh], axis=1), exp_matrix_lh)\n",
    "spearman_corrs_rh, spearman_ps_rh = compute_spearman_matrix(np.concatenate([dfa_matrix_rh, rrs_matrix_rh], axis=1), exp_matrix_rh)\n",
    "\n",
    "# Apply multiple comparisons correction (FDR) across all p-values (lh)\n",
    "pvals_lh_flat = spearman_ps_lh.flatten()\n",
    "rej_lh, pvals_fdr_lh, _, _ = multipletests(pvals_lh_flat, alpha=ALPHA, method='fdr_bh')\n",
    "spearman_ps_fdr_lh = pvals_fdr_lh.reshape(spearman_ps_lh.shape)\n",
    "# NOTE: In statsmodels' multipletests, \"rej_lh\" is True (1) for *significant* (not rejected) null hypothesis, i.e., significant correlations.\n",
    "# So, spearman_signif_lh == 1 means significant (null rejected), 0 means not significant (null not rejected).\n",
    "spearman_signif_lh = rej_lh.reshape(spearman_ps_lh.shape)\n",
    "\n",
    "# Same for rh\n",
    "pvals_rh_flat = spearman_ps_rh.flatten()\n",
    "rej_rh, pvals_fdr_rh, _, _ = multipletests(pvals_rh_flat, alpha=ALPHA, method='fdr_bh')\n",
    "spearman_ps_fdr_rh = pvals_fdr_rh.reshape(spearman_ps_rh.shape)\n",
    "spearman_signif_rh = rej_rh.reshape(spearman_ps_rh.shape)\n",
    "\n",
    "print(\"LH Spearman correlation matrix shape:\", spearman_corrs_lh.shape)\n",
    "print(\"RH Spearman correlation matrix shape:\", spearman_corrs_rh.shape)\n",
    "\n",
    "# ==== 6. PLOTTING ====\n",
    "\n",
    "# ---- Plotting helpers ----\n",
    "small_font = 12\n",
    "smaller_font = 10\n",
    "\n",
    "YLABEL_NAME = TARGET_FREQ_LABELS + TARGET_RS_LABELS\n",
    "# ---- Plot: Spearman correlation matrix ----\n",
    "fig0 = plt.figure(figsize=(3,3)) # Large enough so Axes can be fixed, but the image will remain fixed size\n",
    "ax0 = fig0.add_subplot(111)\n",
    "fixed_im_width = (0.2*len(TARGET_LAYER_TYPES_LABELS)+0.2) # inches\n",
    "fixed_im_height = (0.2*len(YLABEL_NAME)+0.2)\n",
    "\n",
    "im0 = plot_fixed_size_imshow(\n",
    "    ax0,\n",
    "    spearman_corrs_lh,\n",
    "    vmin=-0.5, vmax=0.5,\n",
    "    cmap='coolwarm',\n",
    "    fixed_width=fixed_im_width,\n",
    "    fixed_height=fixed_im_height\n",
    ")\n",
    "#ax0.set_xlabel('Cortical Gradients', fontsize=small_font)\n",
    "ax0.set_xticks(np.arange(len(TARGET_APERIOD_EXPONENTS)))\n",
    "ax0.set_xticklabels(TARGET_APERIOD_EXPONENTS, rotation=90, fontsize=smaller_font)\n",
    "ax0.set_yticks(np.arange(len(YLABEL_NAME)))\n",
    "ax0.set_yticklabels(YLABEL_NAME, fontsize=smaller_font)\n",
    "divider0 = make_axes_locatable(ax0)\n",
    "cax0 = divider0.append_axes(\"right\", size=0.1, pad=0.08)\n",
    "cbar0 = plt.colorbar(im0, cax=cax0, label='Spearman Correlation')\n",
    "cbar0.ax.set_ylabel('Spearman Correlation', rotation=270, labelpad=15, fontsize=small_font)\n",
    "cbar0.ax.tick_params(labelsize=smaller_font)\n",
    "plt.show()\n",
    "\n",
    "# ---- Plot: p-values (FDR) as bins ----\n",
    "# Bins and corresponding colorbar labels (monochrome for significance)\n",
    "p_bins = [0, 0.0001, 0.001, 0.01, 1.0001]  # set upper limit as slightly above 1\n",
    "p_bin_labels = ['<0.0001', '<0.001', '<0.01', 'n.s.']\n",
    "# Shades of gray: black for most significant, light gray for n.s.\n",
    "p_colors = ['#222222', '#555555', '#bbbbbb', '#eeeeee']\n",
    "binned_ps = np.digitize(spearman_ps_fdr_lh, p_bins) - 1\n",
    "cmap = mcolors.ListedColormap(p_colors)\n",
    "bounds = np.arange(len(p_bin_labels)+1)-0.5\n",
    "norm = mcolors.BoundaryNorm(np.arange(len(p_bin_labels)+1), cmap.N)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "im = plot_fixed_size_imshow(\n",
    "    ax,\n",
    "    binned_ps,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    fixed_width=fixed_im_width,\n",
    "    fixed_height=fixed_im_height\n",
    ")\n",
    "#ax.set_xlabel('Cortical Gradients', fontsize=small_font)\n",
    "ax.set_xticks(np.arange(len(TARGET_APERIOD_EXPONENTS)))\n",
    "ax.set_xticklabels(TARGET_APERIOD_EXPONENTS, rotation=90, fontsize=smaller_font)\n",
    "ax.set_yticks(np.arange(len(YLABEL_NAME)))\n",
    "ax.set_yticklabels(YLABEL_NAME, fontsize=smaller_font)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=0.1, pad=0.08)\n",
    "tick_locs = (np.array(p_bins[:-1]) + np.array(p_bins[1:])) / 2\n",
    "cbar = plt.colorbar(im, cax=cax, boundaries=bounds, values=np.arange(len(p_bin_labels)))\n",
    "cbar.set_ticks(np.arange(len(p_bin_labels)))\n",
    "cbar.set_ticklabels(p_bin_labels)\n",
    "cbar.ax.set_yticks(np.arange(len(p_bin_labels)))\n",
    "cbar.ax.set_yticklabels(p_bin_labels)\n",
    "cbar.ax.set_ylabel('p-value', rotation=270, labelpad=15, fontsize=small_font)\n",
    "cbar.ax.tick_params(labelsize=smaller_font)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_exponent_matrix(freq_ranges, freq_labels, xlabel_name, hemi):\n",
    "    import numpy as np, matplotlib.pyplot as plt, matplotlib.cm as cm, pickle\n",
    "\n",
    "    # Vectorized data loading and selection\n",
    "    slices = slice(0, int(num_parcels/2)) if hemi == 'lh' else slice(int(num_parcels/2), None)\n",
    "   # files = [f'/Users/dennis.jungchildmind.org/Desktop/MEG/112225_dfa_outputs/dfa_restin_f{fr[0]}_to_{fr[1]}_parcelonly.pkl' for fr in freq_ranges]\n",
    "    files = [f'/Users/dennis.jungchildmind.org/Desktop/MEG/112625_rrs_outputs/rrs_restin_f{fr[0]}_to_{fr[1]}_parcelonly.pkl' for fr in freq_ranges]\n",
    "    vals = [pickle.load(open(f, 'rb'))[slices, :].ravel() for f in files]\n",
    "    \n",
    "    bins = np.histogram_bin_edges(np.concatenate(vals), bins=100)\n",
    "    n_freq = len(freq_labels)\n",
    "    color_palette = [cm.get_cmap('jet', n_freq)(i) for i in range(n_freq)]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    [plt.hist(v, bins=bins, alpha=0.7, edgecolor='white', color=color_palette[i], label=freq_labels[i]) for i, v in enumerate(vals)]\n",
    "    plt.xlabel(xlabel_name, fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.xticks(fontsize=13)  # Make xticks smaller\n",
    "    plt.yticks(fontsize=13)  # Make yticks smaller\n",
    "    plt.legend(fontsize=12, frameon=False, loc='upper right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "#dfa\n",
    "#hist_exponent_matrix(TARGET_FREQ_RANGES, TARGET_FREQ_LABELS, xlabel_name=\"DFA exponent\", hemi=\"lh\")\n",
    "#rrs\n",
    "hist_exponent_matrix(TARGET_FREQ_RANGES, TARGET_FREQ_LABELS, xlabel_name=\"RS\", hemi=\"lh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "# Path analysis / mediation using Spearman correlation\n",
    "print(\"Testing mediation: Gene  E/I  Gradient (Spearman correlation)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Direct effects using Spearman correlation\n",
    "rho_gene_gradient, _ = spearmanr(X_clean[:, 1], y_clean)  # c path (total effect)\n",
    "rho_gene_EI, _ = spearmanr(X_clean[:, 1], X_clean[:, 0])  # a path\n",
    "rho_EI_gradient, _ = spearmanr(X_clean[:, 0], y_clean)    # b path (bivariate)\n",
    "\n",
    "print(f\"Gene  Gradient (total):    ={rho_gene_gradient:.3f}\")\n",
    "print(f\"Gene  E/I (a path):        ={rho_gene_EI:.3f}\")\n",
    "print(f\"E/I  Gradient (bivariate): ={rho_EI_gradient:.3f}\")\n",
    "\n",
    "# Partial correlation: E/I with gradient, controlling for genes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Residualize E/I and gradient with respect to genes\n",
    "model_EI_gene = LinearRegression().fit(X_clean[:, [1]], X_clean[:, 0])\n",
    "EI_resid = X_clean[:, 0] - model_EI_gene.predict(X_clean[:, [1]])\n",
    "\n",
    "model_grad_gene = LinearRegression().fit(X_clean[:, [1]], y_clean)\n",
    "grad_resid = y_clean - model_grad_gene.predict(X_clean[:, [1]])\n",
    "\n",
    "# Partial correlation using Spearman correlation\n",
    "rho_partial, p_partial = spearmanr(EI_resid, grad_resid)\n",
    "print(f\"\\nE/I  Gradient (controlling for genes, Spearman): ={rho_partial:.3f}, p={p_partial:.4f}\")\n",
    "\n",
    "# Compare standardized betas (still uses linear regression on standardized z-scores)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "y_scaled = scaler.fit_transform(y_clean.reshape(-1, 1)).ravel()\n",
    "\n",
    "model_full = LinearRegression().fit(X_scaled, y_scaled)\n",
    "print(f\"\\nStandardized  coefficients:\")\n",
    "print(f\"  _EI = {model_full.coef_[0]:.3f}\")\n",
    "print(f\"  _gene = {model_full.coef_[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compute Spearman correlations\n",
    "rho_gene_EI, p_gene_EI = spearmanr(X_clean[:, 1], X_clean[:, 0])\n",
    "rho_EI_gradient, p_EI_gradient = spearmanr(X_clean[:, 0], y_clean)\n",
    "rho_gene_gradient, p_gene_gradient = spearmanr(X_clean[:, 1], y_clean)\n",
    "rho_partial, p_partial_spearman = spearmanr(EI_resid, grad_resid)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Increase font sizes globally for all ticks\n",
    "plt.rcParams.update({'xtick.labelsize': 16, 'ytick.labelsize': 16, 'axes.labelsize': 20, 'axes.titlesize': 22})\n",
    "\n",
    "# 1. Gene vs E/I (the key relationship)\n",
    "axes[0].scatter(X_clean[:, 1], X_clean[:, 0], alpha=0.5, s=50, c='purple')\n",
    "z = np.polyfit(X_clean[:, 1], X_clean[:, 0], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(X_clean[:, 1].min(), X_clean[:, 1].max(), 100)\n",
    "axes[0].plot(x_line, p(x_line), 'r-', lw=2)\n",
    "axes[0].set_xlabel('Gene Expression (SST-PVALB)', fontsize=20)\n",
    "axes[0].set_ylabel('Aperiodic Exponent', fontsize=20)\n",
    "axes[0].set_title(f'Gene   E/I\\nSpearman ={rho_gene_EI:.3f}', fontsize=22)\n",
    "# Remove grid\n",
    "axes[0].grid(False)\n",
    "\n",
    "# 2. The masking effect\n",
    "# Color points by gene expression level\n",
    "scatter = axes[1].scatter(X_clean[:, 0], y_clean, c=X_clean[:, 1], cmap='viridis', alpha=0.6, s=50)\n",
    "axes[1].set_xlabel('Aperiodic Exponent', fontsize=20)\n",
    "axes[1].set_ylabel('Gradient Score', fontsize=20)\n",
    "axes[1].set_title(f'E/I  Gradient (colored by gene)\\nSpearman ={rho_EI_gradient:.3f}', fontsize=22)\n",
    "# Colorbar with increased fontsize\n",
    "cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "cbar.set_label('Gene Expression', fontsize=18, rotation=-90, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "# Remove grid\n",
    "axes[1].grid(False)\n",
    "\n",
    "# 3. After controlling for genes (residuals)\n",
    "axes[2].scatter(EI_resid, grad_resid, alpha=0.5, s=50, c='orange')\n",
    "z = np.polyfit(EI_resid, grad_resid, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(EI_resid.min(), EI_resid.max(), 100)\n",
    "axes[2].plot(x_line, p(x_line), 'r--', lw=2)\n",
    "axes[2].set_xlabel('Aperiodic Exponent (residual)', fontsize=20)\n",
    "axes[2].set_ylabel('Gradient (residual)', fontsize=20)\n",
    "axes[2].set_title(f'E/I  Gradient (controlling gene)\\nSpearman ={rho_partial:.3f}, p={p_partial_spearman:.3f}', fontsize=22)\n",
    "# Remove grid\n",
    "axes[2].grid(False)\n",
    "axes[2].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. Gene and E/I are negatively correlated (Spearman ={rho_gene_EI:.3f})\")\n",
    "print(f\"    Higher inhibitory genes = Lower 1/f exponent\")\n",
    "print(f\"\\n2. Gene predicts gradient strongly (Spearman ={rho_gene_gradient:.3f})\")\n",
    "print(f\"    SST-PVALB expression organizes structure\")\n",
    "print(f\"\\n3. E/I does NOT predict gradient independently\")\n",
    "print(f\"    Raw: Spearman ={rho_EI_gradient:.3f}\")\n",
    "print(f\"    Controlling for genes: Spearman ={rho_partial:.3f} (p={p_partial_spearman:.3f})\")\n",
    "print(f\"\\n4. All variance is explained by GENES, not E/I balance\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Your data prep\n",
    "the_gene_data = np.nanmean(get_gene_expression(expression, \"SST\")-get_gene_expression(expression, \"PVALB\"), axis=-1)\n",
    "exp_dat_lh_parc = parcellate_data(exp_data_lh, atlas_data_lh)\n",
    "the_gene_data_lh = the_gene_data[:int(num_parcels/2)]\n",
    "\n",
    "X = np.column_stack([exp_dat_lh_parc, the_gene_data_lh])\n",
    "y = lh_data_all_unmapped['supra']\n",
    "\n",
    "# Remove NaNs\n",
    "index = ~np.isnan(X).any(axis=1) & ~np.isnan(y)\n",
    "X_clean = X[index]\n",
    "y_clean = y[index]\n",
    "\n",
    "print(f\"N regions: {len(y_clean)}\")\n",
    "\n",
    "# === STEP 1: Basic correlations (diagnostic) ===\n",
    "r_EI, p_EI = pearsonr(X_clean[:, 0], y_clean)\n",
    "r_gene, p_gene = pearsonr(X_clean[:, 1], y_clean)\n",
    "\n",
    "print(f\"\\nUnivariate correlations (NOT corrected for spatial autocorrelation):\")\n",
    "print(f\"  E/I:  r={r_EI:.3f}, p={p_EI:.4f}\")\n",
    "print(f\"  Gene: r={r_gene:.3f}, p={p_gene:.4f}\")\n",
    "\n",
    "# === STEP 2: Multiple regression ===\n",
    "model = LinearRegression()\n",
    "model.fit(X_clean, y_clean)\n",
    "r_squared = model.score(X_clean, y_clean)\n",
    "beta_EI, beta_gene = model.coef_\n",
    "\n",
    "print(f\"\\nMultiple Regression:\")\n",
    "print(f\"  R = {r_squared:.3f}\")\n",
    "print(f\"  _EI = {beta_EI:.3f}\")\n",
    "print(f\"  _gene = {beta_gene:.3f}\")\n",
    "\n",
    "# === STEP 3: Variance partitioning ===\n",
    "model_EI = LinearRegression().fit(X_clean[:, [0]], y_clean)\n",
    "model_gene = LinearRegression().fit(X_clean[:, [1]], y_clean)\n",
    "\n",
    "R2_EI = model_EI.score(X_clean[:, [0]], y_clean)\n",
    "R2_gene = model_gene.score(X_clean[:, [1]], y_clean)\n",
    "\n",
    "unique_EI = r_squared - R2_gene\n",
    "unique_gene = r_squared - R2_EI\n",
    "shared = R2_EI + R2_gene - r_squared\n",
    "\n",
    "print(f\"\\nVariance Partitioning:\")\n",
    "print(f\"  Total R:     {r_squared:.3f}\")\n",
    "print(f\"  Unique E/I:   {unique_EI:.3f}\")\n",
    "print(f\"  Unique Gene:  {unique_gene:.3f}\")\n",
    "print(f\"  Shared:       {shared:.3f}\")\n",
    "\n",
    "# === STEP 4: Visualization ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# E/I vs gradient\n",
    "axes[0].scatter(X_clean[:, 0], y_clean, alpha=0.5, s=30)\n",
    "axes[0].set_xlabel('E/I Balance (1/f exponent)')\n",
    "axes[0].set_ylabel('Gradient Value')\n",
    "axes[0].set_title(f'E/I vs Gradient\\nr={r_EI:.3f}')\n",
    "\n",
    "# Gene vs gradient\n",
    "axes[1].scatter(X_clean[:, 1], y_clean, alpha=0.5, s=30, color='orange')\n",
    "axes[1].set_xlabel('SST-PVALB Expression')\n",
    "axes[1].set_ylabel('Gradient Value')\n",
    "axes[1].set_title(f'Gene vs Gradient\\nr={r_gene:.3f}')\n",
    "\n",
    "# Predicted vs actual\n",
    "y_pred = model.predict(X_clean)\n",
    "axes[2].scatter(y_clean, y_pred, alpha=0.5, s=30, color='green')\n",
    "axes[2].plot([y_clean.min(), y_clean.max()], [y_clean.min(), y_clean.max()], 'r--', lw=2)\n",
    "axes[2].set_xlabel('Actual Gradient')\n",
    "axes[2].set_ylabel('Predicted Gradient')\n",
    "axes[2].set_title(f'Full Model\\nR={r_squared:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Variance partitioning pie chart\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sizes = [unique_EI, unique_gene, shared, 1-r_squared]\n",
    "labels = [f'Unique E/I\\n({unique_EI:.2f})', \n",
    "          f'Unique Gene\\n({unique_gene:.2f})',\n",
    "          f'Shared\\n({shared:.2f})',\n",
    "          f'Unexplained\\n({1-r_squared:.2f})']\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#lightgray']\n",
    "ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title('Variance Partitioning', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import yaspy\n",
    "# Load and process BigBrain layer data\n",
    "big_brain_layer_dir = '/Users/dennis.jungchildmind.org/Downloads/BigBrain/thickness/resample/'\n",
    "#supra_sample = lh_data['supra'][0]\n",
    "#infra_sample = lh_data['infra'][0]\n",
    "#total_sample = supra_sample+infra_sample#lh_data['total'][0]#this total is little different from the supra_sample + infra_sample\n",
    "\n",
    "def load_layers_bb(hemi):\n",
    "    \"\"\"Load each layer thickness data for a given hemisphere\"\"\"\n",
    "    layers = {}\n",
    "    for layer in range(1,7):\n",
    "        start = layer-1\n",
    "        filename = f'{hemi}.{start}-{layer}.32k.shape.gii'\n",
    "        layers[f'L{layer}'] = nib.load(os.path.join(big_brain_layer_dir, filename)).darrays[0].data\n",
    "    return np.stack([layers[f'L{layer}'] for layer in range(1,7)], axis=1)\n",
    "\n",
    "# Load data for both hemispheres\n",
    "bb_layers_lh = load_layers_bb('lh')\n",
    "bb_layers_rh = load_layers_bb('rh')\n",
    "\n",
    "print(f\"BigBrain layer data shape: {bb_layers_lh.shape}\")\n",
    "\n",
    "# Prepare template data (BigBrain data) for regression analysis\n",
    "def prepare_template_data(layer_data, target_layer):\n",
    "    \"\"\"Prepare template data for regression\"\"\"\n",
    "    supra = np.sum(layer_data[:,0:3], axis=1)#as defined in the ex vivo data (L1-3)\n",
    "    infra = np.sum(layer_data[:,3:], axis=1)#as defined in the ex vivo data (L4-6) * \n",
    "    total = np.sum(layer_data, axis=1)\n",
    "    vertex_id = np.arange(layer_data.shape[0])#positional information\n",
    "    #x = np.column_stack([supra,infra, vertex_id])\n",
    "    x = np.column_stack([supra, vertex_id])\n",
    "    y = layer_data[:,target_layer]  # Layer 4 * we want to regress out layer 4 from the infragranular (L4-6 in ex vivo)\n",
    "    return x, y\n",
    "\n",
    "#BigBrain data as template data for the model fitting\n",
    "#perform for each layer and hemisphere\n",
    "r2_all = {'lh': [], 'rh': []}\n",
    "mse_all = {'lh': [], 'rh': []}\n",
    "mlr_models = {'lh': [], 'rh': []}\n",
    "bb_layers = {'lh': bb_layers_lh, 'rh': bb_layers_rh}\n",
    "\n",
    "  \n",
    "for hemi in ['lh', 'rh']:\n",
    "    for layer in range(0,6):\n",
    "        x_template, y_template = prepare_template_data(bb_layers[hemi],[layer])\n",
    "\n",
    "        # Fit multiple linear regression model\n",
    "        mlr_model = LinearRegression(fit_intercept=True,tol=1e-6,copy_X=True)\n",
    "        #mlr_model = RidgeCV(alphas=np.logspace(-3, 3, 100), cv=10)\n",
    "        #mlr_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        mlr_model.fit(x_template, y_template)\n",
    "        y_pred = mlr_model.predict(x_template)\n",
    "        \n",
    "        # Calculate and store metrics\n",
    "        r2_all[hemi].append(r2_score(y_template, y_pred))\n",
    "        mse_all[hemi].append(mean_squared_error(y_template, y_pred))\n",
    "        mlr_models[hemi].append(mlr_model)\n",
    "        print(f'{hemi.upper()} Model performance - R2: {r2_all[hemi][-1]:.3f}, MSE: {mse_all[hemi][-1]:.3f}')\n",
    "#plt.plot(y_template,y_pred,'.')\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "x = np.arange(6)\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, r2_all['lh'], width, label='LH', color='#2ecc71')  # Emerald green\n",
    "plt.bar(x + width/2, r2_all['rh'], width, label='RH', color='#3498db')  # Bright blue\n",
    "plt.xticks(x, ['L1', 'L2', 'L3', 'L4', 'L5', 'L6'])\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('R Score')\n",
    "plt.xlabel('Cortical Layer')\n",
    "plt.title('MLR Performance')\n",
    "plt.legend(loc='upper right', framealpha=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the previous section, let's calculate infra_cleaned with multiple linear regression\n",
    "mlr_model_lh = LinearRegression(fit_intercept=True,tol=1e-6,copy_X=True)\n",
    "#mlr_model_lh = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "x_template_lh, y_template_lh = prepare_template_data(bb_layers_lh,3)\n",
    "mlr_model_lh.fit(x_template_lh, y_template_lh)\n",
    "y_pred_lh = mlr_model_lh.predict(x_template_lh)\n",
    "print('r2_score',r2_score(y_template_lh, y_pred_lh))\n",
    "print('mean_squared_error',mean_squared_error(y_template_lh, y_pred_lh))\n",
    "\n",
    "mlr_model_rh = LinearRegression(fit_intercept=True,tol=1e-6,copy_X=True)\n",
    "#mlr_model_rh = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "x_template_rh, y_template_rh = prepare_template_data(bb_layers_rh,3)\n",
    "mlr_model_rh.fit(x_template_rh, y_template_rh)\n",
    "y_pred_rh = mlr_model_rh.predict(x_template_rh)\n",
    "print('r2_score',r2_score(y_template_rh, y_pred_rh))\n",
    "print('mean_squared_error',mean_squared_error(y_template_rh, y_pred_rh))\n",
    "\n",
    "\n",
    "# Process samples\n",
    "def process_samples(mlr_model,supra_sample, infra_sample):\n",
    "    \"\"\"Process all samples to remove layer 4 contribution\"\"\"\n",
    "    n_vertices, n_samples = supra_sample.shape\n",
    "    predicted_x4 = np.zeros((n_vertices, n_samples))\n",
    "    infra_cleaned = np.zeros((n_vertices, n_samples))\n",
    "    vertex_id = np.arange(n_vertices)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        #X = np.column_stack([supra_sample[:,i], infra_sample[:,i],vertex_id])\n",
    "        X = np.column_stack([supra_sample[:,i],vertex_id])\n",
    "        pred_layers = mlr_model.predict(X)\n",
    "        \n",
    "        predicted_x4[:,i] = pred_layers#np.sum(pred_layers,axis=1)\n",
    "        \n",
    "        infra_cleaned[:,i]= infra_sample[:,i] - predicted_x4[:,i]\n",
    "        \n",
    "        print(f\"Sample {i+1}: Layer 4 range [{predicted_x4.min():.3f}, {predicted_x4.max():.3f}]\")\n",
    "        \n",
    "    return predicted_x4, infra_cleaned\n",
    "\n",
    "\n",
    "supra_sample_lh = lh_data['supra'][0]\n",
    "infra_sample_lh = lh_data['infra'][0]\n",
    "supra_sample_rh = rh_data['supra'][0]\n",
    "infra_sample_rh = rh_data['infra'][0]\n",
    "predicted_x4_lh, infra_cleaned_lh = process_samples(mlr_model_lh,supra_sample_lh, infra_sample_lh)\n",
    "predicted_x4_rh, infra_cleaned_rh = process_samples(mlr_model_rh,supra_sample_rh, infra_sample_rh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_sample_lh = lh_data['total'][0]\n",
    "total_sample_rh = rh_data['total'][0]\n",
    "\n",
    "ratio_cleaned_lh = infra_cleaned_lh / total_sample_lh\n",
    "ratio_cleaned_rh = infra_cleaned_rh / total_sample_rh\n",
    "\n",
    "\n",
    "# Calculate and plot cleaned data\n",
    "#cleaned_data_lh = np.mean(infra_cleaned_lh, axis=1)\n",
    "#cleaned_data_rh = np.mean(infra_cleaned_rh, axis=1)\n",
    "\n",
    "data2plot_lh = np.mean(ratio_cleaned_lh, axis=1) \n",
    "data2plot_rh = np.mean(ratio_cleaned_rh, axis=1)\n",
    "\n",
    "#data2plot_lh = np.mean(infra_cleaned_lh, axis=1)\n",
    "#data2plot_rh = np.mean(infra_cleaned_rh, axis=1)\n",
    "\n",
    "cmap = 'RdBu_r'\n",
    "\n",
    "#data2plot = np.mean(predicted_x4,axis=1)\n",
    "\n",
    "# Visualization\n",
    "surf_path = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.R.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "plotter = yaspy.Plotter(surf_path, hemi='rh')\n",
    "overlay = plotter.overlay(data2plot_rh, cmap=cmap, alpha=1,vmin=0.4,vmax=0.6)\n",
    "plotter.border(data2plot_rh, alpha=0)\n",
    "\n",
    "# Create and display figure\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6,4))\n",
    "ax1.imshow(plotter.screenshot(\"lateral\"))\n",
    "ax1.axis('off')\n",
    "\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes(\"bottom\", size=\"3%\", pad=0.05)\n",
    "cbar = plt.colorbar(overlay, cax=cax, orientation='horizontal')\n",
    "cbar.ax.tick_params(labelsize=\"small\")\n",
    "\n",
    "ax2.imshow(plotter.screenshot(\"medial\"))\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lh_data_parc['total'][i] = parcellate_data(lh_data['total'][i].T, atlas, 'lh').T\n",
    "#lh_data_parc['total'][i] = parcellate_data(lh_data['total'][i].T, atlas, 'lh').T\n",
    "lh_data_parc['infra_cleaned'] = {}\n",
    "rh_data_parc['infra_cleaned'] = {}\n",
    "\n",
    "lh_data_parc['infra_cleaned'][0] = parcellate_data(infra_cleaned_lh.T, atlas, 'lh').T\n",
    "rh_data_parc['infra_cleaned'][0] = parcellate_data(infra_cleaned_rh.T, atlas, 'rh').T\n",
    "\n",
    "infra_cleaned_corr_lh = {0: calculate_correlation_matrix(lh_data_parc['infra_cleaned'][0], transpose=False, partial_corr=True, shrink_it=False, first_index=1)}\n",
    "infra_cleaned_corr_rh = {0: calculate_correlation_matrix(rh_data_parc['infra_cleaned'][0], transpose=False, partial_corr=True, shrink_it=False, first_index=1)}\n",
    "print(infra_cleaned_corr_lh[0].shape)\n",
    "print(infra_cleaned_corr_rh[0].shape)\n",
    "\n",
    "lh_data_parc['ratio_cleaned'] = {}\n",
    "rh_data_parc['ratio_cleaned'] = {}\n",
    "lh_data_parc['ratio_cleaned'][0] = parcellate_data(infra_cleaned_lh.T/total_sample_lh.T, atlas, 'lh').T\n",
    "rh_data_parc['ratio_cleaned'][0] = parcellate_data(infra_cleaned_rh.T/total_sample_rh.T, atlas, 'rh').T\n",
    "\n",
    "ratio_cleaned_corr_lh = {0: calculate_correlation_matrix(lh_data_parc['ratio_cleaned'][0], transpose=False, partial_corr=True, shrink_it=False, first_index=1)}\n",
    "ratio_cleaned_corr_rh = {0: calculate_correlation_matrix(rh_data_parc['ratio_cleaned'][0], transpose=False, partial_corr=True, shrink_it=False, first_index=1)}\n",
    "print(ratio_cleaned_corr_lh[0].shape)\n",
    "print(ratio_cleaned_corr_rh[0].shape)\n",
    "\n",
    "lh_data_parc['predicted_x4'] = {}\n",
    "rh_data_parc['predicted_x4'] = {}\n",
    "lh_data_parc['predicted_x4'][0] = parcellate_data(predicted_x4_lh.T, atlas, 'lh').T\n",
    "rh_data_parc['predicted_x4'][0] = parcellate_data(predicted_x4_rh.T, atlas, 'rh').T\n",
    "\n",
    "predicted_x4_corr_lh = {0: calculate_correlation_matrix(lh_data_parc['predicted_x4'][0], transpose=False, partial_corr=True, shrink_it=False, first_index=1)}\n",
    "predicted_x4_corr_rh = {0: calculate_correlation_matrix(rh_data_parc['predicted_x4'][0], transpose=False, partial_corr=True, shrink_it=False, first_index=1)}\n",
    "print(predicted_x4_corr_lh[0].shape)\n",
    "print(predicted_x4_corr_rh[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niwrap3912",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Post-processing analysis script for cortical layer intensity data.\n",
    "This code processes and visualizes results from quality_control_native_surface_slurm.py\n",
    "\n",
    "#use conda env: niwrap3912\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore, gaussian_kde\n",
    "from scipy.stats import linregress\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Data parameters\n",
    "LAYER_TYPE = 'inf' #parameters can be 'inf', 'pial', 'white'\n",
    "RESOLUTION = '120um'  # 120um or 240um\n",
    "DEPTH = '960um' #960 or 2000um\n",
    "STEPS = 3\n",
    "# Analysis parameters\n",
    "data_type = \"diff\"\n",
    "do_zscore = True\n",
    "add_bigbrain = True\n",
    "\n",
    "# File paths\n",
    "base_path = \"/Users/dennis.jungchildmind.org/Desktop/exvivo\"\n",
    "\n",
    "#data_base_path = f\"/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm/at_inf_surface/output_{DEPTH}_method0/output_{RESOLUTION}_max_{DEPTH}_dist_method0\"\n",
    "data_base_path = f\"/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm/at_{LAYER_TYPE}_surface/output_{DEPTH}_method0/output_{RESOLUTION}_max_{DEPTH}_dist_method0\"\n",
    "bigbrain_base_path = '/Users/dennis.jungchildmind.org/Desktop/BigBrain/PlosBiology2020gii'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualization parameters\n",
    "xlim_max = 1\n",
    "figure_size_depth_profiles = (8, 4)\n",
    "figure_size_kde = (14, 4)\n",
    "figure_size_scatter = (4, 3)\n",
    "\n",
    "# Analysis depth positions (relative to zero index)\n",
    "if RESOLUTION == '120um':\n",
    "    voxel_offset_up = STEPS\n",
    "    voxel_offset_down = -STEPS\n",
    "else:\n",
    "    voxel_offset_up = 2\n",
    "    voxel_offset_down = -2\n",
    "\n",
    "# KDE parameters\n",
    "kde_x_range = (-10, 10)\n",
    "kde_n_points = 500\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize data containers\n",
    "all_data = {\n",
    "    'lh': {'intensity_data': [], 'subject_names': []},\n",
    "    'rh': {'intensity_data': [], 'subject_names': []}\n",
    "}\n",
    "\n",
    "def process_intensity_data(data_array, data_type, do_zscore):\n",
    "    \"\"\"Process intensity data with optional differencing and z-scoring.\"\"\"\n",
    "    if data_type == 'diff':\n",
    "        data_array = np.diff(data_array, axis=0)\n",
    "    if do_zscore:\n",
    "        data_array = zscore(data_array, axis=0)\n",
    "    return data_array\n",
    "\n",
    "\n",
    "# Process ex-vivo subjects\n",
    "for subjects in os.listdir(base_path):\n",
    "    if subjects:\n",
    "        print(subjects)\n",
    "        for hemispheres in ['lh', 'rh']:\n",
    "            intensity_file_path = os.path.join(\n",
    "                data_base_path, subjects, \n",
    "                f\"{hemispheres}/{LAYER_TYPE}_{RESOLUTION}_method0_manual_raw_intensity.npz\"\n",
    "            )\n",
    "\n",
    "            print(intensity_file_path)\n",
    "            if os.path.exists(intensity_file_path):\n",
    "                data = np.load(intensity_file_path, allow_pickle=True)\n",
    "                dist_array = data['dist_array']\n",
    "                \n",
    "                # Process data\n",
    "                tmp_dat = process_intensity_data(data['all_values'], data_type, do_zscore)\n",
    "                \n",
    "                # Store data by hemisphere\n",
    "                clean_subject_name = subjects.replace('_new_confidence', '')\n",
    "                print(f\"Loaded ex-vivo subject: {clean_subject_name}, hemisphere: {hemispheres}\")\n",
    "                \n",
    "                all_data[hemispheres]['intensity_data'].append(tmp_dat)\n",
    "                all_data[hemispheres]['subject_names'].append(clean_subject_name)\n",
    "\n",
    "\n",
    "if add_bigbrain:\n",
    "    # Load BigBrain data\n",
    "    for hemi in ['lh', 'rh']:\n",
    "        #specify the layer surface for the BigBrain data to match the ex vivo data\n",
    "        if LAYER_TYPE == 'inf':\n",
    "            LAYER_SURFACE = 'layer3'\n",
    "            SUBPATH = 'at_inf_surface'\n",
    "        elif LAYER_TYPE == 'pial':\n",
    "            LAYER_SURFACE = 'layer0'\n",
    "            SUBPATH = 'at_pial_surface'\n",
    "        elif LAYER_TYPE == 'white':\n",
    "            LAYER_SURFACE = 'layer6'\n",
    "            SUBPATH = 'at_white_surface'\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid LAYER_TYPE: {LAYER_TYPE}\")\n",
    "\n",
    "        bb_file_path = os.path.join(bigbrain_base_path, SUBPATH, f'bigbrain_{hemi}_{LAYER_SURFACE}_{RESOLUTION}_max_{DEPTH}_method0_manual_raw_intensity.npz')\n",
    "        \n",
    "        if os.path.exists(bb_file_path):\n",
    "            bb_data = np.load(bb_file_path)['all_values']\n",
    "            bb_data = process_intensity_data(bb_data, data_type, do_zscore)\n",
    "            \n",
    "            all_data[hemi]['intensity_data'].append(bb_data)\n",
    "            all_data[hemi]['subject_names'].append('bigbrain')\n",
    "            print(f\"Loaded BigBrain data for hemisphere: {hemi}\")\n",
    "\n",
    "\n",
    "dist_array = np.load(bb_file_path)['dist_array']\n",
    "# Find zero index for distance array\n",
    "zero_indices = np.where(dist_array == 0)[0]\n",
    "if len(zero_indices) > 0:\n",
    "    zero_index = zero_indices[0]\n",
    "else:\n",
    "    zero_index = np.argmin(np.abs(dist_array))\n",
    "    print(f\"No exact zero found, using closest value at index {zero_index} with value {dist_array[zero_index]}\")\n",
    "\n",
    "# Create convenient access variables\n",
    "lh_intensity_data_uncut = all_data['lh']['intensity_data']\n",
    "rh_intensity_data_uncut = all_data['rh']['intensity_data']\n",
    "lh_subject_names = all_data['lh']['subject_names']\n",
    "rh_subject_names = all_data['rh']['subject_names']\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nData loading summary:\")\n",
    "print(f\"Left hemisphere: {len(lh_subject_names)} subjects - {lh_subject_names}\")\n",
    "print(f\"Right hemisphere: {len(rh_subject_names)} subjects - {rh_subject_names}\")\n",
    "\n",
    "# Calculate rolling average of consecutive pairs for distance array\n",
    "dist_array_avg = np.array([(dist_array[i] + dist_array[i + 1]) / 2 for i in range(len(dist_array) - 1)])\n",
    "print(f\"Distance array average: {dist_array_avg}\")\n",
    "\n",
    "# Plot depth profiles for both hemispheres\n",
    "plt.figure(figsize=figure_size_depth_profiles)\n",
    "\n",
    "for subplot_idx, (hemi, subject_names, subject_data) in enumerate([\n",
    "    ('lh', lh_subject_names, lh_intensity_data_uncut),\n",
    "    ('rh', rh_subject_names, rh_intensity_data_uncut)\n",
    "], 1):\n",
    "    \n",
    "    plt.subplot(1, 2, subplot_idx)\n",
    "\n",
    "    # Plot each subject\n",
    "    grand_average_intensity = []\n",
    "    for i, tmp in enumerate(subject_data):\n",
    "        mean_intensity = np.mean(tmp, axis=1)\n",
    "        sem_intensity = np.std(tmp, axis=1) / np.sqrt(tmp.shape[1] - 1)\n",
    "        \n",
    "        yvals = []\n",
    "        if data_type == 'raw':\n",
    "            yvals = dist_array\n",
    "        elif data_type == 'diff':\n",
    "            yvals = dist_array_avg\n",
    "\n",
    "        plt.errorbar(mean_intensity, yvals, xerr=sem_intensity, \n",
    "                    fmt='-o', linewidth=0.75, markersize=3, alpha=0.5, \n",
    "                    label=subject_names[i])\n",
    "        grand_average_intensity.append(mean_intensity)\n",
    "\n",
    "    # Plot grand average\n",
    "    grand_average_intensity = np.nanmean(grand_average_intensity, axis=0)\n",
    "    plt.plot(grand_average_intensity, yvals, 'k-', linewidth=4, \n",
    "             alpha=1, label='Grand Average', zorder=0)\n",
    "    \n",
    "    # Formatting\n",
    "    if do_zscore:\n",
    "        plt.xlim(-xlim_max, xlim_max)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.xlabel('Mean Intensity Difference (Z-score)')\n",
    "    plt.ylabel('Distance from Inf Surface (mm)')\n",
    "    plt.title(f'{hemi.upper()}')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small', frameon=False)\n",
    "    \n",
    "    # Remove spines\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze intensity data at specific depth positions\n",
    "print(f\"Data shape: {subject_data[0].shape}\")\n",
    "print(f\"Distance array: {dist_array_avg}\")\n",
    "\n",
    "# Find key depth positions using configured offsets\n",
    "zero_index = np.where(dist_array_avg == 0)[0][0]\n",
    "one_voxel_up = zero_index + voxel_offset_up\n",
    "one_voxel_down = zero_index + voxel_offset_down\n",
    "\n",
    "print(f\"Zero index: {zero_index}\")\n",
    "print(f\"Depths - Zero: {dist_array_avg[zero_index]:.3f}, Up: {dist_array_avg[one_voxel_up]:.3f}, Down: {dist_array_avg[one_voxel_down]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KDE distributions for each subject\n",
    "hemi = 'rh'\n",
    "subject_data = lh_intensity_data_uncut if hemi == 'lh' else rh_intensity_data_uncut\n",
    "subject_names = lh_subject_names if hemi == 'lh' else rh_subject_names\n",
    "n_subjects = len(subject_data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 7, figsize=figure_size_kde)\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "# Storage for peak distances\n",
    "peak_distances_all_subjects = {\n",
    "    'at_surface': [],\n",
    "    '1_voxel_up': [],\n",
    "    '1_voxel_down': []\n",
    "}\n",
    "\n",
    "# Define positions for analysis\n",
    "positions = [\n",
    "    (zero_index, f'at {LAYER_TYPE} surface', 'at_surface'),\n",
    "    (one_voxel_up, f'{voxel_offset_up} voxel up', '1_voxel_up'),\n",
    "    (one_voxel_down, f'{voxel_offset_down} voxel down', '1_voxel_down')\n",
    "]\n",
    "\n",
    "for i, tmp in enumerate(subject_data):\n",
    "    ax = axes[i]\n",
    "    tmp = np.where(~np.isnan(tmp), tmp, 0)  # Remove NaN values\n",
    "    \n",
    "    for pos_idx, label, key in positions:\n",
    "        tmp_kde = tmp[pos_idx, :]\n",
    "        #check if there is any Nan values\n",
    "        #x_range = np.linspace(kde_x_range[0], kde_x_range[1], kde_n_points)\n",
    "        x_range = np.linspace(np.nanmin(tmp_kde), np.nanmax(tmp_kde), kde_n_points)\n",
    "        kde = gaussian_kde(tmp_kde)\n",
    "        kde_vals = kde(x_range)\n",
    "        line = ax.plot(x_range, kde_vals, alpha=1, linewidth=1.5, label=label)\n",
    "        \n",
    "        # Add peak indicator and store peak distance\n",
    "        peak_idx = np.argmax(kde_vals)\n",
    "        peak_x, peak_y = x_range[peak_idx], kde_vals[peak_idx]\n",
    "        ax.plot([peak_x, peak_x], [0, peak_y], ':', alpha=0.5, \n",
    "                linewidth=1.25, color=line[0].get_color())\n",
    "        \n",
    "        peak_distances_all_subjects[key].append(peak_x)\n",
    "    \n",
    "    if do_zscore:\n",
    "        ax.set_xlim(-2.5,2.5)\n",
    "    else:\n",
    "        #ax.set_xlim(np.nanpercentile(tmp[pos_idx, :], 20), np.nanpercentile(tmp[pos_idx, :],90)) #for inf\n",
    "        ax.set_xlim(np.nanpercentile(tmp[pos_idx, :], 1), np.nanpercentile(tmp[pos_idx, :],99)) #for white\n",
    "\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.set_title(f'{subject_names[i]}')\n",
    "\n",
    "# Add legend\n",
    "if n_subjects < len(axes):\n",
    "    legend_ax = axes[n_subjects]\n",
    "    legend_ax.axis('off')\n",
    "    for pos_idx, label, key in positions:\n",
    "        legend_ax.plot([], [], alpha=1, linewidth=1.5, label=label)\n",
    "    legend_ax.legend(loc='center', frameon=False)\n",
    "\n",
    "# Hide remaining empty subplots\n",
    "for j in range(n_subjects + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6151455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print peak distances summary\n",
    "print(\"\\nPeak distances for all subjects:\")\n",
    "for key, values in peak_distances_all_subjects.items():\n",
    "    print(f\"{key}: {values}\")\n",
    "    print(f\"  Mean: {np.mean(values):.3f}, Std: {np.std(values):.3f}\")\n",
    "\n",
    "# Convert to numpy arrays for analysis\n",
    "at_inf_surface = np.array(peak_distances_all_subjects['at_surface'])\n",
    "voxel_up = np.array(peak_distances_all_subjects['1_voxel_up'])\n",
    "voxel_down = np.array(peak_distances_all_subjects['1_voxel_down'])\n",
    "\n",
    "# Create scatter plot with distinguished colors\n",
    "n_subjects = len(subject_names)\n",
    "colors = plt.cm.tab10(np.arange(n_subjects)) if n_subjects <= 10 else \\\n",
    "         np.vstack([plt.cm.tab10(np.arange(10)), plt.cm.Set3(np.arange(n_subjects - 10))])\n",
    "\n",
    "plt.figure(figsize=figure_size_scatter)\n",
    "for i, subject_name in enumerate(subject_names):\n",
    "    plt.plot(abs(voxel_up[i] - at_inf_surface[i]), \n",
    "             abs(voxel_down[i] - at_inf_surface[i]), 'o', \n",
    "             color=colors[i], label=subject_name, markersize=8, \n",
    "             markeredgecolor='black')\n",
    "\n",
    "plt.legend(subject_names, bbox_to_anchor=(1.05, 1), loc='upper left', \n",
    "           frameon=False, fontsize=10)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlabel('Distance to Voxel Up', fontsize=14)\n",
    "plt.ylabel('Distance to Voxel Down', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.plot([0, 1.5], [0, 1.5], 'k-', linewidth=1, zorder=0)  # x=y line\n",
    "if do_zscore:\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "voxels_up = np.abs(voxel_up - at_inf_surface)\n",
    "voxels_down = np.abs(voxel_down - at_inf_surface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08432ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thickness_corr_wrt_bigbrain['lh']['supra'][:-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the correlation data (from visualize_brain.ipynb)\n",
    "#/Users/dennis.jungchildmind.org/OneDrive - Child Mind Institute/layer_project/cortical_layer/jupyter-notebook/visualization\n",
    "'''\n",
    "with open('/Users/dennis.jungchildmind.org/OneDrive - Child Mind Institute/layer_project/cortical_layer/jupyter-notebook/visualization/thickness_corr_wrt_bigbrain_32k.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    subject_names_corr = data['subject_names']\n",
    "    thickness_corr_wrt_bigbrain = data['thickness_corr_wrt_bigbrain']\n",
    "\n",
    "'''\n",
    "with open('/Users/dennis.jungchildmind.org/OneDrive - Child Mind Institute/layer_project/cortical_layer/jupyter-notebook/visualization/thickness_corr_wrt_bigbrain.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    subject_names_corr = data['subject_names']\n",
    "    thickness_corr_wrt_bigbrain = data['thickness_corr_wrt_bigbrain']\n",
    "\n",
    "#get the reorder index of the subject names_corr[hemi] in subject_names\n",
    "reorder_index = [subject_names.index(subject) for subject in subject_names_corr[hemi]]\n",
    "#reorder subject_names with the reorder_index\n",
    "subject_names_reordered = [subject_names[i] for i in reorder_index]\n",
    "print('new',len(subject_names_reordered),subject_names_reordered)\n",
    "\n",
    "\n",
    "fontsize = 16\n",
    "\n",
    "YVAR = 'ave'#(1) 'ave' (2) 'down' (3) 'up'\n",
    "\n",
    "if YVAR == 'ave':\n",
    "    independent_variable = (voxels_up + voxels_down) / 2\n",
    "elif YVAR == 'down':\n",
    "    independent_variable = voxels_down\n",
    "elif YVAR == 'up':\n",
    "    independent_variable = voxels_up\n",
    "\n",
    "independent_variable = independent_variable[reorder_index]#this should be reordered so that it matches with the thickness_corr_wrt_bigbrain\n",
    "# Set high DPI for better quality\n",
    "plt.figure(figsize=(8,6), dpi=100)\n",
    "\n",
    "for j, layer_type in enumerate(['infra','supra']):\n",
    "    plt.scatter(independent_variable[:-1], thickness_corr_wrt_bigbrain[hemi][layer_type][:-1], \n",
    "                label=layer_type.upper(), s=60, alpha=0.8)\n",
    "    #calculate linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(independent_variable[:-1], thickness_corr_wrt_bigbrain[hemi][layer_type][:-1])\n",
    "    \n",
    "    #plot the linear regression\n",
    "    plt.plot(independent_variable, slope*independent_variable + intercept, '-', \n",
    "             label=f'Linear Regression\\n(R={r_value:.2f}, p={p_value:.3f})', linewidth=2, alpha=0.7)\n",
    "    for i, subject in enumerate(subject_names_reordered):\n",
    "        plt.annotate(subject, (independent_variable[i], thickness_corr_wrt_bigbrain[hemi][layer_type][i]), \n",
    "                    fontsize=fontsize-1, ha='center', va='bottom', alpha=0.8)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False, fontsize=fontsize*0.8)  \n",
    "\n",
    "plt.xlabel(f'Average Distance to {LAYER_TYPE.capitalize()} Surface (mm)', fontsize=fontsize, fontweight='bold')\n",
    "plt.ylabel('Similarity of thickness with BigBrain (Ï)', fontsize=fontsize, fontweight='bold')\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "#only set the minimum of x-axis\n",
    "\n",
    "# Set fewer ticks on x-axis\n",
    "plt.locator_params(axis='x', nbins=4)\n",
    "\n",
    "#remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_linewidth(1.5)\n",
    "plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "plt.tight_layout()\n",
    "#save the figure\n",
    "SUBPATH = 'corr_wrt_bigbrain_figure'\n",
    "#create subfolder if it doesn't exist\n",
    "os.makedirs(SUBPATH, exist_ok=True)\n",
    "plt.savefig(f'{SUBPATH}/{hemi}_thickness_corr_wrt_bigbrain_{layer_type}_{RESOLUTION}.png', dpi=300)\n",
    "\n",
    "#plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054ffa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niwrap3912",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use niwrap2912 conda environment\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import map_coordinates\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import zscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global DO_ZSCORE_BEFORE_DIFF\n",
    "DO_ZSCORE_BEFORE_DIFF = False\n",
    "# Configuration\n",
    "#for exvivo (bigbrain stuff is on the next cell!)\n",
    "base_path = '/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm/at_inf_surface/output_960um_method0/output_120um_max_960um_dist_method0'\n",
    "workbench_base_path ='/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm/at_inf_surface/output_960um_method0'\n",
    "manual_base_path = '/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm/at_inf_surface/output_960um_method0'\n",
    "output_dir = './figure'\n",
    "hemi = 'lh'\n",
    "fontsize = 8\n",
    "percentil_low = 10\n",
    "percentil_high = 70\n",
    "cmap = 'inferno'\n",
    "resolutions = ['120', '240']\n",
    "\n",
    "\n",
    "if DO_ZSCORE_BEFORE_DIFF:\n",
    "    data_types = ['raw', 'raw_zscore', 'diff', 'diff_before_zscore']\n",
    "elif not DO_ZSCORE_BEFORE_DIFF:\n",
    "    data_types = ['raw', 'raw_zscore', 'diff_after_zscore', 'diff_before_zscore']\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all subject folders\n",
    "subject_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "print(f\"Found {len(subject_folders)} subjects: {subject_folders}\")\n",
    "\n",
    "def load_and_process_data(file_path, sorted_order=None):\n",
    "    \"\"\"Load data and compute processed versions (diff, zscore).\"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Use provided sorted order or compute new one\n",
    "    if sorted_order is None:\n",
    "        sorted_order = np.argsort(np.nanmean(data['all_values'], axis=0))\n",
    "    \n",
    "    # Extract and sort raw values\n",
    "    raw = data['all_values'][:, sorted_order]\n",
    "    \n",
    "    # Compute processed versions\n",
    "    raw_zscore = zscore(raw, axis=0, nan_policy='omit')\n",
    "    \n",
    "\n",
    "    #take differentials before and after zscoring\n",
    "    diff = (np.diff(raw, axis=0))\n",
    "    diff_before_zscore = zscore(diff, axis=0, nan_policy='omit')#this is original\n",
    "    #\n",
    "    diff_after_zscore = (np.diff(raw_zscore, axis=0))\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'raw': raw,\n",
    "        'raw_zscore': raw_zscore,\n",
    "        'diff': diff,\n",
    "        'diff_after_zscore': diff_after_zscore,\n",
    "        'diff_before_zscore': diff_before_zscore,\n",
    "        'sorted_order': sorted_order,\n",
    "        'data': data,\n",
    "        \n",
    "    }\n",
    "\n",
    "def set_colorbar_limits(data, data_type):\n",
    "    \"\"\"Set appropriate colorbar limits based on data type.\"\"\"\n",
    "    if data_type == 'raw':\n",
    "        return np.nanpercentile(data, percentil_low), np.nanpercentile(data, percentil_high)\n",
    "    elif data_type in ['diff','diff_after_zscore', 'diff_before_zscore','raw_zscore']:\n",
    "        p_low, p_high = np.nanpercentile(data, percentil_low), np.nanpercentile(data, percentil_high)\n",
    "        #max_clim = round(max(abs(p_low), abs(p_high)), 2)\n",
    "        return p_low, p_high\n",
    "\n",
    "def setup_colorbar(im, ax, data_type, vmin, vmax):\n",
    "    \"\"\"Setup colorbar with appropriate ticks and labels.\"\"\"\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    #if data_type in ['diff', 'diff_after_zscore', 'diff_before_zscore']:\n",
    "       # cbar.set_ticks([vmin, 0, vmax])\n",
    "       # cbar.set_ticklabels([f'{vmin:.2f}', '0', f'{vmax:.2f}'])\n",
    "    \n",
    "    return cbar\n",
    "\n",
    "def create_subplot(ax, data, title, data_type, plot_dist_values):\n",
    "    \"\"\"Create a single subplot with proper formatting.\"\"\"\n",
    "    masked_data = np.ma.masked_invalid(data)\n",
    "    \n",
    "    im = ax.imshow(masked_data, aspect='auto', \n",
    "                   # The 'extent' argument in imshow sets the axis limits and orientation.\n",
    "                   # Here, x-axis goes from 0 to number of columns (vertices), \n",
    "                   # y-axis goes from the last to the first value in plot_dist_values (to flip y-axis so 0 is at the top).\n",
    "                   #extent=[0, data.shape[1], plot_dist_values[-1], plot_dist_values[0]], \n",
    "                   cmap=cmap)\n",
    "    #I need to set each ytick to the values in plot_dist_values\n",
    "    ax.set_yticks(range(len(plot_dist_values)))\n",
    "    # Set yticklabels for odd-numbered indices only\n",
    "    yticklabels = [f\"{np.round(val,2)}\" if i % 2 == 1 else \"\" for i, val in enumerate(plot_dist_values)]\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    \n",
    "    vmin, vmax = set_colorbar_limits(data, data_type)\n",
    "    im.set_clim(vmin, vmax)\n",
    "    \n",
    "    setup_colorbar(im, ax, data_type, vmin, vmax)\n",
    "    \n",
    "    ax.set_xlabel('Vertices (ordered by column mean intensity)', fontsize=fontsize+1)\n",
    "    ax.set_ylabel('Rel. Inf Surf (mm)', fontsize=fontsize)\n",
    "    ax.set_title(title, fontsize=fontsize+1,fontweight='bold')\n",
    "    \n",
    "    return im\n",
    "\n",
    "# Process each subject\n",
    "data_storage = {}\n",
    "all_dist_values = {}\n",
    "for subject_name in subject_folders:\n",
    "    print(f\"\\nProcessing subject: {subject_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and process data for all resolutions\n",
    "        for resolution in resolutions:\n",
    "            print(f\"  Processing resolution: {resolution}\")\n",
    "            manual_path = f'{manual_base_path}/output_{resolution}um_max_960um_dist_method0/{subject_name}/{hemi}/inf_{resolution}um_method0_manual_raw_intensity.npz'\n",
    "            \n",
    "            if not os.path.exists(manual_path):\n",
    "                print(f\"    Warning: Manual file not found: {manual_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Initialize data storage dictionaries for this subject\n",
    "            if subject_name not in data_storage:\n",
    "                data_storage[subject_name] = {}\n",
    "            \n",
    "            data_storage[subject_name][resolution] = {'manual': {}}\n",
    "            \n",
    "            # Load manual data first to establish sorting order\n",
    "            manual_results = load_and_process_data(manual_path)\n",
    "            sorted_order = manual_results['sorted_order']\n",
    "            \n",
    "            # Store processed data\n",
    "            data_storage[subject_name][resolution]['manual'] = manual_results\n",
    "            \n",
    "            # Extract distance array\n",
    "            all_dist_values[resolution] = manual_results['data']['dist_array']\n",
    "\n",
    "        # Skip if no data was loaded\n",
    "        if not all_dist_values:\n",
    "            print(f\"  Skipping {subject_name}: No data files found\")\n",
    "            continue\n",
    "        \n",
    "        # Use the first resolution's distance values for plotting\n",
    "        plot_dist_values = all_dist_values[resolutions[0]]\n",
    "        # Calculate the average between each pair of successive values in plot_dist_values\n",
    "        if isinstance(plot_dist_values, (list, np.ndarray)) and len(plot_dist_values) > 1:\n",
    "            avg_between_successive = [(plot_dist_values[i] + plot_dist_values[i+1]) / 2 for i in range(len(plot_dist_values)-1)]\n",
    "            print(\"Averages between successive values in plot_dist_values:\", avg_between_successive)\n",
    "        else:\n",
    "            print(\"plot_dist_values does not have enough elements to compute averages between successive values.\")\n",
    "        print(np.array(avg_between_successive))\n",
    "\n",
    "        # Create individual figures for each data type\n",
    "        individual_figures = []\n",
    "        for data_type in data_types:\n",
    "            print(f\"  Creating plots for data type: {data_type}\")\n",
    "            \n",
    "            # Get data for both resolutions\n",
    "            data1_manual = data_storage[subject_name][resolutions[0]]['manual'][data_type]\n",
    "            data2_manual = data_storage[subject_name][resolutions[1]]['manual'][data_type]\n",
    "            \n",
    "            # Create figure with subplots\n",
    "            #fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 3))\n",
    "            \n",
    "            # Create subplots\n",
    "            #create_subplot(ax1, data1_manual, '120um', data_type, plot_dist_values)\n",
    "            #create_subplot(ax2, data2_manual, '240um', data_type, plot_dist_values)\n",
    "            \n",
    "            fig, (ax1) = plt.subplots(1, 1, figsize=(6,2))\n",
    "            # Create subplots\n",
    "            if data_type == 'raw' or data_type == 'raw_zscore':\n",
    "                create_subplot(ax1, data1_manual, '120um', data_type, plot_dist_values)\n",
    "            else:\n",
    "                #for differences    \n",
    "                create_subplot(ax1, data1_manual, '120um', data_type, avg_between_successive)\n",
    "\n",
    "            # Add tight layout\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save individual figure\n",
    "            individual_figures.append(fig)\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Create combined 2x2 figure\n",
    "\n",
    "        combined_fig, axes = plt.subplots(2, 2, figsize=(4,2),dpi=300)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Add supertitle for the subject\n",
    "        combined_fig.suptitle(f'{subject_name}', fontsize=fontsize-2, fontweight='bold', y=0.98)\n",
    "\n",
    "        # Copy each individual figure to the combined figure\n",
    "        for i, (data_type, individual_fig) in enumerate(zip(data_types, individual_figures)):\n",
    "            # Get the individual figure's canvas as an image\n",
    "            individual_fig.canvas.draw()\n",
    "            buf = np.frombuffer(individual_fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "            buf = buf.reshape(individual_fig.canvas.get_width_height()[::-1] + (4,))\n",
    "            buf = buf[:, :, :3]  # Remove alpha channel\n",
    "            \n",
    "            # Display in the combined figure\n",
    "            axes[i].imshow(buf)\n",
    "            axes[i].set_title(f'{data_type}', fontsize=fontsize-4, fontweight='bold')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Adjust layout and save combined figure\n",
    "        plt.tight_layout()\n",
    "        output_path = os.path.join(output_dir, hemi)\n",
    "        #create the folder if it doesn't exist\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        if DO_ZSCORE_BEFORE_DIFF:\n",
    "            #plt.savefig(os.path.join(output_path,f'combined_comparison_120um_vs_240um_{subject_name}_zscore_before_diff.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.savefig(os.path.join(output_path,f'combined_comparison_120um_{subject_name}_zscore_before_diff.png'), dpi=300, bbox_inches='tight')\n",
    "        else:\n",
    "            #plt.savefig(os.path.join(output_path,f'combined_comparison_120um_vs_240um_{subject_name}_zscore_after_diff.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.savefig(os.path.join(output_path,f'combined_comparison_120um_{subject_name}_zscore_after_diff.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close(combined_fig)\n",
    "        print(f\"  Saved figure: {output_path}\")\n",
    "        \n",
    "        # Close individual figures to free memory\n",
    "        for fig in individual_figures:\n",
    "            plt.close(fig)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {subject_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nProcessing complete! Figures saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of correct np.diff usage: subtracting two arrays elementwise\n",
    "\n",
    "# Print a 2D list (matrix) without numpy\n",
    "matrix = [[1, 2,3,4,5], [2, 3,4,5,6 ],[1,2,3,4,5]]\n",
    "print(np.diff(matrix,axis=0))\n",
    "plt.imshow(np.diff(matrix,axis=0))\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a54012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for bigbrain\n",
    "# Configuration\n",
    "hemi = 'lh'\n",
    "fontsize = 8\n",
    "percentil_low = 10\n",
    "percentil_high = 70\n",
    "cmap = 'plasma'\n",
    "resolutions = ['120', '240']\n",
    "\n",
    "if DO_ZSCORE_BEFORE_DIFF:\n",
    "    data_types = ['raw', 'raw_zscore', 'diff', 'diff_before_zscore']\n",
    "elif not DO_ZSCORE_BEFORE_DIFF:\n",
    "    data_types = ['raw', 'raw_zscore', 'diff_after_zscore', 'diff_before_zscore']\n",
    "\n",
    "subject_name = 'BigBrain'\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def load_and_process_data(file_path, sorted_order=None):\n",
    "    \"\"\"Load data and compute processed versions (diff, zscore) with NaN handling.\"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Use provided sorted order or compute new one\n",
    "    if sorted_order is None:\n",
    "        # Use nanmean for sorting to handle NaN values\n",
    "        sorted_order = np.argsort(np.nanmean(data['all_values'], axis=0))\n",
    "    \n",
    "    # Extract and sort raw values\n",
    "    raw = data['all_values'][:, sorted_order]\n",
    "    print('shape',raw.shape)\n",
    "    print(raw)\n",
    "\n",
    "    # Compute processed versions with NaN handling\n",
    "    # For zscore, use nanmean and nanstd\n",
    "    raw_mean = np.nanmean(raw, axis=0, keepdims=True)\n",
    "    raw_std = np.nanstd(raw, axis=0, keepdims=True)\n",
    "    raw_std[raw_std == 0] = 1  # Avoid division by zero\n",
    "    raw_zscore = (raw - raw_mean) / raw_std  # Yes, this is z-score normalization\n",
    "    #import zscore from scipy.stats\n",
    "    #raw_zscore = zscore(raw, axis=0, nan_policy='omit')\n",
    "    # For diff, use np.diff which preserves NaN values\n",
    "     #take differentials before and after zscoring\n",
    "    diff = (np.diff(raw, axis=0))\n",
    "    diff_before_zscore = zscore(diff, axis=0, nan_policy='omit')#this is original\n",
    "\n",
    "    #\n",
    "    diff_after_zscore = (np.diff(raw_zscore, axis=0))\n",
    "\n",
    "    #get sign only\n",
    "   # diff_before_zscore = np.sign(diff_before_zscore)\n",
    "    #diff_after_zscore = np.sign(diff_after_zscore)\n",
    "    #END 10/01/2025\n",
    " \n",
    "\n",
    "    return {\n",
    "        'raw': raw,\n",
    "        'raw_zscore': raw_zscore,\n",
    "        'diff': diff,\n",
    "        'diff_after_zscore': diff_after_zscore,\n",
    "        'diff_before_zscore': diff_before_zscore,\n",
    "        'sorted_order': sorted_order,\n",
    "        'data': data,\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "def set_colorbar_limits(data, data_type):\n",
    "    \"\"\"Set appropriate colorbar limits based on data type with NaN handling.\"\"\"\n",
    "    # Use nanpercentile to handle NaN values\n",
    "    if data_type == 'raw':\n",
    "        return np.nanpercentile(data, percentil_low), np.nanpercentile(data, percentil_high)\n",
    "    elif data_type in ['diff','diff_after_zscore', 'diff_before_zscore','raw_zscore']:\n",
    "        p_low, p_high = np.nanpercentile(data, percentil_low), np.nanpercentile(data, percentil_high)\n",
    "        #max_clim = round(max(abs(p_low), abs(p_high)), 2)\n",
    "        return p_low, p_high\n",
    "\n",
    "def setup_colorbar(im, ax, data_type, vmin, vmax):\n",
    "    \"\"\"Setup colorbar with appropriate ticks and labels.\"\"\"\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    return cbar\n",
    "\n",
    "def create_subplot(ax, data, title, data_type, plot_dist_values):\n",
    "    \"\"\"Create a single subplot with proper formatting and NaN handling.\"\"\"\n",
    "    # Create masked array to handle NaN values in visualization\n",
    "    masked_data = np.ma.masked_invalid(data)\n",
    "    \n",
    "    \n",
    "    im = ax.imshow(masked_data, aspect='auto', \n",
    "                   # The 'extent' argument in imshow sets the axis limits and orientation.\n",
    "                   # Here, x-axis goes from 0 to number of columns (vertices), \n",
    "                   # y-axis goes from the last to the first value in plot_dist_values (to flip y-axis so 0 is at the top).\n",
    "                   #extent=[0, data.shape[1], plot_dist_values[-1], plot_dist_values[0]], \n",
    "                   cmap=cmap)\n",
    "    #I need to set each ytick to the values in plot_dist_values\n",
    "    ax.set_yticks(range(len(plot_dist_values)))\n",
    "    # Set yticklabels for odd-numbered indices only\n",
    "    yticklabels = [f\"{np.round(val,2)}\" if i % 2 == 1 else \"\" for i, val in enumerate(plot_dist_values)]\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    vmin, vmax = set_colorbar_limits(data, data_type)\n",
    "    \n",
    "    # Handle case where vmin or vmax might be NaN\n",
    "    if np.isnan(vmin) or np.isnan(vmax):\n",
    "        print(f\"Warning: NaN values in colorbar limits for {title}, using data min/max\")\n",
    "        vmin, vmax = np.nanmin(data), np.nanmax(data)\n",
    "    \n",
    "    print('vmin',vmin,'vmax',vmax)\n",
    "    im.set_clim(vmin, vmax)\n",
    "    \n",
    "    setup_colorbar(im, ax, data_type, vmin, vmax)\n",
    "    \n",
    "    ax.set_xlabel('Vertices (ordered by column mean intensity)', fontsize=fontsize+1)\n",
    "    ax.set_ylabel('Rel. Inf Surf (mm)', fontsize=fontsize)\n",
    "    ax.set_title(title, fontsize=fontsize+1,fontweight='bold')\n",
    "    \n",
    "    return im\n",
    "\n",
    "# Process each subject\n",
    "\n",
    "\n",
    "# Load and process data for all resolutions\n",
    "for resolution in resolutions:\n",
    "    print(f\"  Processing resolution: {resolution}\")\n",
    "    \n",
    "    # Define file paths\n",
    "    manual_path = f'/Users/dennis.jungchildmind.org/Desktop/BigBrain/PlosBiology2020gii/at_inf_surface/bigbrain_{hemi}_layer3_{resolution}um_max_960um_method0_manual_raw_intensity.npz'\n",
    "    #workbench_path = f'/Users/dennis.jungchildmind.org/Desktop/BigBrain/PlosBiology2020gii/BigBrain_voxel_up_and_down/surf_voxel_up_and_down/{resolution}um_method1/bigbrain_workbench_raw_intensity.npz'\n",
    "\n",
    "    # Initialize data storage dictionaries for this subject\n",
    "    # Initialize data storage dictionaries for this subject\n",
    "    if subject_name not in data_storage:\n",
    "        data_storage[subject_name] = {}\n",
    "            \n",
    "    data_storage[subject_name][resolution] = {\n",
    "        'manual': {}\n",
    "       # 'workbench': {}\n",
    "    }\n",
    "    \n",
    "    # Check if files exist\n",
    "    #if not os.path.exists(workbench_path):\n",
    "    #    print(f\"    Warning: Workbench file not found: {workbench_path}\")\n",
    "    #    continue\n",
    "    if not os.path.exists(manual_path):\n",
    "        print(f\"    Warning: Manual file not found: {manual_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Load manual data first to establish sorting order\n",
    "    manual_results = load_and_process_data(manual_path)\n",
    "    \n",
    "    # Load workbench data using the same sorting order\n",
    "    #workbench_results = load_and_process_data(workbench_path)\n",
    "    # Store processed data\n",
    "    data_storage[subject_name][resolution]['manual']= manual_results\n",
    "    #data_storage[subject_name][resolution]['workbench'] = workbench_results\n",
    "    \n",
    "    # Extract distance array\n",
    "    all_dist_values[resolution] = manual_results['data']['dist_array']\n",
    "    print(all_dist_values[resolution])\n",
    "\n",
    "\n",
    "\n",
    "# Use the first resolution's distance values for plotting\n",
    "plot_dist_values = all_dist_values[resolutions[0]]\n",
    "\n",
    "\n",
    "# Calculate the average between each pair of successive values in plot_dist_values\n",
    "if isinstance(plot_dist_values, (list, np.ndarray)) and len(plot_dist_values) > 1:\n",
    "    avg_between_successive = [(plot_dist_values[i] + plot_dist_values[i+1]) / 2 for i in range(len(plot_dist_values)-1)]\n",
    "    print(\"Averages between successive values in plot_dist_values:\", avg_between_successive)\n",
    "else:\n",
    "    print(\"plot_dist_values does not have enough elements to compute averages between successive values.\")\n",
    "print(np.array(avg_between_successive))\n",
    "\n",
    "# Create individual figures for each data type\n",
    "individual_figures = []\n",
    "for data_type in data_types:\n",
    "    print(f\"  Creating plots for data type: {data_type}\")\n",
    "    \n",
    "    # Get data for both resolutions\n",
    "    data1_manual = data_storage[subject_name]['120']['manual'][data_type]\n",
    "    data2_manual = data_storage[subject_name]['240']['manual'][data_type]\n",
    "    \n",
    "    # Check for NaN values and reports\n",
    "    nan_count_1 = np.sum(np.isnan(data1_manual))\n",
    "    nan_count_2 = np.sum(np.isnan(data2_manual))\n",
    "    if nan_count_1 > 0 or nan_count_2 > 0:\n",
    "        print(f\"    Warning: Found {nan_count_1} NaN values in 120um data, {nan_count_2} NaN values in 240um data\")\n",
    "    \n",
    "    \n",
    "    # Create figure with subplots\n",
    "    #fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 3))\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(6, 2))\n",
    "    # Create subplots\n",
    "    if data_type == 'raw' or data_type == 'raw_zscore':\n",
    "        create_subplot(ax1, data1_manual, '120um', data_type, plot_dist_values)\n",
    "    else:\n",
    "        #for differences    \n",
    "        create_subplot(ax1, data1_manual, '120um', data_type, avg_between_successive)\n",
    "\n",
    "    #\n",
    "\n",
    "    # Add tight layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save individual figure\n",
    "    individual_figures.append(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Create combined 2x2 figure\n",
    "combined_fig, axes = plt.subplots(2, 2, figsize=(4,2),dpi=300)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Add supertitle for the subject\n",
    "combined_fig.suptitle(f'{subject_name}', fontsize=fontsize-2, fontweight='bold', y=0.98)\n",
    "\n",
    "# Copy each individual figure to the combined figure\n",
    "for i, (data_type, individual_fig) in enumerate(zip(data_types, individual_figures)):\n",
    "    # Get the individual figure's canvas as an image\n",
    "    individual_fig.canvas.draw()\n",
    "    buf = np.frombuffer(individual_fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "    buf = buf.reshape(individual_fig.canvas.get_width_height()[::-1] + (4,))\n",
    "    buf = buf[:, :, :3]  # Remove alpha channel\n",
    "    \n",
    "    # Display in the combined figure\n",
    "    axes[i].imshow(buf)\n",
    "    axes[i].set_title(f'{data_type}', fontsize=fontsize-4, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Adjust layout and save combined figure with extra space for supertitle\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "output_path = os.path.join(output_dir, hemi)\n",
    "#create the folder if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "if DO_ZSCORE_BEFORE_DIFF:\n",
    "    #plt.savefig(os.path.join(output_path,f'combined_comparison_120um_vs_240um_bigbrain_zscore_before_diff.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(output_path,f'combined_comparison_120um_bigbrain_zscore_before_diff.png'), dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "   # plt.savefig(os.path.join(output_path,f'combined_comparison_120um_vs_240um_bigbrain_zscore_after_diff.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(output_path,f'combined_comparison_120um_bigbrain_zscore_after_diff.png'), dpi=300, bbox_inches='tight')\n",
    "#plt.close(combined_fig)\n",
    "print(f\"  Saved figure: {output_path}\")\n",
    "\n",
    "# Close individual figures to free memory\n",
    "for fig in individual_figures:\n",
    "    plt.close(fig)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (data_storage['BigBrain']['120']['manual']['diff_before_zscore']);\n",
    "#let's do correlation between all pairs of columns in tmp\n",
    "correlations = np.corrcoef(tmp)\n",
    "#set diaognals to 0\n",
    "np.fill_diagonal(correlations, 0)\n",
    "plt.imshow(correlations,cmap='inferno')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.axhline(7, color='cyan', linestyle='--', linewidth=1)\n",
    "plt.axvline(7, color='cyan', linestyle='--', linewidth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56459867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do scatter plot of diff vs diff_before_zscore\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# First subplot\n",
    "im1 = axes[0].hexbin(data_storage['BigBrain']['120']['manual']['diff'], data_storage['BigBrain']['120']['manual']['diff_before_zscore'], gridsize=50, cmap='viridis', mincnt=0.1)\n",
    "axes[0].set_xlabel('Diff',fontsize=fontsize+1)\n",
    "axes[0].set_ylabel('Diff before zscore',fontsize=fontsize+1)\n",
    "axes[0].set_title('BigBrain 120um - Diff vs Diff before zscore', fontsize=fontsize+2,fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0], label='Count')\n",
    "\n",
    "# Second subplot\n",
    "im2 = axes[1].hexbin(data_storage['BigBrain']['120']['manual']['diff_after_zscore'], data_storage['BigBrain']['120']['manual']['diff_before_zscore'], gridsize=50, cmap='viridis', mincnt=0.1)\n",
    "axes[1].set_xlabel('Diff after zscore',fontsize=fontsize+1)\n",
    "axes[1].set_ylabel('Diff before zscore',fontsize=fontsize+1)\n",
    "axes[1].set_title('BigBrain 120um - Diff after zscore vs Diff before zscore', fontsize=fontsize+2,fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1], label='Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data2plot.shape)\n",
    "print((data_storage['BigBrain']['120']['manual'][DATA_TYPE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01827670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.diff(np.mean(data_storage['BigBrain']['120']['manual']['raw'],axis=1)))\n",
    "plt.axvline(7, color='k', linestyle='--', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aaccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_TYPE = 'diff'\n",
    "CMAP = 'plasma'\n",
    "plt.figure(figsize=(12,1))\n",
    "data2plot = (data_storage['BigBrain']['120']['manual'][DATA_TYPE])\n",
    "#sort by column mean vlaue of data2plot\n",
    "# Sort columns by their range (max - min, ignoring NaNs)\n",
    "col_range = np.nanmax(data2plot, axis=0) - np.nanmin(data2plot, axis=0)\n",
    "sorted_data2plot = data2plot[:, np.argsort(col_range)]\n",
    "plt.imshow(sorted_data2plot,aspect='auto',cmap=CMAP)\n",
    "plt.title(f'BigBrain 120um - {DATA_TYPE}', fontsize=fontsize+2,fontweight='bold')\n",
    "plt.xlabel('Vertices',fontsize=fontsize+1)\n",
    "plt.ylabel('Rel. Inf Surf (mm)',fontsize=fontsize+1)\n",
    "#plt.clim(np.nanpercentile(data_storage['BigBrain']['120']['manual'][DATA_TYPE],20),np.nanpercentile(data_storage['BigBrain']['120']['manual'][DATA_TYPE],80))\n",
    "min_val = np.nanpercentile(data_storage['BigBrain']['120']['manual'][DATA_TYPE],10)\n",
    "max_val = np.nanpercentile(data_storage['BigBrain']['120']['manual'][DATA_TYPE],70)\n",
    "#fine the min of the two\n",
    "min_val = max(np.abs(min_val),np.abs(max_val))\n",
    "#plt.clim(-1,1)\n",
    "#put yticks at the values of plot_dist_values\n",
    "#calculate mean of every 2 values in plot_dist_values\n",
    "# Check if plot_dist_values is a scalar or array\n",
    "if np.isscalar(plot_dist_values):\n",
    "    dist_values_inbetween = plot_dist_values\n",
    "else:\n",
    "    dist_values_inbetween = np.array([(plot_dist_values[i] + plot_dist_values[i+1]) / 2 for i in range(len(plot_dist_values)-1)])\n",
    "plt.yticks(range(len(dist_values_inbetween)), np.round(dist_values_inbetween,2))\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min_val,max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14628a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use niwrap2912 conda environment\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import map_coordinates\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Configuration\n",
    "#for exvivo (bigbrain stuff is on the next cell!)\n",
    "base_path = '/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm/output_120um_max_960um_dist_method0'\n",
    "workbench_base_path ='/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm'\n",
    "manual_base_path = '/Users/dennis.jungchildmind.org/Desktop/exvivo_postslurm'\n",
    "\n",
    "# BigBrain configuration\n",
    "bigbrain_base_path = '/Users/dennis.jungchildmind.org/Desktop/BigBrain/PlosBiology2020gii'\n",
    "bigbrain_output_dir = '/Users/dennis.jungchildmind.org/Desktop/BigBrain/output'\n",
    "\n",
    "output_dir = './figure'\n",
    "hemi = 'lh'\n",
    "fontsize = 8\n",
    "percentil_low = 20\n",
    "percentil_high = 80\n",
    "cmap = 'plasma'\n",
    "resolutions = ['120', '240']\n",
    "data_types = ['raw', 'raw_zscore', 'diff', 'diff_zscore']\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all subject folders\n",
    "subject_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "print(f\"Found {len(subject_folders)} subjects: {subject_folders}\")\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    \"\"\"Load data and compute processed versions (diff, zscore).\"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Extract raw values without sorting\n",
    "    raw = data['all_values']\n",
    "    \n",
    "    # Compute processed versions\n",
    "    raw_zscore = zscore(raw, axis=0, nan_policy='omit')\n",
    "    \n",
    "    diff = np.diff(raw, axis=0)\n",
    "    diff_zscore = zscore(diff, axis=0, nan_policy='omit')\n",
    "    \n",
    "    return {\n",
    "        'raw': raw,\n",
    "        'raw_zscore': raw_zscore,\n",
    "        'diff': diff,\n",
    "        'diff_zscore': diff_zscore,\n",
    "        'data': data\n",
    "    }\n",
    "\n",
    "def load_bigbrain_data():\n",
    "    \"\"\"Load BigBrain data for comparison.\"\"\"\n",
    "    bigbrain_data = {}\n",
    "    \n",
    "    # Define BigBrain resolution mapping\n",
    "    bigbrain_resolutions = {\n",
    "        '120': '120um',\n",
    "        '240': '240um'\n",
    "    }\n",
    "    \n",
    "    #f'/Users/dennis.jungchildmind.org/Desktop/BigBrain/PlosBiology2020gii/bigbrain_{hemi}_layer3_{resolution}um_method1_manual_raw_intensity.npz'\n",
    "    for resolution in resolutions:\n",
    "        bb_resolution = bigbrain_resolutions[resolution]\n",
    "        bb_file_path = os.path.join(bigbrain_base_path, f'bigbrain_{hemi}_layer3_{bb_resolution}_method1_manual_raw_intensity.npz')\n",
    "        \n",
    "        if os.path.exists(bb_file_path):\n",
    "            print(f\"Loading BigBrain data: {bb_file_path}\")\n",
    "            bb_results = load_and_process_data(bb_file_path)\n",
    "            bigbrain_data[resolution] = {'bigbrain': bb_results}\n",
    "        else:\n",
    "            print(f\"BigBrain file not found: {bb_file_path}\")\n",
    "    \n",
    "    return bigbrain_data\n",
    "\n",
    "# Process each subject and store data for analysis\n",
    "data_storage = {}\n",
    "all_dist_values = {}\n",
    "\n",
    "for subject_name in subject_folders:\n",
    "    print(f\"\\nProcessing subject: {subject_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and process data for all resolutions\n",
    "        for resolution in resolutions:\n",
    "            print(f\"  Processing resolution: {resolution}\")\n",
    "            manual_path = f'{manual_base_path}/output_{resolution}um_max_960um_dist_method0/{subject_name}/{hemi}/inf_{resolution}um_method0_manual_raw_intensity.npz'\n",
    "            \n",
    "            if not os.path.exists(manual_path):\n",
    "                print(f\"    Warning: Manual file not found: {manual_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Initialize data storage dictionaries for this subject\n",
    "            if subject_name not in data_storage:\n",
    "                data_storage[subject_name] = {}\n",
    "            \n",
    "            data_storage[subject_name][resolution] = {'manual': {}}\n",
    "            \n",
    "            # Load manual data\n",
    "            manual_results = load_and_process_data(manual_path)\n",
    "            \n",
    "            # Store processed data\n",
    "            data_storage[subject_name][resolution]['manual'] = manual_results\n",
    "            \n",
    "            # Extract distance array\n",
    "            all_dist_values[resolution] = manual_results['data']['dist_array']\n",
    "\n",
    "        # Skip if no data was loaded\n",
    "        if not all_dist_values:\n",
    "            print(f\"  Skipping {subject_name}: No data files found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Successfully stored data for {subject_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {subject_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Load BigBrain data\n",
    "print(\"\\nLoading BigBrain data...\")\n",
    "bigbrain_data = load_bigbrain_data()\n",
    "\n",
    "# Add BigBrain data to data_storage for comparison\n",
    "#if bigbrain_data:\n",
    "#    data_storage['BigBrain'] = bigbrain_data\n",
    "#    print(f\"Successfully loaded BigBrain data for resolutions: {list(bigbrain_data.keys())}\")\n",
    "\n",
    "print(f\"\\nData loading complete! Stored data for {len(data_storage)} subjects\")\n",
    "print(f\"Available subjects: {list(data_storage.keys())}\")\n",
    "print(f\"Available resolutions: {resolutions}\")\n",
    "print(f\"Available data types: {data_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed732922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% PCA Analysis Functions\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "RESOLUTION = '120'\n",
    "def perform_pca_analysis(data, plot_variance=False):\n",
    "    \"\"\"\n",
    "    Perform PCA analysis on input data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Input data to perform PCA on\n",
    "    plot_variance : bool\n",
    "        Whether to plot explained variance ratio\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing PCA results\n",
    "        - 'pca_model': fitted PCA model\n",
    "        - 'transformed_data': PCA-transformed data\n",
    "        - 'explained_variance_ratio': explained variance ratio for each component\n",
    "        - 'cumulative_variance': cumulative explained variance\n",
    "    \"\"\"\n",
    "    # Convert to numpy array and handle NaN values\n",
    "    data_array = np.array(data)\n",
    "    data_array[np.isnan(data_array)] = 0\n",
    "    \n",
    "    # Fit PCA (without standardization to preserve original scale)\n",
    "    pca_model = PCA()\n",
    "    transformed_data = pca_model.fit_transform(data_array)\n",
    "    \n",
    "    # Calculate variance metrics\n",
    "    explained_variance_ratio = pca_model.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Data shape: {data_array.shape}\")\n",
    "    print(f\"First 5 components explain {cumulative_variance[4]:.3f} of variance\")\n",
    "    print(f\"PC1 explains {explained_variance_ratio[0]:.3f} of variance\")\n",
    "    \n",
    "    # Plot if requested\n",
    "    if plot_variance:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(explained_variance_ratio[:10], 'o-')  # Show first 10 components\n",
    "        plt.xlabel('Principal Component')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.title('PCA Explained Variance Ratio')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'pca_model': pca_model,\n",
    "        'transformed_data': transformed_data,\n",
    "        'explained_variance_ratio': explained_variance_ratio,\n",
    "        'cumulative_variance': cumulative_variance\n",
    "    }\n",
    "\n",
    "def reconstruct_from_pc(pca_model, transformed_data, pc_indices=[0]):\n",
    "    \"\"\"\n",
    "    Reconstruct data using specified principal components\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca_model : sklearn PCA object\n",
    "        Fitted PCA model\n",
    "    transformed_data : array-like\n",
    "        PCA-transformed data\n",
    "    pc_indices : list\n",
    "        List of PC indices to use for reconstruction (default: [0] for first PC only)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (reconstructed_data, pc_scores, pc_loadings)\n",
    "    \"\"\"\n",
    "    # Get scores and loadings for selected PCs\n",
    "    pc_scores = transformed_data[:, pc_indices]\n",
    "    pc_loadings = pca_model.components_[pc_indices, :]\n",
    "    \n",
    "    # Reconstruct in original space\n",
    "    if len(pc_indices) == 1:\n",
    "        reconstructed_data = np.outer(pc_scores.flatten(), pc_loadings.flatten()) + pca_model.mean_\n",
    "    else:\n",
    "        reconstructed_data = np.dot(pc_scores, pc_loadings) + pca_model.mean_\n",
    "    \n",
    "    return reconstructed_data, pc_scores, pc_loadings\n",
    "\n",
    "# %% Run PCA Analysis on All Subjects\n",
    "\n",
    "def run_pca_on_all_subjects(data_storage, resolution='240'):\n",
    "    \"\"\"\n",
    "    Run PCA analysis on all subjects in data_storage\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_storage : dict\n",
    "        Dictionary containing subject data\n",
    "    resolution : str\n",
    "        Resolution to analyze ('120' or '240')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (all_pca_results, all_var_explained)\n",
    "    \"\"\"\n",
    "    all_pca_results = {}\n",
    "    all_var_explained = {}\n",
    "    \n",
    "    print(f\"Running PCA analysis for {resolution}um resolution...\")\n",
    "    \n",
    "    for subject_name in data_storage.keys():\n",
    "        print(f\"Processing {subject_name}...\")\n",
    "        \n",
    "        # Get data for PCA\n",
    "        data_to_pca = data_storage[subject_name][resolution]['manual']['diff_zscore']\n",
    "        \n",
    "        # Perform PCA\n",
    "        pca_results = perform_pca_analysis(data_to_pca)\n",
    "        \n",
    "        # Get PC1 scores - this is the key part to check!\n",
    "        reconstructed_data, pc1_scores, pc1_loadings = reconstruct_from_pc(\n",
    "            pca_results['pca_model'], \n",
    "            pca_results['transformed_data']\n",
    "        )\n",
    "        \n",
    "        # Debug: Check what we're actually storing\n",
    "        print(f\"  PC1 scores shape: {pc1_scores.shape}\")\n",
    "        print(f\"  PC1 scores type: {type(pc1_scores)}\")\n",
    "        print(f\"  PC1 scores first 5 values: {pc1_scores.flatten()[:5]}\")\n",
    "        \n",
    "        # Store PC1 scores (flattened to 1D array)\n",
    "        all_pca_results[subject_name] = pc1_scores.flatten()\n",
    "        all_var_explained[subject_name] = pca_results['explained_variance_ratio']\n",
    "    \n",
    "    return all_pca_results, all_var_explained\n",
    "\n",
    "\n",
    "def plot_depth_intensity_1d(data, dist_array, label, xlabel=None, ylabel=None, ax=None, color=None):\n",
    "    \"\"\"Plot depth intensity profile with clean styling\"\"\"\n",
    "    # Debug: Check what data we're plotting\n",
    "    print(f\"  Plotting data shape: {np.array(data).shape}\")\n",
    "    print(f\"  Data type: {type(data)}\")\n",
    "    print(f\"  First 5 values: {np.array(data).flatten()[:5]}\")\n",
    "    \n",
    "    # Handle 1D data properly\n",
    "    if len(np.array(data).shape) == 1:\n",
    "        data_mean = np.array(data)\n",
    "        data_std = np.zeros_like(data_mean)  # No std for single profile\n",
    "        data_sem = data_std\n",
    "    else:\n",
    "        data_mean = np.nanmean(data, axis=1)\n",
    "        data_std = np.nanstd(data, axis=1)\n",
    "        data_sem = data_std\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(4, 6))\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    dist_adj = abs(dist_array[0] - dist_array[1]) / 2\n",
    "    \n",
    "    # Plot with clean styling\n",
    "    line = ax.plot(data_mean, dist_array[1:] + dist_adj, \n",
    "                   label=label, linewidth=1.5, color=color)\n",
    "    \n",
    "    # Subtle confidence interval (only if we have std)\n",
    "    if np.any(data_sem > 0):\n",
    "        ax.fill_betweenx(dist_array[1:] + dist_adj, \n",
    "                         data_mean - data_sem, \n",
    "                         data_mean + data_sem, \n",
    "                         alpha=0.15, color=line[0].get_color())\n",
    "    \n",
    "    # Clean reference line\n",
    "    ax.axhline(y=0, color='#666666', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Set limits\n",
    "    ax.set_xlim(np.nanpercentile(data, 2), np.nanpercentile(data, 98))\n",
    "    ax.set_ylim(dist_array[-1], dist_array[0])\n",
    "    \n",
    "    # Minimal styling\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.8)\n",
    "    ax.spines['bottom'].set_linewidth(0.8)\n",
    "    ax.tick_params(width=0.8, length=4)\n",
    "    \n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel, fontsize=12, color='#333333')\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel, fontsize=12, color='#333333')\n",
    "\n",
    "def plot_pca_results(all_pca_results, all_var_explained, all_dist_values, resolution='240'):\n",
    "    \"\"\"\n",
    "    Create comprehensive PCA results visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_pca_results : dict\n",
    "        PC1 scores for each subject\n",
    "    all_var_explained : dict\n",
    "        Explained variance ratios for each subject\n",
    "    all_dist_values : dict\n",
    "        Distance values for plotting\n",
    "    resolution : str\n",
    "        Resolution used for analysis\n",
    "    \"\"\"\n",
    "    # Debug: Check what we received\n",
    "    print(f\"Plotting PCA results for {len(all_pca_results)} subjects\")\n",
    "    for subject_name, pc_scores in all_pca_results.items():\n",
    "        print(f\"Subject {subject_name}: PC scores shape {np.array(pc_scores).shape}\")\n",
    "    \n",
    "    # Color palette\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', \n",
    "              '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "    \n",
    "    # Create combined figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # First subplot: PC1 scores depth profile\n",
    "    all_pc_scores = []\n",
    "    for i, subject_name in enumerate(all_pca_results.keys()): \n",
    "        pc_scores = all_pca_results[subject_name]\n",
    "        subject_label = subject_name.split('_')[0]\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        print(f\"Plotting {subject_name} with PC scores shape: {np.array(pc_scores).shape}\")\n",
    "        \n",
    "        plot_depth_intensity_1d(pc_scores, all_dist_values[resolution], \n",
    "                               label=f'{subject_label}', \n",
    "                               xlabel='PC1 Score', \n",
    "                               ylabel='Distance from Pial Surface (mm)', \n",
    "                               ax=ax1, color=color)\n",
    "        all_pc_scores.append(pc_scores)\n",
    "    \n",
    "    # Set consistent x-limits for first subplot\n",
    "    vmin, vmax = np.nanpercentile(all_pc_scores, [5, 95])    \n",
    "    ax1.set_xlim(vmin, vmax)\n",
    "    ax1.set_title('PC1 Depth Profiles', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Clean legend\n",
    "    legend = ax1.legend(frameon=False, loc='center left', bbox_to_anchor=(1, 0.5), \n",
    "                       handlelength=1.0, handletextpad=0.5)\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontsize(11)\n",
    "        text.set_color('#333333')\n",
    "    for line in legend.get_lines():\n",
    "        line.set_linewidth(3.0)\n",
    "    \n",
    "    ax1.grid(False)\n",
    "    \n",
    "    # Second subplot: Explained variance ratio\n",
    "    for i, subject_name in enumerate(all_var_explained.keys()):\n",
    "        subject_label = subject_name.split('_')[0]\n",
    "        color = colors[i % len(colors)]\n",
    "        # Show first 10 components\n",
    "        variance_ratio = all_var_explained[subject_name][:10]\n",
    "        ax2.plot(range(1, len(variance_ratio)+1), variance_ratio, \n",
    "                'o-', label=subject_label, color=color, markersize=4)\n",
    "    \n",
    "    ax2.set_xlabel('Principal Component')\n",
    "    ax2.set_ylabel('Explained Variance Ratio')\n",
    "    ax2.set_title('Explained Variance by PC', fontsize=14, fontweight='bold')\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_linewidth(0.8)\n",
    "    ax2.spines['bottom'].set_linewidth(0.8)\n",
    "    ax2.tick_params(width=0.8, length=4)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(frameon=False, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return all_pc_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "all_pca_results, all_var_explained = run_pca_on_all_subjects(data_storage, resolution=RESOLUTION)\n",
    "\n",
    "# %% Plotting Functions\n",
    "# Create the visualization\n",
    "all_pc_scores = plot_pca_results(all_pca_results, all_var_explained, all_dist_values, resolution=RESOLUTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488cda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaspy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_on_brain(surface_file_path, hemi, data2plot, cmap):\n",
    "    plotter = yaspy.Plotter(surface_file_path, hemi=hemi)\n",
    "    vmin, vmax = np.nanpercentile(data2plot, [1,99])  # Wider range for better visualization\n",
    "    abs_max =max(abs(vmin),abs(vmax))\n",
    "    # set max and min of intensity_diff by percentile\n",
    "    #abs_max = np.percentile(np.abs(intensity_diff_array), 95)\n",
    "    #int_min = -abs_max\n",
    "    #int_max = abs_max\n",
    "    overlay = plotter.overlay(data2plot, cmap=cmap, vmin=-abs_max, vmax=abs_max)\n",
    "    img0 = yaspy.montage([plotter.screenshot('lateral'),\n",
    "                        plotter.screenshot('medial')],\n",
    "                        pad=8)\n",
    "    # Make the figure\n",
    "    W, H = img0.size\n",
    "    f, ax = plt.subplots(figsize=(6,4))  # Made figure wider\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.imshow(img0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "\n",
    "    # Create some room in the axes for the colorbar\n",
    "    ymax, ymin = plt.ylim()\n",
    "    plt.ylim(ymax + 0.2 * H, ymin)\n",
    "    plt.imshow(img0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # Inset colorbar\n",
    "    cax = ax.inset_axes((0.8, 0.1, 0.18, 0.04))\n",
    "    cbar = plt.colorbar(overlay, cax=cax, orientation=\"horizontal\", label='',extend='both')\n",
    "    cbar.ax.tick_params(labelsize=\"x-small\")\n",
    "\n",
    "cmap = 'jet'\n",
    "pial_file_path = '/Users/dennis.jungchildmind.org/Desktop/BigBrain/PlosBiology2020gii/surfgii/layer3_left_327680.surf.gii'\n",
    "plot_on_brain(pial_file_path, 'lh', kurtosis_120, cmap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niwrap3912",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

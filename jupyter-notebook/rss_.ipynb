{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated keywords. Total: 37 keywords across 4 categories\n",
      "üîç Checking Nature Neuroscience RSS feed at 2025-07-16 10:42:19\n",
      "üéØ Searching for articles containing keywords from 4 categories\n",
      "üìù Total keywords: 37\n",
      "\n",
      "‚úÖ Found 3 relevant articles out of 39 total articles\n",
      "\n",
      "================================================================================\n",
      "üéØ RELEVANT ARTICLES (3 articles)\n",
      "================================================================================\n",
      "\n",
      "1. How the brain shifts between external and internal attention\n",
      "   Link: https://www.cell.com/neuron/fulltext/S0896-6273(25)00471-4?rss=yes\n",
      "   Date: \n",
      "   üéØ RELEVANCE SCORE: 1\n",
      "   üìù KEYWORDS FOUND: pet\n",
      "   üìÇ NEUROIMAGING: pet\n",
      "   üë• Authors: Anna C. Nobre, Daniela Gresch\n",
      "   üìÑ Summary: Nobre and Gresch call for an upgrade of attention research by considering how the brain shifts its focus between contents in the external sensory stream and internal memory representations. They highlight competing hypotheses, review the few experimental attempts and findings, propose candidate neur...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Grouping signals in primate visual cortex\n",
      "   Link: https://www.cell.com/neuron/fulltext/S0896-6273(25)00350-2?rss=yes\n",
      "   Date: \n",
      "   üéØ RELEVANCE SCORE: 1\n",
      "   üìù KEYWORDS FOUND: pet\n",
      "   üìÇ NEUROIMAGING: pet\n",
      "   üë• Authors: Tom P. Franken, John H. Reynolds\n",
      "   üìÑ Summary: Border ownership (BOS) neurons signal which side of a border is foreground. Franken and Reynolds identify grouping in V4‚Äîfast, persistent centripetal ownership preference‚Äîwhich could underlie BOS signals through feedback. Grouping survives stimulus interruption and emerges de novo after saccades; th...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Chimeric brain models: Unlocking insights into human neural development, aging, diseases, and cell therapies\n",
      "   Link: https://www.cell.com/neuron/fulltext/S0896-6273(25)00256-9?rss=yes\n",
      "   Date: \n",
      "   üéØ RELEVANCE SCORE: 1\n",
      "   üìù KEYWORDS FOUND: pet\n",
      "   üìÇ NEUROIMAGING: pet\n",
      "   üë• Authors: Ava V. Papetti, Mengmeng Jin, Ziyuan Ma, Alessandro C. Stillitano, Peng Jiang\n",
      "   üìÑ Summary: Papetti et al. discuss recent advances in understanding human neural development, aging, and disease uncovered through human-rodent chimeric brain models and examine their potential to drive progress in neuroscience research, translational medicine, and cell-based therapies for treating neurodegener...\n",
      "--------------------------------------------------------------------------------\n",
      "üíæ 3 relevant articles saved to relevant_articles.json\n",
      "\n",
      "============================================================\n",
      "üìä SUMMARY REPORT\n",
      "============================================================\n",
      "Total articles checked: 39\n",
      "Relevant articles found: 3\n",
      "Relevance rate: 7.7%\n",
      "\n",
      "üîù Top Keywords Found:\n",
      "   ‚Ä¢ pet: 3 articles\n",
      "\n",
      "üìÇ Category Distribution:\n",
      "   ‚Ä¢ neuroimaging: 3 articles\n",
      "\n",
      "‚≠ê Highest Scoring Articles:\n",
      "   1. [1 pts] How the brain shifts between external and internal attention...\n",
      "   2. [1 pts] Grouping signals in primate visual cortex...\n",
      "   3. [1 pts] Chimeric brain models: Unlocking insights into human neural ...\n",
      "\n",
      "üíæ Detailed report saved to relevant_articles_report_20250716_104220.json\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Set\n",
    "import re\n",
    "import time\n",
    "\n",
    "class KeywordFilteredRSSParser:\n",
    "    def __init__(self, rss_url: str = \"https://www.cell.com/neuron/inpress.rss\"):\n",
    "        self.rss_url = rss_url\n",
    "        self.articles_file = \"nature_neuroscience_articles.json\"\n",
    "        self.relevant_articles_file = \"relevant_articles.json\"\n",
    "        \n",
    "        # Default keywords - you can modify these\n",
    "        self.keywords = {\n",
    "            'imaging': ['eeg', 'fmri', 'imaging', 'mri', 'pet scan', 'neuroimaging', \n",
    "                       'functional magnetic resonance', 'electroencephalography', \n",
    "                       'magnetoencephalography', 'meg', 'bold signal', 'diffusion tensor',\n",
    "                       'dti', 'functional connectivity', 'resting state', 'task-based fmri'],\n",
    "            'techniques': ['electrophysiology', 'optogenetics', 'calcium imaging',\n",
    "                          'two-photon', 'confocal', 'microscopy', 'electrode'],\n",
    "            'analysis': ['machine learning', 'deep learning', 'neural network',\n",
    "                        'classification', 'decoding', 'connectivity analysis']\n",
    "        }\n",
    "        \n",
    "        # Flatten keywords for easier searching\n",
    "        self.all_keywords = []\n",
    "        for category, words in self.keywords.items():\n",
    "            self.all_keywords.extend(words)\n",
    "    \n",
    "    def add_keywords(self, new_keywords: List[str], category: str = 'custom'):\n",
    "        \"\"\"Add new keywords to search for\"\"\"\n",
    "        if category not in self.keywords:\n",
    "            self.keywords[category] = []\n",
    "        \n",
    "        self.keywords[category].extend(new_keywords)\n",
    "        self.all_keywords.extend(new_keywords)\n",
    "        print(f\"Added {len(new_keywords)} keywords to category '{category}'\")\n",
    "    \n",
    "    def set_keywords(self, keywords_dict: Dict[str, List[str]]):\n",
    "        \"\"\"Set custom keywords dictionary\"\"\"\n",
    "        self.keywords = keywords_dict\n",
    "        self.all_keywords = []\n",
    "        for category, words in self.keywords.items():\n",
    "            self.all_keywords.extend(words)\n",
    "        print(f\"Updated keywords. Total: {len(self.all_keywords)} keywords across {len(self.keywords)} categories\")\n",
    "    \n",
    "    def check_relevance(self, article: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Check if article is relevant based on keywords\n",
    "        Returns dict with relevance info\n",
    "        \"\"\"\n",
    "        text_to_search = f\"{article['title']} {article['summary']}\".lower()\n",
    "        \n",
    "        found_keywords = []\n",
    "        keyword_categories = []\n",
    "        \n",
    "        # Check each category\n",
    "        for category, words in self.keywords.items():\n",
    "            category_matches = []\n",
    "            for keyword in words:\n",
    "                if keyword.lower() in text_to_search:\n",
    "                    category_matches.append(keyword)\n",
    "                    if keyword not in found_keywords:\n",
    "                        found_keywords.append(keyword)\n",
    "            \n",
    "            if category_matches:\n",
    "                keyword_categories.append({\n",
    "                    'category': category,\n",
    "                    'matches': category_matches\n",
    "                })\n",
    "        \n",
    "        relevance_score = len(found_keywords)\n",
    "        is_relevant = relevance_score > 0\n",
    "        \n",
    "        return {\n",
    "            'is_relevant': is_relevant,\n",
    "            'relevance_score': relevance_score,\n",
    "            'found_keywords': found_keywords,\n",
    "            'keyword_categories': keyword_categories\n",
    "        }\n",
    "    \n",
    "    def fetch_and_parse_rss(self) -> List[Dict]:\n",
    "        \"\"\"Fetch and parse the RSS feed from Nature Neuroscience\"\"\"\n",
    "        try:\n",
    "            feed = feedparser.parse(self.rss_url)\n",
    "            articles = []\n",
    "            \n",
    "            for entry in feed.entries:\n",
    "                article = {\n",
    "                    'title': self.clean_title(entry.title),\n",
    "                    'link': entry.link,\n",
    "                    'date': self.parse_date(entry.get('published', '')),\n",
    "                    'summary': self.clean_summary(entry.get('summary', '')),\n",
    "                    'authors': self.extract_authors(entry),\n",
    "                    'doi': self.extract_doi(entry),\n",
    "                    'fetched_on': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "                # Add relevance information\n",
    "                relevance_info = self.check_relevance(article)\n",
    "                article.update(relevance_info)\n",
    "                \n",
    "                articles.append(article)\n",
    "            \n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching RSS feed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def clean_title(self, title: str) -> str:\n",
    "        \"\"\"Remove CDATA tags and clean up the title\"\"\"\n",
    "        if title:\n",
    "            title = title.replace('<![CDATA[', '').replace(']]>', '')\n",
    "            title = re.sub(r'<[^>]+>', '', title)\n",
    "        return title.strip()\n",
    "    \n",
    "    def clean_summary(self, summary: str) -> str:\n",
    "        \"\"\"Clean up the summary text\"\"\"\n",
    "        if summary:\n",
    "            summary = summary.replace('<![CDATA[', '').replace(']]>', '')\n",
    "            summary = re.sub(r'<[^>]+>', ' ', summary)\n",
    "            summary = re.sub(r'\\s+', ' ', summary)\n",
    "        return summary.strip()\n",
    "    \n",
    "    def parse_date(self, date_str: str) -> str:\n",
    "        \"\"\"Parse and format the publication date\"\"\"\n",
    "        if not date_str:\n",
    "            return \"\"\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(date_str[:10], '%Y-%m-%d')\n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            return date_str\n",
    "    \n",
    "    def extract_authors(self, entry) -> List[str]:\n",
    "        \"\"\"Extract authors from the entry\"\"\"\n",
    "        authors = []\n",
    "        if hasattr(entry, 'authors'):\n",
    "            for author in entry.authors:\n",
    "                if hasattr(author, 'name'):\n",
    "                    authors.append(author.name)\n",
    "        return authors\n",
    "    \n",
    "    def extract_doi(self, entry) -> str:\n",
    "        \"\"\"Extract DOI from the entry\"\"\"\n",
    "        doi = \"\"\n",
    "        if hasattr(entry, 'prism_doi'):\n",
    "            doi = entry.prism_doi\n",
    "        elif hasattr(entry, 'id'):\n",
    "            if 'doi:' in entry.id:\n",
    "                doi = entry.id.replace('doi:', '')\n",
    "        return doi\n",
    "    \n",
    "    def display_articles(self, articles: List[Dict], title: str = \"Articles\", show_relevance: bool = True):\n",
    "        \"\"\"Display articles in a formatted way\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{title} ({len(articles)} articles)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            print(f\"\\n{i}. {article['title']}\")\n",
    "            print(f\"   Link: {article['link']}\")\n",
    "            print(f\"   Date: {article['date']}\")\n",
    "            \n",
    "            if show_relevance and article.get('is_relevant'):\n",
    "                print(f\"   üéØ RELEVANCE SCORE: {article['relevance_score']}\")\n",
    "                print(f\"   üìù KEYWORDS FOUND: {', '.join(article['found_keywords'])}\")\n",
    "                \n",
    "                # Show which categories matched\n",
    "                for cat_info in article['keyword_categories']:\n",
    "                    print(f\"   üìÇ {cat_info['category'].upper()}: {', '.join(cat_info['matches'])}\")\n",
    "            \n",
    "            if article['authors']:\n",
    "                authors_str = ', '.join(article['authors'][:3])\n",
    "                if len(article['authors']) > 3:\n",
    "                    authors_str += f\" et al. ({len(article['authors'])} total)\"\n",
    "                print(f\"   üë• Authors: {authors_str}\")\n",
    "            \n",
    "            if article['doi']:\n",
    "                print(f\"   üîó DOI: {article['doi']}\")\n",
    "            \n",
    "            if article['summary']:\n",
    "                summary = article['summary'][:300] + \"...\" if len(article['summary']) > 300 else article['summary']\n",
    "                print(f\"   üìÑ Summary: {summary}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    def filter_relevant_articles(self, articles: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Filter articles to only include relevant ones\"\"\"\n",
    "        relevant = [article for article in articles if article.get('is_relevant', False)]\n",
    "        \n",
    "        # Sort by relevance score (highest first)\n",
    "        relevant.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)\n",
    "        \n",
    "        return relevant\n",
    "    \n",
    "    def save_relevant_articles(self, articles: List[Dict]):\n",
    "        \"\"\"Save only relevant articles to a separate file\"\"\"\n",
    "        relevant_articles = self.filter_relevant_articles(articles)\n",
    "        \n",
    "        try:\n",
    "            with open(self.relevant_articles_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(relevant_articles, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"üíæ {len(relevant_articles)} relevant articles saved to {self.relevant_articles_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving relevant articles: {e}\")\n",
    "    \n",
    "    def generate_summary_report(self, articles: List[Dict]):\n",
    "        \"\"\"Generate a summary report of findings\"\"\"\n",
    "        relevant_articles = self.filter_relevant_articles(articles)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üìä SUMMARY REPORT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total articles checked: {len(articles)}\")\n",
    "        print(f\"Relevant articles found: {len(relevant_articles)}\")\n",
    "        print(f\"Relevance rate: {len(relevant_articles)/len(articles)*100:.1f}%\" if articles else \"0%\")\n",
    "        \n",
    "        if relevant_articles:\n",
    "            # Keyword frequency analysis\n",
    "            keyword_freq = {}\n",
    "            category_freq = {}\n",
    "            \n",
    "            for article in relevant_articles:\n",
    "                for keyword in article.get('found_keywords', []):\n",
    "                    keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1\n",
    "                \n",
    "                for cat_info in article.get('keyword_categories', []):\n",
    "                    cat = cat_info['category']\n",
    "                    category_freq[cat] = category_freq.get(cat, 0) + 1\n",
    "            \n",
    "            print(f\"\\nüîù Top Keywords Found:\")\n",
    "            sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "            for keyword, count in sorted_keywords[:10]:\n",
    "                print(f\"   ‚Ä¢ {keyword}: {count} articles\")\n",
    "            \n",
    "            print(f\"\\nüìÇ Category Distribution:\")\n",
    "            for category, count in sorted(category_freq.items()):\n",
    "                print(f\"   ‚Ä¢ {category}: {count} articles\")\n",
    "            \n",
    "            print(f\"\\n‚≠ê Highest Scoring Articles:\")\n",
    "            top_articles = sorted(relevant_articles, key=lambda x: x.get('relevance_score', 0), reverse=True)[:5]\n",
    "            for i, article in enumerate(top_articles, 1):\n",
    "                print(f\"   {i}. [{article['relevance_score']} pts] {article['title'][:60]}...\")\n",
    "    \n",
    "    def check_daily_updates(self):\n",
    "        \"\"\"Main method to check for daily updates with keyword filtering\"\"\"\n",
    "        print(f\"üîç Checking Nature Neuroscience RSS feed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"üéØ Searching for articles containing keywords from {len(self.keywords)} categories\")\n",
    "        print(f\"üìù Total keywords: {len(self.all_keywords)}\")\n",
    "        \n",
    "        # Fetch and analyze articles\n",
    "        articles = self.fetch_and_parse_rss()\n",
    "        \n",
    "        if not articles:\n",
    "            print(\"‚ùå No articles found or error occurred while fetching.\")\n",
    "            return\n",
    "        \n",
    "        # Filter relevant articles\n",
    "        relevant_articles = self.filter_relevant_articles(articles)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(relevant_articles)} relevant articles out of {len(articles)} total articles\")\n",
    "        \n",
    "        if relevant_articles:\n",
    "            self.display_articles(relevant_articles, \"üéØ RELEVANT ARTICLES\")\n",
    "            self.save_relevant_articles(articles)\n",
    "        else:\n",
    "            print(\"üòû No relevant articles found with current keywords.\")\n",
    "            print(\"üí° Consider expanding your keyword list or checking back later.\")\n",
    "        \n",
    "        # Generate summary report\n",
    "        self.generate_summary_report(articles)\n",
    "        \n",
    "        return relevant_articles\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the keyword-filtered RSS parser\"\"\"\n",
    "    \n",
    "    # Initialize parser\n",
    "    parser = KeywordFilteredRSSParser()\n",
    "    \n",
    "    # You can customize keywords like this:\n",
    "    custom_keywords = {\n",
    "        'neuroimaging': ['eeg', 'fmri', 'mri', 'pet', 'neuroimaging', 'bold', \n",
    "                        'functional magnetic resonance', 'electroencephalography',\n",
    "                        'magnetoencephalography', 'meg', 'diffusion tensor imaging',\n",
    "                        'dti', 'resting state', 'task fmri', 'connectivity'],\n",
    "        'brain_stimulation': ['tms', 'transcranial magnetic stimulation', \n",
    "                             'transcranial direct current stimulation', 'tdcs',\n",
    "                             'deep brain stimulation', 'dbs', 'optogenetics'],\n",
    "        'analysis_methods': ['machine learning', 'deep learning', 'artificial intelligence',\n",
    "                           'neural decoding', 'classification', 'regression',\n",
    "                           'connectivity analysis', 'network analysis', 'graph theory'],\n",
    "        'techniques': ['electrophysiology', 'single cell', 'calcium imaging',\n",
    "                      'two-photon microscopy', 'patch clamp', 'microelectrode']\n",
    "    }\n",
    "    \n",
    "    # Set your custom keywords\n",
    "    parser.set_keywords(custom_keywords)\n",
    "    \n",
    "    # Or add additional keywords to existing categories\n",
    "    # parser.add_keywords(['nirs', 'fnirs', 'near infrared'], 'neuroimaging')\n",
    "    \n",
    "    # Check for relevant articles\n",
    "    relevant_articles = parser.check_daily_updates()\n",
    "    \n",
    "    # Save a timestamped report\n",
    "    if relevant_articles:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        report_file = f\"relevant_articles_report_{timestamp}.json\"\n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(relevant_articles, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nüíæ Detailed report saved to {report_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

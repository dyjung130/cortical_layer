{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use niwrap3912 conda environment\n",
    "import os\n",
    "import yaspy\n",
    "import scipy.io\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71dec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#just get the labeling data from the MATLAB file (scholtens2018neuroimage_economo.mat, which is provided by the paper)\n",
    "#Data from this paper: \"An MRI Von Economo â€“ Koskinas atlas\" (Scholtens et al., 2018)\n",
    "#They provide a matlab file with the \"regionDescriptions\"... so since parcellation is much harder from their volumentric data and freesurefer file\n",
    "#use the easier way to do..\n",
    "#But before using this data, make sure to convert the regionDescriptions (which is in stirng array in the struct) to a \"char\" array \n",
    "#because the scipy.io cannot load the string array correctly as it is not supported in the scipy.io.loadmat function\n",
    "#then you can load the field as it is.\n",
    "#load matlab file scholtens2018neuroimage_economo.mat. \n",
    "\n",
    "economo_mat = scipy.io.loadmat('scholtens2018neuroimage_economo_tochar.mat')\n",
    "region_descriptions = [str(desc) for desc in economo_mat['ECONOMO_43']['regionDescriptions'][0][0]]\n",
    "#remove the empty characters in each desciption (meaning remove empty spaces in each description\n",
    "region_descriptions = [desc.strip() for desc in region_descriptions]\n",
    "\n",
    "#calculate cell size\n",
    "#Whereas cell size was calculated according to [H mean x W mean ]; \n",
    "#with H mean (Height) = [H (min-max) /2] and W mean (Width) = [W (min-max) /2] per individual cortical layer and then averaged across all layers.\n",
    "#H is layer1total_thickness_dome\n",
    "#W is layer1total_thickness_wall )\n",
    "\n",
    "#save the cell size data to a numpy array\n",
    "cell_size_data = []\n",
    "for layer in ['I','II','III','IV','V','VI']:\n",
    "    layer_name = f\"layer{layer}\"\n",
    "    cell_size = economo_mat['ECONOMO_43'][f'{layer_name}total_cell_content_cellsize'][0][0]\n",
    "   # print(cell_size)    \n",
    "    cell_size_data.append(cell_size)\n",
    "#get cell_content_cellsize from each layer (I..V) e.g., layerItotal_cell_content_cellsize from the matlab economo_mat\n",
    "#then average across all layers\n",
    "average_cell_size = np.nanmean(cell_size_data,axis=1)\n",
    "print(average_cell_size)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#once the region descriptions are loaded, get the von ek atlas (already mapped in the 32k fsLR space)\n",
    "#from Foit et al,. 2022\n",
    "\n",
    "#so this cell size file \"vonEK_cell_size_conte69_32k_lh.func.gii\" has 43 regions (left and right hemisphere)\n",
    "#the von ek atlas has 44 regions per hemispheres (including ctx-lh-uknown or ctx-rh-unknown)\n",
    "myatlas_base_path = \"/Users/dennis.jungchildmind.org/Downloads/MYATLAS_package_new/\"\n",
    "von_ek_subpath = \"maps/Surface/HCP_conte69/conte69_32k/gii/cytoarchitecture/\"\n",
    "von_ek_lh = \"vonEK_cell_size_conte69_32k_lh.func.gii\"\n",
    "von_ek_rh = \"vonEK_cell_size_conte69_32k_rh.func.gii\"\n",
    "von_ek_cell_size_lh = nib.load(os.path.join(myatlas_base_path, von_ek_subpath,von_ek_lh)).darrays[0].data\n",
    "von_ek_cell_size_rh = nib.load(os.path.join(myatlas_base_path, von_ek_subpath,von_ek_rh)).darrays[0].data\n",
    "#this cell size data should be the same as the oens from the matlab file (from the scholtens2018neuroimage_economo.mat)\n",
    "#though it is not the correct way because it does not have the \"label\" information in the myatlas file,\n",
    "#we can still estimate the parcellation based on the \"unique\" values associated at each vertex 090225 -DJ\n",
    "parcel_index = np.unique(von_ek_cell_size_lh)\n",
    "#set each vertex with the unique parcel index\n",
    "von_ek_atlas = np.zeros(von_ek_cell_size_lh.shape)\n",
    "for i, parcel in enumerate(parcel_index):\n",
    "    von_ek_atlas[von_ek_cell_size_lh == parcel] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6404538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the atlas from the enigmatoolbox \n",
    "#needed to download from https://github.com/MICA-MNI/ENIGMA/blob/master/enigmatoolbox/datasets/parcellations/economo_koskinas_conte69.csv\n",
    "#load the economo_koskinas_conte69.csv file\n",
    "# this is in 32k conte (fs_LR) space\n",
    "import pandas as pd\n",
    "file_path = \"/Users/dennis.jungchildmind.org/OneDrive - Child Mind Institute/layer_project/cortical_layer/external/economo_enigamtoolbox\"\n",
    "file_name = \"economo_koskinas_conte69.csv\"\n",
    "\n",
    "#load the file\n",
    "economo_koskinas_atlas = pd.read_csv(os.path.join(file_path, file_name), header=None)\n",
    "#first half is the left hemisphere and the second half is the right hemisphere\n",
    "economo_koskinas_atlas_lh = economo_koskinas_atlas.iloc[:32492,:].to_numpy().flatten()\n",
    "economo_koskinas_atlas_rh = economo_koskinas_atlas.iloc[32492:,:].to_numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d06060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(economo_koskinas_atlas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d0ae4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_brain(data2plot, surface_file, hemisphere='lh', title=None,\n",
    "               cmap='jet', figsize=(6,4), output_dir=None, savefilename_prefix=None, do_percentile=False, save_files = True):\n",
    "    \"\"\"\n",
    "    Plot brain atlas data on cortical surface\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data2plot : array\n",
    "        Data to plot on the surface\n",
    "    surface_file : str\n",
    "        Path to surface file (.surf.gii)\n",
    "    hemisphere : str, default 'lh'\n",
    "        Hemisphere ('lh' or 'rh')\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    cmap : str, default 'jet'\n",
    "        Colormap to use\n",
    "    figsize : tuple, default (6,4)\n",
    "        Figure size\n",
    "    output_dir : str, optional\n",
    "        Directory to save outputs. If None, no files are saved\n",
    "    filename_prefix : str, default 'von_ek_atlas'\n",
    "        Prefix for saved files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib figure\n",
    "        The created figure\n",
    "    \"\"\"\n",
    "    plotter = yaspy.Plotter(surface_file, hemi=hemisphere)\n",
    "    overlay = plotter.overlay(data2plot, cmap=cmap)\n",
    "\n",
    "    # Generate multi-view montage\n",
    "    views = ['lateral','medial']\n",
    "    screenshots = [plotter.screenshot(view) for view in views]\n",
    "    montage = yaspy.montage([[screenshots[0]],[screenshots[1]]], pad=8)\n",
    "    \n",
    "    # Create and style figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    plt.imshow(montage)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    if do_percentile:\n",
    "        vmin = np.percentile(data2plot, 5)\n",
    "        vmax = np.percentile(data2plot, 95)\n",
    "    else:\n",
    "        vmin = np.nanmin(data2plot)\n",
    "        vmax = np.nanmax(data2plot)\n",
    "    # Add colorbar\n",
    "    cax = ax.inset_axes([0.8, 0, 0.50, 0.04])\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, cax=cax, orientation=\"horizontal\", extend='both')\n",
    "    cbar.set_label('Parcel Index', rotation=0, labelpad=10)\n",
    "    cbar.set_ticks([np.nanmin(data2plot), np.nanmax(data2plot)])\n",
    "    cbar.ax.tick_params(labelsize=\"x-small\")\n",
    "    \n",
    "\n",
    "    if save_files:\n",
    "        # Save outputs if directory specified\n",
    "        if output_dir is not None:\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            plt.savefig(f'{output_dir}/{savefilename_prefix}.png', dpi=300)\n",
    "            np.save(f'{output_dir}/{savefilename_prefix}_32k_fs_LR.npy', data2plot)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "hemi = 'rh'\n",
    "# Use the function\n",
    "if hemi == 'lh':\n",
    "    surface_file = '/Users/dennis.jungchildmind.org/Downloads/MYATLAS_package_new/cortical_surface/conte69_32k/S1200.L.pial_MSMAll.32k_fs_LR.surf.gii'\n",
    "elif hemi == 'rh':\n",
    "    surface_file = '/Users/dennis.jungchildmind.org/Downloads/MYATLAS_package_new/cortical_surface/conte69_32k/S1200.R.pial_MSMAll.32k_fs_LR.surf.gii'\n",
    "data2plot = von_ek_atlas.copy()\n",
    "datamask = von_ek_atlas > 0\n",
    "data2plot[~datamask] = np.nan\n",
    "fig = plot_brain(data2plot, surface_file, hemisphere=hemi,\n",
    "                 output_dir='von_ek_atlas', savefilename_prefix='von_ek_atlas',save_files=False)\n",
    "\n",
    "data2plot = economo_koskinas_atlas_rh.copy().astype(float)\n",
    "datamask = economo_koskinas_atlas_rh > 0\n",
    "data2plot[~datamask] = np.nan\n",
    "\n",
    "# Create discrete colormap with 5 distinct colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "discrete_colors = {\n",
    "    'Agranular': '#3478F6',   # Blue\n",
    "    'Frontal': '#FFB6FF',     # Pink\n",
    "    'Parietal': '#66FF66',    # Green  \n",
    "    'Polar': '#FFFF66',       # Yellow\n",
    "    'Granular': '#9966FF'     # Purple\n",
    "}\n",
    "discrete_cmap = ListedColormap(discrete_colors.values())\n",
    "\n",
    "fig2 = plot_brain(data2plot, surface_file, hemisphere=hemi,\n",
    "                 output_dir='von_ek_atlas', savefilename_prefix='von_ek_atlas',save_files=False, do_percentile=False, cmap=discrete_cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do analysis separate for lh and rh\n",
    "lh_intensity_data_uncut = [];\n",
    "rh_intensity_data_uncut = [];\n",
    "lh_intensity_data_at_zero = [];#at midline\n",
    "rh_intensity_data_at_zero = [];#at midline\n",
    "lh_label_data = [];\n",
    "rh_label_data = [];\n",
    "lh_subject_names = [];\n",
    "rh_subject_names = [];\n",
    "layer = 'inf'\n",
    "data_type = \"diff\"\n",
    "\n",
    "base_path = \"/Users/dennis.jungchildmind.org/Desktop/exvivo\"\n",
    "\n",
    "base_path_intensity = \"/Users/dennis.jungchildmind.org/Desktop/exvivo_figures_norm_120um\"\n",
    "for subjects in os.listdir(base_path):\n",
    "    #for now skip I54_confidence\n",
    "    if subjects == 'I54_new_confidence':\n",
    "        continue\n",
    "    if subjects:\n",
    "        for hemispheres in ['lh','rh']:\n",
    "            intensity_file_path = os.path.join(base_path_intensity, subjects, f\"{hemispheres}/intensity_norm_{data_type}_results.npz\")\n",
    "\n",
    "            if os.path.exists(intensity_file_path):\n",
    "                # Load intensity data\n",
    "                data = np.load(intensity_file_path, allow_pickle=True)\n",
    "                params = data['params']\n",
    "                dist_array = params.item()['dist_array']\n",
    "\n",
    "                if data_type == 'diff':\n",
    "                    #at 0.06 is where the zero should be* I checked with local file\n",
    "                    at_zero = np.where(dist_array == 0.06)[0][0]#this is where the zero should be at for diff dtta\n",
    "                else:\n",
    "                    at_zero = np.where(dist_array == 0.06)[0][0]# \n",
    "\n",
    "                #\n",
    "                ap_order = data['inf_ap_order']\n",
    "                # Sort data from anterior-posterior order to the surface order\n",
    "                surface_order = np.argsort(ap_order)\n",
    "\n",
    "                tmp_dat = data[f'{layer}_{data_type}_intensity'];\n",
    "                tmp_dat = tmp_dat[:,surface_order]\n",
    "                if hemispheres == 'lh':\n",
    "                    lh_intensity_data_uncut.append(tmp_dat)\n",
    "                    lh_intensity_data_at_zero.append(tmp_dat[at_zero,:])\n",
    "                    lh_subject_names.append(subjects.replace('_new_confidence',''))\n",
    "                else:\n",
    "                    rh_intensity_data_uncut.append(tmp_dat)\n",
    "                    rh_intensity_data_at_zero.append(tmp_dat[at_zero,:])\n",
    "                    rh_subject_names.append(subjects.replace('_new_confidence',''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ec96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parcellate intensity data based on the von ek atlas\n",
    "#first get the unique values in the von ek atlas\n",
    "atlas = economo_koskinas_atlas_lh #von_ek_atlas or economo_koskinas_atlas_lh or economo_koskinas_atlas_rh (should be the same for the latter two)\n",
    "unique_values = np.unique(atlas)\n",
    "print(unique_values)\n",
    "\n",
    "\n",
    "rh_intensity_data_at_zero_parcelled = np.zeros((rh_intensity_data_at_zero[0].shape[0], len(rh_intensity_data_at_zero)))\n",
    "rh_intensity_var_at_zero_parcelled = np.zeros((rh_intensity_data_at_zero[0].shape[0], len(rh_intensity_data_at_zero)))\n",
    "print(rh_intensity_data_at_zero_parcelled.shape)\n",
    "#iterate for each file\n",
    "for j, tmp in enumerate(rh_intensity_data_at_zero):\n",
    "    print(tmp.shape)\n",
    "    #iterate for each parcel\n",
    "    for i, parcel in enumerate(unique_values):\n",
    "        indices = np.where(atlas == parcel)[0]\n",
    "        #grab the data first\n",
    "        tmp_data = tmp[indices]\n",
    "        #find standard deviation of the data\n",
    "        tmp_std = np.nanstd(tmp_data)\n",
    "        #average the data but for the data that's within 2 std\n",
    "        tmp_data_within_2std = tmp_data[tmp_data < tmp_std*2]\n",
    "        tmp_mean = np.nanmean(tmp_data_within_2std)\n",
    "        tmp_var = np.nanvar(tmp_data_within_2std)\n",
    "        rh_intensity_data_at_zero_parcelled[indices,j] = tmp_mean\n",
    "        rh_intensity_var_at_zero_parcelled[indices,j] = tmp_var\n",
    "\n",
    "\n",
    "\n",
    "lh_intensity_data_at_zero_parcelled = np.zeros((lh_intensity_data_at_zero[0].shape[0], len(lh_intensity_data_at_zero)))\n",
    "lh_intensity_var_at_zero_parcelled = np.zeros((lh_intensity_data_at_zero[0].shape[0], len(lh_intensity_data_at_zero)))\n",
    "print(lh_intensity_data_at_zero_parcelled.shape)\n",
    "#iterate for each file\n",
    "for j, tmp in enumerate(lh_intensity_data_at_zero):\n",
    "    print(tmp.shape)\n",
    "    #iterate for each parcel\n",
    "    for i, parcel in enumerate(unique_values):\n",
    "        indices = np.where(atlas == parcel)[0]\n",
    "        #grab the data first\n",
    "        tmp_data = tmp[indices]\n",
    "        #find standard deviation of the data\n",
    "        tmp_std = np.nanstd(tmp_data)\n",
    "        #average the data but for the data that's within 2 std\n",
    "        tmp_data_within_2std = tmp_data[tmp_data < tmp_std*2]\n",
    "        tmp_mean = np.nanmean(tmp_data_within_2std)\n",
    "        tmp_var = np.nanvar(tmp_data_within_2std)\n",
    "        lh_intensity_data_at_zero_parcelled[indices,j] = tmp_mean\n",
    "        lh_intensity_var_at_zero_parcelled[indices,j] = tmp_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee59f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "plot_individual_data = True\n",
    "plot_hemi = 'lh'\n",
    "data_type = 'mean' #'mean' or 'var'\n",
    "cmap = 'viridis'\n",
    "#atlas = von_ek_atlas \n",
    "# Surface files and settings\n",
    "lh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.L.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "rh_surf = '/Users/dennis.jungchildmind.org/Downloads/HCP_S1200_Atlas_Z4_pkXDZ/S1200.R.white_MSMAll.32k_fs_LR.surf.gii'\n",
    "\n",
    "\n",
    "def mask_data(data, atlas_data):\n",
    "    data_masked = data.copy()\n",
    "    data_masked[atlas_data == 0] = np.nan\n",
    "    return data_masked\n",
    "\n",
    "# Select data based on hemisphere\n",
    "if plot_hemi == 'lh':\n",
    "    if data_type == 'mean':\n",
    "        data_parc = lh_intensity_data_at_zero_parcelled\n",
    "        clabel = 'Mean (intensity diff)'\n",
    "    elif data_type == 'var':\n",
    "        data_parc = lh_intensity_var_at_zero_parcelled\n",
    "        clabel = 'Var (intensity diff)'\n",
    "    surf2plot = lh_surf\n",
    "    subject_names = lh_subject_names\n",
    "elif plot_hemi == 'rh':\n",
    "    if data_type == 'mean':\n",
    "        data_parc = rh_intensity_data_at_zero_parcelled\n",
    "        clabel = 'Mean (intensity diff)'\n",
    "    elif data_type == 'var':\n",
    "        data_parc = rh_intensity_var_at_zero_parcelled\n",
    "        clabel = 'Var (intensity diff)'\n",
    "    surf2plot = rh_surf\n",
    "    subject_names = rh_subject_names\n",
    "\n",
    "num_cols = lh_intensity_data_at_zero_parcelled.shape[1]#use the one with the higher number of sujbects (lh or rh)\n",
    "\n",
    "#just create one plotter and reuse when overlay\n",
    "plotter = yaspy.Plotter(surf2plot, hemi=plot_hemi)\n",
    "\n",
    "if plot_individual_data:\n",
    "    #perform standardization on the data_parc before averaging using zscore\n",
    "    from scipy.stats import zscore\n",
    "    data_parc_zscore = zscore(data_parc, axis=0, nan_policy='omit')\n",
    "    data2plot_avg = np.nanmean(data_parc_zscore,axis=1)\n",
    "    data2plot_sub = data_parc\n",
    "    num_sub = data2plot_sub.shape[1]\n",
    "    # Create figure with 2 rows (lateral/medial) and 1 + num_sub columns (avg + 11 raw)\n",
    "    fig, axes = plt.subplots(2, 1+num_cols, figsize=(30, 4))\n",
    "    \n",
    "    \n",
    "    # Calculate global min/max for this key across all data points\n",
    "    all_data = np.concatenate([data2plot_avg[data2plot_avg != 0].flatten()])\n",
    "    data_min = -1#np.nanpercentile(all_data[~np.isnan(all_data)], 5)\n",
    "    data_max = 1#np.nanpercentile(all_data[~np.isnan(all_data)], 95)\n",
    "# \n",
    "    # Plot average data (first column)\n",
    "    data_masked = mask_data(data2plot_avg, atlas)\n",
    "    \n",
    "    for view_idx, view in enumerate(['lateral', 'medial']):\n",
    "        #plotter = yaspy.Plotter(surf2plot, hemi=plot_hemi)\n",
    "        #calculate data_min and datA_max of this data only\n",
    "        plotter.overlay(data_masked, cmap=cmap, alpha=1,\n",
    "                    vmin=data_min, vmax=data_max)\n",
    "        plotter.border(data_masked, alpha=0)\n",
    "        img = plotter.screenshot(view)\n",
    "        \n",
    "        axes[view_idx,0].imshow(img)\n",
    "        axes[view_idx,0].axis('off')\n",
    "        \n",
    "        if view_idx == 0:\n",
    "            axes[view_idx,0].set_title(f\"\\n(Average)\", fontweight='bold',fontsize=16)\n",
    "        else:\n",
    "            divider = make_axes_locatable(axes[view_idx,0])\n",
    "        #remove the colorbar for the averaged zscored data\n",
    "        ''''\n",
    "        else:\n",
    "            divider = make_axes_locatable(axes[view_idx,0])\n",
    "            cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.05)\n",
    "            norm = plt.Normalize(vmin=data_min, vmax=data_max)\n",
    "            cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                            cax=cax, orientation='horizontal')\n",
    "            cbar.set_label(clabel,fontsize=16)\n",
    "        '''\n",
    "\n",
    "    # Plot raw data (remaining columns)\n",
    "    for col in range(num_sub):  # Plot all raw data columns\n",
    "        data_masked = mask_data(data2plot_sub[:,col], atlas[0:32492])\n",
    "        # Calculate min/max for this individual subject only\n",
    "        subject_data = data_masked[data_masked != 0]\n",
    "        data_min = np.nanpercentile(subject_data[~np.isnan(subject_data)], 5)\n",
    "        data_max = np.nanpercentile(subject_data[~np.isnan(subject_data)], 95)\n",
    "        \n",
    "        for view_idx, view in enumerate(['lateral', 'medial']):\n",
    "            #plotter = yaspy.Plotter(surf2plot, hemi=plot_hemi)\n",
    "            plotter.overlay(data_masked, cmap=cmap, alpha=1,\n",
    "                        vmin=data_min, vmax=data_max)\n",
    "            plotter.border(data_masked, alpha=0)\n",
    "            img = plotter.screenshot(view)\n",
    "            \n",
    "            axes[view_idx,col+1].imshow(img)\n",
    "            axes[view_idx,col+1].axis('off')\n",
    "            \n",
    "            if view_idx == 0:\n",
    "                axes[view_idx,col+1].set_title(f\"{subject_names[col]}\", fontweight='bold',fontsize=16)\n",
    "            else:\n",
    "                divider = make_axes_locatable(axes[view_idx,col+1])\n",
    "                cax = divider.append_axes(\"bottom\", size=\"5%\", pad=0.05)\n",
    "                norm = plt.Normalize(vmin=data_min, vmax=data_max)\n",
    "                cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), \n",
    "                                cax=cax, orientation='horizontal')\n",
    "                cbar.set_label(clabel,fontsize=16)\n",
    "\n",
    "    # Handle empty columns if num_cols > num_sub\n",
    "    if num_cols > num_sub:\n",
    "        for col in range(num_sub, num_cols):\n",
    "            axes[0,col+1].axis('off')\n",
    "            axes[1,col+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb7571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(economo_koskinas_atlas_lh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niwrap3912",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
